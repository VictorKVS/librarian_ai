# üìÑ –§–∞–π–ª: async_tasks.py
# üìÇ –ü—É—Ç—å: core/tasks/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–ª–Ω–æ–≥–æ NLP-–ø–∞–π–ø–ª–∞–π–Ω–∞

import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
from celery import shared_task, Task
from celery.exceptions import MaxRetriesExceededError

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
from core.text_processing.chunker import SemanticChunker, ChunkingStrategy
from ingest.text_extraction import extract_text_from_file, FileType
from ingest.embedding import EmbeddingGenerator
from processing.ner import EntityExtractor
from processing.graph_builder import KnowledgeGraphBuilder
from db.operations import DocumentStorage
from utils.monitoring import TaskMonitor
from utils.error_handling import DocumentProcessingError

logger = logging.getLogger(__name__)

class DocumentProcessingTask(Task):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        logger.error(f"Task {task_id} failed: {exc}", exc_info=True)
        TaskMonitor.track_failure(task_id, str(exc))
        super().on_failure(exc, task_id, args, kwargs, einfo)

@shared_task(bind=True, base=DocumentProcessingTask)
def process_document_async(self, doc_info: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
    5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
    6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Args:
        doc_info: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
            - doc_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            - original_filename: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            - chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 512)
            - min_confidence: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π (0.7)
            - filters: –§–∏–ª—å—Ç—Ä—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
            - strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è (hybrid)
            - user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            - session_id: ID —Å–µ—Å—Å–∏–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    
    Raises:
        DocumentProcessingError: –ü—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        doc_path = Path(doc_info['doc_path'])
        if not doc_path.exists():
            raise FileNotFoundError(f"Document not found: {doc_path}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        monitor = TaskMonitor(self.request.id)
        chunker = SemanticChunker()
        embedder = EmbeddingGenerator()
        entity_extractor = EntityExtractor()
        graph_builder = KnowledgeGraphBuilder()
        storage = DocumentStorage()
        
        # 1. –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞—á–∏
        monitor.start_processing(
            filename=doc_info.get('original_filename', doc_path.name),
            user_id=doc_info.get('user_id'),
            session_id=doc_info.get('session_id')
        )
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        self.update_state(state='PROGRESS', meta={'stage': 'extracting_text'})
        text = extract_text_from_file(
            doc_path,
            file_type=FileType.from_path(doc_path)
        )
        monitor.log_stage_completion('text_extraction')
        
        # 3. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
        self.update_state(state='PROGRESS', meta={'stage': 'chunking'})
        chunks = chunker.chunk_text(
            text=text,
            strategy=doc_info.get('strategy', 'hybrid'),
            chunk_size=doc_info.get('chunk_size', 512),
            semantic_threshold=0.85,
            min_chunk_size=32
        )
        monitor.log_stage_completion('chunking', metrics={'chunks_count': len(chunks)})
        
        # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.update_state(state='PROGRESS', meta={'stage': 'embedding'})
        embedder.generate_embeddings(chunks)
        monitor.log_stage_completion('embedding')
        
        # 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.update_state(state='PROGRESS', meta={'stage': 'entity_extraction'})
        entity_extractor.extract_entities(
            chunks,
            min_confidence=doc_info.get('min_confidence', 0.7),
            entity_filters=doc_info.get('filters', [])
        )
        monitor.log_stage_completion('entity_extraction')
        
        # 6. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        self.update_state(state='PROGRESS', meta={'stage': 'graph_building'})
        graph = graph_builder.build_graph(chunks)
        monitor.log_stage_completion('graph_building')
        
        # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.update_state(state='PROGRESS', meta={'stage': 'saving_results'})
        doc_id = storage.save_document(
            chunks=chunks,
            graph=graph,
            doc_name=doc_info.get('original_filename', doc_path.name),
            metadata={
                'user_id': doc_info.get('user_id'),
                'session_id': doc_info.get('session_id'),
                'processing_time': monitor.get_processing_time(),
                'chunking_strategy': doc_info.get('strategy', 'hybrid')
            }
        )
        monitor.log_stage_completion('saving_results')
        
        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏
        result = {
            'status': 'completed',
            'document_id': doc_id,
            'chunks_count': len(chunks),
            'entities_count': sum(len(chunk.entities) for chunk in chunks),
            'processing_time': monitor.get_processing_time(),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        monitor.complete_processing(result)
        return result
        
    except Exception as e:
        error_msg = f"Document processing failed: {str(e)}"
        logger.error(error_msg, exc_info=True)
        monitor.log_error(error_msg)
        
        try:
            self.retry(exc=DocumentProcessingError(error_msg), countdown=60, max_retries=3)
        except MaxRetriesExceededError:
            raise DocumentProcessingError(f"Max retries exceeded for document: {doc_path.name}")