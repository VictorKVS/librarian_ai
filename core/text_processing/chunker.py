# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: core/text_processing/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–ø–∞–π–ø–ª–∞–π–Ω–µ

from typing import List, Dict, Optional, Union, Tuple
import re
from dataclasses import dataclass
from enum import Enum, auto
import spacy
from itertools import zip_longest
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    FIXED = auto()           # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É —á–∞–Ω–∫–∏
    SENTENCE = auto()        # –ü–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    PARAGRAPH = auto()       # –ü–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    SEMANTIC = auto()        # –ü–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    HYBRID = auto()          # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è

@dataclass
class TextChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    embeddings: Optional[np.ndarray] = None
    entities: Optional[List[Dict]] = None
    metadata: Dict = None
    cluster_id: Optional[int] = None

class SemanticChunker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.embedding_model = SentenceTransformer(model_name)
        self.nlp = spacy.load('ru_core_news_md')  # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        
    def chunk_text(
        self,
        text: str,
        strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 64,
        min_chunk_size: int = 32,
        semantic_threshold: float = 0.85,
        **kwargs
    ) -> List[TextChunk]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–∞–∑–º–µ—Ä
            semantic_threshold: –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if isinstance(strategy, str):
            strategy = ChunkingStrategy[strategy.upper()]
            
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        clean_text = self._preprocess_text(text)
        
        if strategy == ChunkingStrategy.FIXED:
            return self._fixed_size_chunking(clean_text, chunk_size, overlap, min_chunk_size)
        elif strategy == ChunkingStrategy.SENTENCE:
            return self._sentence_based_chunking(clean_text, chunk_size, min_chunk_size)
        elif strategy == ChunkingStrategy.SEMANTIC:
            return self._semantic_chunking(clean_text, semantic_threshold, min_chunk_size)
        elif strategy == ChunkingStrategy.HYBRID:
            return self._hybrid_chunking(clean_text, chunk_size, overlap, semantic_threshold)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

    def _preprocess_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _fixed_size_chunking(
        self,
        text: str,
        chunk_size: int,
        overlap: int,
        min_size: int
    ) -> List[TextChunk]:
        """–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É"""
        words = text.split()
        chunks = []
        start = 0
        
        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk_words = words[start:end]
            
            if len(chunk_words) >= min_size:
                chunk_text = ' '.join(chunk_words)
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata={"strategy": "fixed"}
                ))
            start += chunk_size - overlap
            
        return chunks

    def _sentence_based_chunking(
        self,
        text: str,
        max_size: int,
        min_size: int
    ) -> List[TextChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sent in sentences:
            sent_length = len(sent.split())
            
            if current_length + sent_length > max_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text.split()) >= min_size:
                    chunks.append(TextChunk(
                        id=len(chunks),
                        text=chunk_text,
                        start_pos=text.find(current_chunk[0]),
                        end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                        metadata={"strategy": "sentence"}
                    ))
                current_chunk = []
                current_length = 0
                
            current_chunk.append(sent)
            current_length += sent_length
            
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text.split()) >= min_size:
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=text.find(current_chunk[0]),
                    end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    metadata={"strategy": "sentence"}
                ))
                
        return chunks

    def _semantic_chunking(
        self,
        text: str,
        threshold: float,
        min_size: int
    ) -> List[TextChunk]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        sentences = [sent.text for sent in self.nlp(text).sents]
        if not sentences:
            return []
            
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        embeddings = self.embedding_model.encode(sentences)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means
        optimal_clusters = self._find_optimal_clusters(embeddings)
        kmeans = KMeans(n_clusters=optimal_clusters).fit(embeddings)
        labels = kmeans.labels_
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        chunks = []
        for cluster_id in set(labels):
            cluster_sents = [sent for sent, label in zip(sentences, labels) if label == cluster_id]
            chunk_text = ' '.join(cluster_sents)
            
            if len(chunk_text.split()) >= min_size:
                start_pos = text.find(cluster_sents[0])
                end_pos = text.find(cluster_sents[-1]) + len(cluster_sents[-1])
                
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    embeddings=self.embedding_model.encode(chunk_text),
                    cluster_id=cluster_id,
                    metadata={"strategy": "semantic"}
                ))
                
        return chunks

    def _hybrid_chunking(
        self,
        text: str,
        chunk_size: int,
        overlap: int,
        threshold: float
    ) -> List[TextChunk]:
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è + —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è"""
        semantic_chunks = self._semantic_chunking(text, threshold, min_size=10)
        final_chunks = []
        
        for chunk in semantic_chunks:
            if len(chunk.text.split()) <= chunk_size * 1.5:
                final_chunks.append(chunk)
            else:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
                fixed_chunks = self._fixed_size_chunking(
                    chunk.text,
                    chunk_size,
                    overlap,
                    min_size=32
                )
                for fc in fixed_chunks:
                    fc.metadata["parent_cluster"] = chunk.cluster_id
                final_chunks.extend(fixed_chunks)
                
        return final_chunks

    def _find_optimal_clusters(self, embeddings: np.ndarray, max_clusters: int = 10) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–º –ª–æ–∫—Ç—è"""
        distortions = []
        for k in range(1, min(max_clusters, len(embeddings))):
            kmeans = KMeans(n_clusters=k).fit(embeddings)
            distortions.append(kmeans.inertia_)
            
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è "–ª–æ–∫—Ç—è"
        if len(distortions) >= 3:
            deltas = np.diff(distortions)
            optimal = np.argmin(deltas) + 1
            return max(2, min(optimal, max_clusters))
        return min(2, len(embeddings))

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    chunker = SemanticChunker()
    
    sample_text = """
    Librarian AI - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. 
    –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã NLP –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤. 
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
    
    –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. 
    –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """
    
    print("=== Fixed Chunking ===")
    fixed_chunks = chunker.chunk_text(sample_text, strategy="FIXED")
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:60]}...")
    
    print("\n=== Semantic Chunking ===")
    semantic_chunks = chunker.chunk_text(sample_text, strategy="SEMANTIC")
    for chunk in semantic_chunks:
        print(f"Chunk {chunk.id} (Cluster {chunk.cluster_id}): {chunk.text[:60]}...")