# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: librarian_ai/ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–ø–∞–π–ø–ª–∞–π–Ω–µ
# üö´ –≠—Ç–æ—Ç –º–æ–¥—É–ª—å **–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ò–ò –Ω–∞–ø—Ä—è–º—É—é**. –û–Ω –≥–æ—Ç–æ–≤–∏—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–º—ã—Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM.

from typing import List, Dict, Optional, Union, Tuple
import re
from dataclasses import dataclass
from enum import Enum, auto
import spacy
from itertools import zip_longest
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    FIXED = auto()
    SENTENCE = auto()
    PARAGRAPH = auto()
    SEMANTIC = auto()
    HYBRID = auto()

@dataclass
class TextChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    embeddings: Optional[np.ndarray] = None
    entities: Optional[List[Dict]] = None
    metadata: Dict = None
    cluster_id: Optional[int] = None

class SemanticChunker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)
        self.nlp = spacy.load('ru_core_news_md')

    def chunk_text(
        self,
        text: str,
        strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 64,
        min_chunk_size: int = 32,
        semantic_threshold: float = 0.85,
        **kwargs
    ) -> List[TextChunk]:
        if isinstance(strategy, str):
            strategy = ChunkingStrategy[strategy.upper()]
        clean_text = self._preprocess_text(text)

        if strategy == ChunkingStrategy.FIXED:
            return self._fixed_size_chunking(clean_text, chunk_size, overlap, min_chunk_size)
        elif strategy == ChunkingStrategy.SENTENCE:
            return self._sentence_based_chunking(clean_text, chunk_size, min_chunk_size)
        elif strategy == ChunkingStrategy.SEMANTIC:
            return self._semantic_chunking(clean_text, semantic_threshold, min_chunk_size)
        elif strategy == ChunkingStrategy.HYBRID:
            return self._hybrid_chunking(clean_text, chunk_size, overlap, semantic_threshold)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

    def _preprocess_text(self, text: str) -> str:
        return re.sub(r'\s+', ' ', text).strip()

    def _fixed_size_chunking(self, text: str, chunk_size: int, overlap: int, min_size: int) -> List[TextChunk]:
        words = text.split()
        chunks = []
        start = 0

        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk_words = words[start:end]
            if len(chunk_words) >= min_size:
                chunk_text = ' '.join(chunk_words)
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata={"strategy": "fixed"}
                ))
            start += chunk_size - overlap

        return chunks

    def _sentence_based_chunking(self, text: str, max_size: int, min_size: int) -> List[TextChunk]:
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        chunks = []
        current_chunk = []
        current_length = 0

        for sent in sentences:
            sent_length = len(sent.split())
            if current_length + sent_length > max_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text.split()) >= min_size:
                    chunks.append(TextChunk(
                        id=len(chunks),
                        text=chunk_text,
                        start_pos=text.find(current_chunk[0]),
                        end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                        metadata={"strategy": "sentence"}
                    ))
                current_chunk = []
                current_length = 0

            current_chunk.append(sent)
            current_length += sent_length

        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text.split()) >= min_size:
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=text.find(current_chunk[0]),
                    end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    metadata={"strategy": "sentence"}
                ))

        return chunks

    def _semantic_chunking(self, text: str, threshold: float, min_size: int) -> List[TextChunk]:
        sentences = [sent.text for sent in self.nlp(text).sents]
        if not sentences:
            return []

        embeddings = self.embedding_model.encode(sentences)
        optimal_clusters = self._find_optimal_clusters(embeddings)
        kmeans = KMeans(n_clusters=optimal_clusters).fit(embeddings)
        labels = kmeans.labels_

        chunks = []
        for cluster_id in set(labels):
            cluster_sents = [sent for sent, label in zip(sentences, labels) if label == cluster_id]
            chunk_text = ' '.join(cluster_sents)

            if len(chunk_text.split()) >= min_size:
                start_pos = text.find(cluster_sents[0])
                end_pos = text.find(cluster_sents[-1]) + len(cluster_sents[-1])

                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    embeddings=self.embedding_model.encode(chunk_text),
                    cluster_id=cluster_id,
                    metadata={"strategy": "semantic"}
                ))

        return chunks

    def _hybrid_chunking(self, text: str, chunk_size: int, overlap: int, threshold: float) -> List[TextChunk]:
        semantic_chunks = self._semantic_chunking(text, threshold, min_size=10)
        final_chunks = []

        for chunk in semantic_chunks:
            if len(chunk.text.split()) <= chunk_size * 1.5:
                final_chunks.append(chunk)
            else:
                fixed_chunks = self._fixed_size_chunking(chunk.text, chunk_size, overlap, min_size=32)
                for fc in fixed_chunks:
                    fc.metadata["parent_cluster"] = chunk.cluster_id
                final_chunks.extend(fixed_chunks)

        return final_chunks

    def _find_optimal_clusters(self, embeddings: np.ndarray, max_clusters: int = 10) -> int:
        distortions = []
        for k in range(1, min(max_clusters, len(embeddings))):
            kmeans = KMeans(n_clusters=k).fit(embeddings)
            distortions.append(kmeans.inertia_)

        if len(distortions) >= 3:
            deltas = np.diff(distortions)
            optimal = np.argmin(deltas) + 1
            return max(2, min(optimal, max_clusters))
        return min(2, len(embeddings))

if __name__ == "__main__":
    chunker = SemanticChunker()
    sample_text = """
    Librarian AI - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. 
    –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã NLP –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤. 
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.

    –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. 
    –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """

    print("=== Fixed Chunking ===")
    fixed_chunks = chunker.chunk_text(sample_text, strategy="FIXED")
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:60]}...")

    print("\n=== Semantic Chunking ===")
    semantic_chunks = chunker.chunk_text(sample_text, strategy="SEMANTIC")
    for chunk in semantic_chunks:
        print(f"Chunk {chunk.id} (Cluster {chunk.cluster_id}): {chunk.text[:60]}...")
