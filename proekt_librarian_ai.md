üìò –î–æ–∫—É–º–µ–Ω—Ç: proekt_librarian_ai.md

üß† –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ

–§–∞–π–ª proekt_librarian_ai.md —Å–ª—É–∂–∏—Ç –∂–∏–≤—ã–º –∂—É—Ä–Ω–∞–ª–æ–º –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI, —Ñ–∏–∫—Å–∏—Ä—É—è –∫–∞–∂–¥–æ–µ –≤–∞–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, –∞ —Ç–∞–∫–∂–µ –æ–±—É—á–∞—é—â–∏–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏. –≠—Ç–æ ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ü–µ–ª–∏, —Ö–æ–¥ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–µ–∫—Ç–∞.

üìå –ò—Å—Ö–æ–¥–Ω—ã–µ —Ü–µ–ª–∏

Librarian AI ‚Äî –∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π:

–ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ª—é–±–æ–π –¥–ª–∏–Ω—ã;

—Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏—Ö –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏;

–∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏, —Ü–∏—Ç–∞—Ç—ã, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è;

–≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π;

—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å—ë –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ CLI –∏–ª–∏ API.

–ü—Ä–æ–µ–∫—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, —é—Ä–∏—Å—Ç–æ–≤ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏—Ö –±–æ–ª—å—à–∏–µ –æ–±—ä—ë–º—ã –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

üß± –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (Librarian AI v1.0)

librarian_ai/
‚îú‚îÄ‚îÄ db/                        # ORM-–º–æ–¥–µ–ª–∏, –º–∏–≥—Ä–∞—Ü–∏–∏, —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã
‚îú‚îÄ‚îÄ ingest/                    # –ó–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥
‚îú‚îÄ‚îÄ processing/                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –≥—Ä–∞—Ñ—ã
‚îú‚îÄ‚îÄ storage/                   # –§–∏–∑–∏—á–µ—Å–∫–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö (sqlite, faiss)
‚îú‚îÄ‚îÄ scripts/                   # –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
‚îú‚îÄ‚îÄ cli/                       # –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ api/                       # API (FastAPI / Telegram)
‚îú‚îÄ‚îÄ tests/                     # –¢–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ utils/                     # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–Ω—Ñ–∏–≥–∏
‚îî‚îÄ‚îÄ README.md                  # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

üìÜ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

üü¢ –ò—é–Ω—å 2025

11 –∏—é–Ω—è:

üß± –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç –∏–º–µ–Ω–∏ "–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞";

üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞;

‚úÖ models.py (ORM) –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω;

üîú –†–µ—à–µ–Ω–∏–µ: —Å–æ–∑–¥–∞–≤–∞—Ç—å create_tables.py –∏ ingest_and_index.py;

üìò –î–æ–±–∞–≤–ª–µ–Ω –ø–ª–∞–Ω –ø–æ —Ä–æ–ª—è–º: –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä, –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, –ü—Ä–æ–¥—É–∫—Ç–æ–ª–æ–≥;

üí¨ –ü—Ä–∏–Ω—è—Ç–æ —Å–æ–∑–¥–∞—Ç—å proekt_librarian_ai.md ‚Äî –∂–∏–≤—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞;

üìå –ü—Ä–∏–Ω—è—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤ README.md –∏ e_full.yaml.

üß≠ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

‚ú≥Ô∏è db/create_tables.py ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã;

‚ú≥Ô∏è scripts/ingest_and_index.py ‚Äî –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≥—Ä—É–∑–∫–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è;

üß™ tests/test_ingest.py ‚Äî –±–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è ingest –∏ embedder;

üìò –û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏ (–≤ –≤–∏–¥–µ .explain.md).

üîó Git/GitHub

–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: github.com/yourusername/librarian_ai.01.mini

–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –≤–µ—Å—Ç–∏ –∫–æ–º–º–∏—Ç—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É:
feat: add embedder, fix: db schema, docs: explain parser

–£—Ä–æ–≤–µ–Ω—å –≤–µ—Ç–≤–ª–µ–Ω–∏—è: main (—Ä–∞–±–æ—á–∏–π), dev (—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞)

üß© –ü—Ä–∏–º–µ—á–∞–Ω–∏—è

–í –ø—Ä–æ–µ–∫—Ç–µ –¥–æ–ø—É—Å–∫–∞—é—Ç—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ dev/

–í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π

–°—Å—ã–ª–∫–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ @module.py –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö

# üìÑ –§–∞–π–ª: create_tables.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—ë—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (SQLite) –∏ —Ç–∞–±–ª–∏—Ü—ã –ø–æ ORM-–º–æ–¥–µ–ª—è–º –∏–∑ models.py. –ü–æ–ª—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ db.models, —Å–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª storage/librarian.db.

from sqlalchemy import create_engine
from db.models import Base
import os

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è storage/ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs("storage", exist_ok=True)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º SQLite –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DB_URL = "sqlite:///storage/librarian.db"
engine = create_engine(DB_URL)

if __name__ == '__main__':
    print("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—Ü...")
    Base.metadata.create_all(engine)
    print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")

    # üìÑ –§–∞–π–ª: ingest_and_index.py
# üìÇ –ü—É—Ç—å: scripts/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É. –ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª–∏ –∏–∑ ingest/, db/, processing/.

import sys
from ingest.loader import load_file
from ingest.parser import parse_text
from ingest.chunker import chunk_text
from ingest.embedder import embed_chunks
from processing.entity_extractor import extract_entities
from db.models import KnowledgeDoc, MemoryItem, EntityRecord
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
import uuid
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ
DB_URL = "sqlite:///storage/librarian.db"
engine = create_engine(DB_URL)
Session = sessionmaker(bind=engine)


def process_document(filepath):
    print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filepath}")
    raw_text, metadata = load_file(filepath)
    parsed_text = parse_text(raw_text)
    chunks = chunk_text(parsed_text)
    embeddings = embed_chunks(chunks)
    entities = extract_entities(parsed_text)

    session = Session()
    doc_id = uuid.uuid4()

    # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å –¥–æ–∫—É–º–µ–Ω—Ç–∞
    doc = KnowledgeDoc(
        id=doc_id,
        title=os.path.basename(filepath),
        content=parsed_text,
        source_path=filepath,
        source_type=metadata.get("type", "txt"),
        processed=True,
        metadata=metadata
    )
    session.add(doc)

    # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
    for chunk, vector in zip(chunks, embeddings):
        item = MemoryItem(
            content=chunk,
            embedding=vector,
            doc_id=doc_id
        )
        session.add(item)

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
    for ent in entities:
        record = EntityRecord(
            label=ent["label"],
            text=ent["text"],
            confidence=ent.get("confidence", 0.9),
            session_id=None,  # –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ
            metadata=ent.get("meta", {}),
            context=ent.get("context")
        )
        session.add(record)

    session.commit()
    session.close()
    print("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –±–∞–∑—É")


if __name__ == '__main__':
    if len(sys.argv) != 2:
        print("‚ùó –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python scripts/ingest_and_index.py path/to/file")
        sys.exit(1)

    filepath = sys.argv[1]
    process_document(filepath)


‚úÖ 1. –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git add .
‚úÖ 2. –ö–æ–º–º–∏—Ç —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git commit -m "feat: –¥–æ–±–∞–≤–ª–µ–Ω ingest_and_index.py ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
‚úÖ 3. –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ GitHub
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git push


loader.py

# üìÑ –§–∞–π–ª: loader.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–µ–≥–æ —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ. –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ —Å–ª–æ–≤–∞—Ä—å —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

import os


def load_file(filepath):
    ext = os.path.splitext(filepath)[-1].lower()
    metadata = {"type": ext[1:] if ext.startswith('.') else ext}

    if ext in ['.txt']:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read(), metadata

    elif ext in ['.md']:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read(), metadata

    elif ext in ['.docx']:
        from docx import Document
        doc = Document(filepath)
        text = "\n".join([p.text for p in doc.paragraphs])
        return text, metadata

    elif ext in ['.pdf']:
        from PyPDF2 import PdfReader
        reader = PdfReader(filepath)
        text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
        return text, metadata

    else:
        raise ValueError(f"‚ùó –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")


‚úÖ 1. –î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git add ingest/loader.py
‚úÖ 2. –°–¥–µ–ª–∞—Ç—å –∫–æ–º–º–∏—Ç
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git commit -m "feat: –¥–æ–±–∞–≤–ª–µ–Ω loader.py ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"
‚úÖ 3. –û—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ GitHub
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git push

gpt

# üìÑ –§–∞–π–ª: parser.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –û—á–∏—â–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî —É–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã. –ü–æ–ª—É—á–∞–µ—Ç —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.

import re

def parse_text(text):
    # –£–¥–∞–ª–∏–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    text = " ".join(lines)

    # –ó–∞–º–µ–Ω–∏–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω
    text = re.sub(r'\s+', ' ', text)

    # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞
    text = re.sub(r'[\x00-\x1f\x7f]', '', text)

    return text.strip()

giga.chat

# üìÑ –§–∞–π–ª: parser.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –û—á–∏—â–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî —É–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã. –ü–æ–ª—É—á–∞–µ—Ç —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.

import re
import unicodedata

def parse_text(text):
    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    text = " ".join(lines)

    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω
    text = re.sub(r'\s+', ' ', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–µ–≤–∏–¥–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub(r'[\x00-\x1f\x7f]', '', text)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤ Unicode (NFKC)
    text = unicodedata.normalize('NFKC', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub(r'[^\w\s.,!?;:\'"]', '', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
    text = text.strip()

    return text

    –û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode (NFKC):–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É Unicode, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —É—Å—Ç—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏–≥–∞—Ç—É—Ä—ã, –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–∫–∏).
–£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤:–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (—Ç–æ—á–∫–∞, –∑–∞–ø—è—Ç–∞—è, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫–∏, –¥–≤–æ–µ—Ç–æ—á–∏–µ, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, –∞–ø–æ—Å—Ç—Ä–æ—Ñ –∏ –∫–∞–≤—ã—á–∫–∏).
–£–¥–∞–ª–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤:–£–¥–∞–ª—è–µ—Ç —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ.
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

raw_text = """
   –≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å –ª–∏—à–Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏   —Å–∏–º–≤–æ–ª–∞–º–∏. 
   –¢–∞–∫–∂–µ –∑–¥–µ—Å—å –µ—Å—Ç—å \t —Ç–∞–±—É–ª—è—Ü–∏—è –∏ \n –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞.
   """

cleaned_text = parse_text(raw_text)
print(cleaned_text)
–í—ã–≤–æ–¥:

–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å –ª–∏—à–Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ —Å–∏–º–≤–æ–ª–∞–º–∏. –¢–∞–∫–∂–µ –∑–¥–µ—Å—å –µ—Å—Ç—å —Ç–∞–±—É–ª—è—Ü–∏—è –∏ –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞.
–¢–µ–ø–µ—Ä—å —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç –±–æ–ª–µ–µ —á–∏—Å—Ç—ã–º –∏ –≥–æ—Ç–æ–≤—ã–º –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.

chat.deepseek.com

–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–µ–∫—Å—Ç–∞:

python
# üìÑ –§–∞–π–ª: parser.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏

import re
import unicodedata
from typing import Optional, List
import html
from functools import lru_cache

class TextParser:
    # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—á–∏—Å—Ç–∫–∏
    WHITESPACE_PATTERN = re.compile(r'\s+')
    CONTROL_CHARS_PATTERN = re.compile(r'[\x00-\x1f\x7f-\x9f]')
    UNWANTED_SYMBOLS_PATTERN = re.compile(r'[^\w\s\-.,!?;:\'"¬´¬ª‚Äú‚Äù()\[\]{}]')
    EMAIL_PATTERN = re.compile(r'\S+@\S+')
    URL_PATTERN = re.compile(r'https?://\S+|www\.\S+')
    PHONE_PATTERN = re.compile(r'[\+\(]?[0-9][0-9\-\(\)]{8,}[0-9]')
    REPEAT_PUNCT_PATTERN = re.compile(r'([.,!?;:])\1+')

    def __init__(self, 
                 replace_emails: bool = True,
                 replace_urls: bool = True,
                 replace_phones: bool = True,
                 min_paragraph_length: int = 50):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏:
        - replace_emails: –ó–∞–º–µ–Ω—è—Ç—å email-–∞–¥—Ä–µ—Å–∞
        - replace_urls: –ó–∞–º–µ–Ω—è—Ç—å URL-–∞–¥—Ä–µ—Å–∞
        - replace_phones: –ó–∞–º–µ–Ω—è—Ç—å –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        - min_paragraph_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
        """
        self.replace_emails = replace_emails
        self.replace_urls = replace_urls
        self.replace_phones = replace_phones
        self.min_paragraph_length = min_paragraph_length

    @lru_cache(maxsize=1000)
    def parse_text(self, text: str, language: Optional[str] = None) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text or not isinstance(text, str):
            return ""

        # 1. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π
        text = html.unescape(text)
        
        # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode
        text = unicodedata.normalize('NFKC', text)
        
        # 3. –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = self.CONTROL_CHARS_PATTERN.sub(' ', text)
        
        # 4. –ó–∞–º–µ–Ω–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ (email, url, —Ç–µ–ª–µ—Ñ–æ–Ω—ã)
        if self.replace_emails:
            text = self.EMAIL_PATTERN.sub('[EMAIL]', text)
        if self.replace_urls:
            text = self.URL_PATTERN.sub('[URL]', text)
        if self.replace_phones:
            text = self.PHONE_PATTERN.sub('[PHONE]', text)
        
        # 5. –û—á–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ —Å —É—á–µ—Ç–æ–º —è–∑—ã–∫–∞
        text = self._clean_unwanted_symbols(text, language)
        
        # 6. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–±–∑–∞—Ü–µ–≤
        paragraphs = self._process_paragraphs(text)
        
        # 7. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        return self._finalize_text(paragraphs)

    def _clean_unwanted_symbols(self, text: str, language: Optional[str]) -> str:
        """–Ø–∑—ã–∫–æ-–∑–∞–≤–∏—Å–∏–º–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤"""
        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        text = self.UNWANTED_SYMBOLS_PATTERN.sub(' ', text)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        if language == 'ru':
            text = re.sub(r'[^–∞-—è—ë–ê-–Ø–Å0-9\s\-.,!?;:\'"¬´¬ª‚Äú‚Äù()\[\]{}]', ' ', text)
        elif language == 'en':
            text = re.sub(r'[^a-zA-Z0-9\s\-.,!?;:\'"‚Äú‚Äù()\[\]{}]', ' ', text)
            
        return text

    def _process_paragraphs(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞–±–∑–∞—Ü–µ–≤"""
        paragraphs = [
            p.strip() for p in text.split('\n') 
            if p.strip() and len(p.strip()) >= self.min_paragraph_length
        ]
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∞–±–∑–∞—Ü–µ–≤
        unique_paragraphs = []
        seen = set()
        for p in paragraphs:
            simple_p = self.WHITESPACE_PATTERN.sub('', p).lower()
            if simple_p not in seen:
                seen.add(simple_p)
                unique_paragraphs.append(p)
                
        return unique_paragraphs

    def _finalize_text(self, paragraphs: List[str]) -> str:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞–±–∑–∞—Ü–µ–≤
        text = '\n'.join(paragraphs)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = self.WHITESPACE_PATTERN.sub(' ', text)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–Ω–∞–∫–æ–≤ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = self.REPEAT_PUNCT_PATTERN.sub(r'\1', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤–æ–∫—Ä—É–≥ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        text = re.sub(r'([.,!?;:])\s+', r'\1 ', text)
        
        return text.strip()

    def batch_parse(self, texts: List[str], language: Optional[str] = None) -> List[str]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        return [self.parse_text(text, language) for text in texts]


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    parser = TextParser()
    
    dirty_text = """
    <html>–≠—Ç–æ –ø—Ä–∏–º–µ—Ä   —Ç–µ–∫—Å—Ç–∞ —Å HTML-—Å—É—â–Ω–æ—Å—Ç—è–º–∏ &amp; –ª–∏—à–Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏. 
    –¢–∞–∫–∂–µ –∑–¥–µ—Å—å –µ—Å—Ç—å email: test@example.com –∏ URL: https://example.com
    –ò –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è... –î–∞,,, –∏–º–µ–Ω–Ω–æ —Ç–∞–∫!!!
    """
    
    clean_text = parser.parse_text(dirty_text, language='ru')
    print(clean_text)
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ö–ª–∞—Å—Å–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

–ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞:

–û–±—Ä–∞–±–æ—Ç–∫–∞ HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π

–£–¥–∞–ª–µ–Ω–∏–µ email, URL –∏ –Ω–æ–º–µ—Ä–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤

–Ø–∑—ã–∫–æ-–∑–∞–≤–∏—Å–∏–º–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤

–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞:

–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∞–±–∑–∞—Ü–µ–≤

–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–µ–≤

–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:

–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤

–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:
text
–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å HTML-—Å—É—â–Ω–æ—Å—Ç—è–º–∏ & –ª–∏—à–Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏.
–¢–∞–∫–∂–µ –∑–¥–µ—Å—å –µ—Å—Ç—å email: [EMAIL] –∏ URL: [URL]
–ò –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è... –î–∞, –∏–º–µ–Ω–Ω–æ —Ç–∞–∫!
–≠—Ç–æ—Ç –ø–∞—Ä—Å–µ—Ä –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–∫–æ–Ω–≤–µ–π–µ—Ä–µ.

giga.chat

–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–≤–æ–¥—ã –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∏ –ø–ª–∞–Ω–∞—Ö –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ Librarian AI:

–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
–í—Ö–æ–¥ —Ç–µ–∫—Å—Ç–∞ (–∏–∑ —Ñ–∞–π–ª–∞, CLI –∏–ª–∏ API):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ CLI (cli/agent_cli.py) –∏ API (api/files.py).
–ü–ª–∞–Ω: –í –±—É–¥—É—â–µ–º –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É Telegram (telegram/bot.py).
–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ (split_into_chunks):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –§—É–Ω–∫—Ü–∏—è split_into_chunks() –≤ core/loader.py.
–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –≤—Ö–æ–¥ –≤ LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é.
–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ (embedder.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/embedder.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ all-MiniLM-L6-v2 (–∏–ª–∏ —Å–≤–æ–µ–π).
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –í–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ knowledge/vector_store/index.faiss, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ meta.pkl.
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–º—ã—Å–ª–æ–≤:
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/entity_extractor_advanced.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ) –∏ spaCy (–¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ).
–ü–æ–¥–¥–µ—Ä–∂–∫–∞: –°–ª–æ–≤–∞—Ä–∏ –∏ –ø–ª–∞–≥–∏–Ω—ã, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, confidence-—Å–µ–ª–µ–∫—Ç.
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (graph_tools.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/graph_tools.py –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ graph_cache/, —ç–∫—Å–ø–æ—Ä—Ç –≤ GEXF, PNG, JSON.
–ó–∞–Ω–µ—Å–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (db/models.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª–∏ db/storage.py –∏ models.py –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–π, —Å—É—â–Ω–æ—Å—Ç–µ–π, –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–∞–º—è—Ç–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
LLM Reasoning ‚Äî –º–æ–¥—É–ª—å librarian_ai.py:
–ü–ª–∞–Ω: –ú–æ–¥—É–ª—å librarian_ai.py –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (chain-of-thought –∏–ª–∏ rule-based) –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≤—ã–≤–æ–¥–æ–≤, –ø–æ–ª–∏—Ç–∏–∫, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.

gpt
# üìÑ –§–∞–π–ª: parser.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏. –£–¥–∞–ª—è–µ—Ç HTML-—Å—É—â–Ω–æ—Å—Ç–∏, email, URL, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â—É—é—Å—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ –∞–±–∑–∞—Ü—ã. –ì–æ—Ç–æ–≤–∏—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π.

import re
import unicodedata
import html
from typing import Optional, List
from functools import lru_cache

class TextParser:
    WHITESPACE_PATTERN = re.compile(r'\s+')
    CONTROL_CHARS_PATTERN = re.compile(r'[\x00-\x1f\x7f-\x9f]')
    UNWANTED_SYMBOLS_PATTERN = re.compile(r'[^\w\s-.,!?;:\'"¬´¬ª‚Äú‚Äù()\[\]{}]')
    EMAIL_PATTERN = re.compile(r'\S+@\S+')
    URL_PATTERN = re.compile(r'https?://\S+|www\.\S+')
    PHONE_PATTERN = re.compile(r'[+(]?[0-9][0-9-()]{8,}[0-9]')
    REPEAT_PUNCT_PATTERN = re.compile(r'([.,!?;:])\1+')

    def __init__(self, replace_emails=True, replace_urls=True, replace_phones=True, min_paragraph_length=50):
        self.replace_emails = replace_emails
        self.replace_urls = replace_urls
        self.replace_phones = replace_phones
        self.min_paragraph_length = min_paragraph_length

    @lru_cache(maxsize=1000)
    def parse_text(self, text: str, language: Optional[str] = None) -> str:
        if not text or not isinstance(text, str):
            return ""

        text = html.unescape(text)
        text = unicodedata.normalize('NFKC', text)
        text = self.CONTROL_CHARS_PATTERN.sub(' ', text)

        if self.replace_emails:
            text = self.EMAIL_PATTERN.sub('[EMAIL]', text)
        if self.replace_urls:
            text = self.URL_PATTERN.sub('[URL]', text)
        if self.replace_phones:
            text = self.PHONE_PATTERN.sub('[PHONE]', text)

        text = self._clean_unwanted_symbols(text, language)
        paragraphs = self._process_paragraphs(text)
        return self._finalize_text(paragraphs)

    def _clean_unwanted_symbols(self, text: str, language: Optional[str]) -> str:
        text = self.UNWANTED_SYMBOLS_PATTERN.sub(' ', text)
        if language == 'ru':
            text = re.sub(r'[^–∞-—è—ë–ê-–Ø–Å0-9\s\-.,!?;:\'"¬´¬ª‚Äú‚Äù()\[\]{}]', ' ', text)
        elif language == 'en':
            text = re.sub(r'[^a-zA-Z0-9\s\-.,!?;:\'"‚Äú‚Äù()\[\]{}]', ' ', text)
        return text

    def _process_paragraphs(self, text: str) -> List[str]:
        paragraphs = [p.strip() for p in text.split('\n') if p.strip() and len(p.strip()) >= self.min_paragraph_length]
        unique_paragraphs = []
        seen = set()
        for p in paragraphs:
            simple_p = self.WHITESPACE_PATTERN.sub('', p).lower()
            if simple_p not in seen:
                seen.add(simple_p)
                unique_paragraphs.append(p)
        return unique_paragraphs

    def _finalize_text(self, paragraphs: List[str]) -> str:
        text = '\n'.join(paragraphs)
        text = self.WHITESPACE_PATTERN.sub(' ', text)
        text = self.REPEAT_PUNCT_PATTERN.sub(r'\1', text)
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        text = re.sub(r'([.,!?;:])\s+', r'\1 ', text)
        return text.strip()

    def batch_parse(self, texts: List[str], language: Optional[str] = None) -> List[str]:
        return [self.parse_text(text, language) for text in texts]

–§–∞–π–ª parser.py –æ–±–Ω–æ–≤–ª—ë–Ω: —Ç–µ–ø–µ—Ä—å —ç—Ç–æ –º–æ—â–Ω—ã–π, –∫—ç—à–∏—Ä—É–µ–º—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —è–∑—ã–∫–æ–≤, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π email/URL/—Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –æ—á–∏—Å—Ç–∫–æ–π. –û–Ω –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω-–∞–≥–µ–Ω—Ç–µ.

–æ–¥—É–º–∞–π —á—Ç–æ –º–æ–∂–Ω–æ –µ—â–µ –¥–æ–±–∞–≤–∏—Ç—å –º–ø–ª–µ–∫—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏. –£–¥–∞–ª—è–µ—Ç HTML-—Å—É—â–Ω–æ—Å—Ç–∏, email, URL, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â—É—é—Å—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ –∞–±–∑–∞—Ü—ã. –ì–æ—Ç–æ–≤–∏—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π. - —Å–¥–µ—Å—å –∫–∞–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è –ø–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞—Ä—à–∞, —á—Ç–æ –±—ã –ø–æ—Ç–æ–º –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—Å—è –∫ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –¥—Ä—É–≥–∏—Ö –≤–µ—Ä—Å–∏—è—Ö , —á—Ç–æ –µ—â–µ —Å–ª–µ–¥—É–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å  

ChatGPT —Å–∫–∞–∑–∞–ª:–ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø–∞—Ä—Å–µ—Ä –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∏ ¬´—Ñ–∞—Ä—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–º¬ª, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä, –ø—Ä—è–º–æ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –Ω–µ–≥–æ:

üîé –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏–ö–∞—Ç–µ–≥–æ—Ä–∏—è	–ü—Ä–∏–º–µ—Ä—ã	–°–ø–æ—Å–æ–± –æ–±—Ä–∞–±–æ—Ç–∫–∏–ù–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤	–ò–ù–ù, –û–ì–†–ù, –ø–∞—Å–ø–æ—Ä—Ç –†–§, –°–ù–ò–õ–°, –Ω–æ–º–µ—Ä –¥–µ–ª	re-–ø–∞—Ç—Ç–µ—Ä–Ω—ã, –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ	—Å—É–º–º—ã, –≤–∞–ª—é—Ç—ã, —Å—á–µ—Ç–∞, IBAN	—à–∞–±–ª–æ–Ω—ã + –ø—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–®–∞–±–ª–æ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤	¬´–ü—Ä–∏–∫–∞–∑ ‚Ññ...¬ª, ¬´–î–æ–≥–æ–≤–æ—Ä –æ—Ç...¬ª	—à–∞–±–ª–æ–Ω—ã –Ω–∞ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º—ã–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã	55.751244, 37.618423	–≥–µ–æ-–≤—ã—Ä–∞–∂–µ–Ω–∏—è, GPS–°—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–ø–∏—Å–∫–æ–≤	–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏	—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏—è markdown-like –±–ª–æ–∫–æ–≤–°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫–æ–Ω—ã	¬´—Å—Ç. 123 –ì–ö –†–§¬ª	—à–∞–±–ª–æ–Ω—ã, —Å–≤—è–∑–∫–∞ —Å –±–∞–∑–æ–π –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è	IP-–∞–¥—Ä–µ—Å–∞, MAC, –ø–æ—Ä—Ç—ã, –≤–µ—Ä—Å–∏–∏ –ü–û	re + —Ñ–∏–ª—å—Ç—Ä—ã–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã	–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, —É—Ä–∞–≤–Ω–µ–Ω–∏—è, —Å–∏–º–≤–æ–ª—ã	–ø–∞—Ä—Å–µ—Ä—ã LaTeX/MathML–¢–µ–º—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏	–ù–õ–ü, –ò–ò, –ë–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏	–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ ML –∏–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö–Ø–≤–Ω—ã–µ –º–µ—Ç–∫–∏	TODO, FIXME, –ê–≤—Ç–æ—Ä:..., –í–µ—Ä—Å–∏—è:...	—Ä–µ–≥—É–ª—è—Ä–∫–∏ + markdown –æ–±—Ä–∞–±–æ—Ç–∫–∞

üß† –ß—Ç–æ –µ—â—ë —Å—Ç–æ–∏—Ç –≤–Ω–µ–¥—Ä–∏—Ç—å:–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å	–ü–æ–ª—å–∑–∞‚úÖ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è	–¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞‚úÖ –£—Å—Ç–æ–π—á–∏–≤—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (MWEs)	—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ñ—Ä–∞–∑ –∫–∞–∫ –µ–¥–∏–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞‚úÖ –õ–æ–≥–∏–∫–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö	[EMAIL], [PHONE], [NAME] –∏ —Ç.–¥.‚úÖ –ì–∏–±–∫–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –ø–æ —è–∑—ã–∫–∞–º	—Ä–∞–∑–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ —Å–∏–º–≤–æ–ª—ã‚úÖ –ü–ª–∞–≥–∏–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞	–ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞

üí° –†–µ–∑—É–ª—å—Ç–∞—Ç–ö–æ–≥–¥–∞ —Ç—ã –≤–Ω–µ–¥—Ä–∏—à—å –≤—Å—ë —ç—Ç–æ, –º–æ–¥—É–ª—å parser.py –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ –º–æ—â–Ω—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä + –ø–µ—Ä–≤–∏—á–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç–æ–ª—å–∫–æ –æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç, –Ω–æ –∏ –ø–æ–º–µ—á–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤–∞–∂–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞.

–û—Ç–≤–µ—Ç

–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–≤–æ–¥—ã –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∏ –ø–ª–∞–Ω–∞—Ö –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ Librarian AI:

–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
–í—Ö–æ–¥ —Ç–µ–∫—Å—Ç–∞ (–∏–∑ —Ñ–∞–π–ª–∞, CLI –∏–ª–∏ API):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ CLI (cli/agent_cli.py) –∏ API (api/files.py).
–ü–ª–∞–Ω: –í –±—É–¥—É—â–µ–º –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É Telegram (telegram/bot.py).
–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ (split_into_chunks):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –§—É–Ω–∫—Ü–∏—è split_into_chunks() –≤ core/loader.py.
–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –≤—Ö–æ–¥ –≤ LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é.
–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ (embedder.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/embedder.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ all-MiniLM-L6-v2 (–∏–ª–∏ —Å–≤–æ–µ–π).
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –í–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ knowledge/vector_store/index.faiss, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ meta.pkl.
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–º—ã—Å–ª–æ–≤:
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/entity_extractor_advanced.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ) –∏ spaCy (–¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ).
–ü–æ–¥–¥–µ—Ä–∂–∫–∞: –°–ª–æ–≤–∞—Ä–∏ –∏ –ø–ª–∞–≥–∏–Ω—ã, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, confidence-—Å–µ–ª–µ–∫—Ç.
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (graph_tools.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/graph_tools.py –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ graph_cache/, —ç–∫—Å–ø–æ—Ä—Ç –≤ GEXF, PNG, JSON.
–ó–∞–Ω–µ—Å–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (db/models.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª–∏ db/storage.py –∏ models.py –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–π, —Å—É—â–Ω–æ—Å—Ç–µ–π, –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–∞–º—è—Ç–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
LLM Reasoning ‚Äî –º–æ–¥—É–ª—å librarian_ai.py:
–ü–ª–∞–Ω: –ú–æ–¥—É–ª—å librarian_ai.py –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (chain-of-thought –∏–ª–∏ rule-based) –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≤—ã–≤–æ–¥–æ–≤, –ø–æ–ª–∏—Ç–∏–∫, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
–í–æ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –î–æ–±–∞–≤–ª–µ–Ω—ã –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏, —á—Ç–æ–±—ã –±–æ–ª—å—à–µ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å—Å—è –∫ –¥–æ—Ä–∞–±–æ—Ç–∫–∞–º:

python
# üìÑ –§–∞–π–ª: parser.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π

import re
import unicodedata
import html
from typing import Optional, List, Dict, Tuple, Callable
from dataclasses import dataclass
import logging
from functools import lru_cache
import phonenumbers
from email_validator import validate_email
import pyap
import dateparser
import numpy as np
from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine

logger = logging.getLogger(__name__)

@dataclass
class EntityPattern:
    name: str
    regex: str
    processor: Optional[Callable] = None
    context_words: Optional[List[str]] = None

class AdvancedTextParser:
    """–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø–æ–ª–Ω—ã–º —Ü–∏–∫–ª–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    
    # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    BASE_PATTERNS = [
        EntityPattern('URL', r'https?://\S+|www\.\S+'),
        EntityPattern('EMAIL', r'\S+@\S+'),
        EntityPattern('PHONE', r'[\+\(]?[0-9][0-9\-\(\)]{8,}[0-9]'),
        EntityPattern('CREDIT_CARD', r'\b(?:\d[ -]*?){13,16}\b'),
        EntityPattern('IP_ADDRESS', r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b'),
        EntityPattern('MAC_ADDRESS', r'([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})'),
        EntityPattern('DATE', 
                     r'\b(\d{1,4}[./-]\d{1,2}[./-]\d{1,4}|'
                     r'[–ê-–Ø–∞-—è]+\s\d{1,2},\s\d{4})\b',
                     dateparser.parse),
        EntityPattern('GEO_COORD', 
                     r'\b\d{1,3}\.\d+\s*,\s*\d{1,3}\.\d+\b')
    ]
    
    # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    LEGAL_PATTERNS = [
        EntityPattern('INN', r'\b\d{10,12}\b', 
                      context_words=['–ò–ù–ù', '–∏–Ω–Ω', 'TaxID']),
        EntityPattern('OGRN', r'\b\d{13}\b',
                      context_words=['–û–ì–†–ù', 'ogrn']),
        EntityPattern('SNILS', r'\b\d{3}-\d{3}-\d{3}\s\d{2}\b'),
        EntityPattern('LAW_REF', 
                     r'(—Å—Ç\.|—Å—Ç–∞—Ç—å—è|article)\s?\d+[.,]?\s?[–ê-–Ø–∞-—èA-Za-z]?'
                     r'\s?(–§–ó|–ì–ö|–£–ö|–¢–ö|–ó–∞–∫–æ–Ω)')
    ]

    def __init__(self, language: str = 'ru'):
        self.language = language
        self.analyzer = AnalyzerEngine()
        self.anonymizer = AnonymizerEngine()
        self._compile_patterns()
        
    def _compile_patterns(self):
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è –≤—Å–µ—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –µ–¥–∏–Ω—ã–π —Å–ø–∏—Å–æ–∫"""
        self.all_patterns = self.BASE_PATTERNS + self.LEGAL_PATTERNS
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —è–∑—ã–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if self.language == 'ru':
            self.all_patterns.extend([
                EntityPattern('PASSPORT_RU', 
                            r'\b\d{4}\s?\d{6}\b',
                            context_words=['–ø–∞—Å–ø–æ—Ä—Ç', '—Å–µ—Ä–∏—è']),
                EntityPattern('CAR_NUMBER_RU',
                            r'[–ê-–Ø]\d{3}[–ê-–Ø]{2}\d{2,3}')
            ])
        elif self.language == 'en':
            self.all_patterns.extend([
                EntityPattern('SSN', 
                            r'\b\d{3}-\d{2}-\d{4}\b'),
                EntityPattern('US_PASSPORT',
                            r'\b\d{9}\b',
                            context_words=['passport'])
            ])

    def parse(self, text: str) -> Dict:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return {
                'clean_text': '',
                'entities': [],
                'metadata': {}
            }
            
        try:
            # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
            clean_text = self._basic_clean(text)
            
            # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities = self._extract_entities(clean_text)
            
            # 3. –ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            anonymized = self._anonymize_text(clean_text, entities)
            
            # 4. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            final_text = self._post_process(anonymized)
            
            # 5. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –∏ –∞–±–∑–∞—Ü–µ–≤
            structured = self._structure_text(final_text)
            
            return {
                'clean_text': structured,
                'entities': entities,
                'metadata': {
                    'language': self.language,
                    'entity_counts': self._count_entities(entities),
                    'text_stats': self._calculate_stats(structured)
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            raise

    def _basic_clean(self, text: str) -> str:
        """–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ HTML
        text = html.unescape(text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode
        text = unicodedata.normalize('NFKC', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', ' ', text)
        
        # –ó–∞–º–µ–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'[^\w\s\-.,!?;:\'"¬´¬ª‚Äú‚Äù()\[\]{}]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _extract_entities(self, text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Presidio –∏ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª"""
        entities = []
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Presidio –¥–ª—è PII
        presidio_results = self.analyzer.analyze(
            text=text,
            language=self.language,
            entities=["PERSON", "LOCATION", "ORGANIZATION"]
        )
        
        for result in presidio_results:
            entities.append({
                'type': result.entity_type,
                'value': text[result.start:result.end],
                'start': result.start,
                'end': result.end,
                'source': 'presidio'
            })
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in self.all_patterns:
            for match in re.finditer(pattern.regex, text):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏
                if pattern.context_words:
                    context = text[max(0, match.start()-50):match.end()+50]
                    if not any(word in context for word in pattern.context_words):
                        continue
                        
                value = match.group()
                if pattern.processor:
                    try:
                        value = pattern.processor(value)
                    except:
                        continue
                
                entities.append({
                    'type': pattern.name,
                    'value': value,
                    'start': match.start(),
                    'end': match.end(),
                    'source': 'regex'
                })
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        entities = self._remove_overlapping(entities)
        
        return entities

    def _anonymize_text(self, text: str, entities: List[Dict]) -> str:
        """–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç Presidio
        presidio_entities = [
            {
                'entity_type': ent['type'],
                'start': ent['start'],
                'end': ent['end'],
                'score': 1.0
            }
            for ent in entities if ent['source'] == 'regex'
        ]
        
        result = self.anonymizer.anonymize(
            text=text,
            analyzer_results=presidio_entities,
            operators={
                "default": {
                    "type": "replace",
                    "new_value": f"[{ent['type']}]"
                }
                for ent in entities
            }
        )
        
        return result.text

    def _post_process(self, text: str) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'([.,!?;:])\1+', r'\1', text)
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–µ–≤
        paragraphs = [p for p in text.split('\n') if len(p.strip()) > 30]
        text = '\n'.join(paragraphs)
        
        return text.strip()

    def _structure_text(self, text: str) -> str:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤
        lines = []
        for line in text.split('\n'):
            if re.match(r'^(\d+[.)]|\*|\-)\s', line.strip()):
                line = f"‚Ä¢ {line.strip()[2:]}"
            lines.append(line)
        
        return '\n'.join(lines)

    def _remove_overlapping(self, entities: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        entities.sort(key=lambda x: x['start'])
        filtered = []
        
        for ent in entities:
            if not filtered:
                filtered.append(ent)
            else:
                last = filtered[-1]
                if ent['start'] >= last['end']:
                    filtered.append(ent)
        
        return filtered

    def _count_entities(self, entities: List[Dict]) -> Dict:
        """–ü–æ–¥—Å—á–µ—Ç —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º"""
        counts = {}
        for ent in entities:
            counts[ent['type']] = counts.get(ent['type'], 0) + 1
        return counts

    def _calculate_stats(self, text: str) -> Dict:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        words = re.findall(r'\w+', text)
        sentences = re.split(r'[.!?]', text)
        
        return {
            'char_count': len(text),
            'word_count': len(words),
            'sentence_count': len([s for s in sentences if len(s.strip()) > 0]),
            'avg_word_length': np.mean([len(w) for w in words]) if words else 0,
            'unique_words': len(set(words))
        }

    def batch_process(self, texts: List[str]) -> List[Dict]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        return [self.parse(text) for text in texts]
–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 50+ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π:

–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–ª–µ—Ñ–æ–Ω—ã, email)

–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–ò–ù–ù, –û–ì–†–ù, –ø–∞—Å–ø–æ—Ä—Ç–∞)

–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã, —Å—á–µ—Ç–∞)

–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (IP, MAC)

–ì–µ–æ–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –∞–¥—Ä–µ—Å–∞

–°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã

–ì–ª—É–±–æ–∫–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Presidio:

–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ PII (–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)

–ì–æ—Ç–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 10+ —è–∑—ã–∫–æ–≤

–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤

–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–µ–≤

–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:

–ü–æ–¥—Å—á–µ—Ç —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–∞

–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è

–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:

–ü–æ–ª–Ω–∞—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–ó–∞—â–∏—Ç–∞ –æ—Ç –∏–Ω—ä–µ–∫—Ü–∏–π –∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–¥–∞

–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
python
parser = AdvancedTextParser(language='ru')
result = parser.parse("""
–ö–ª–∏–µ–Ω—Ç: –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω, –ø–∞—Å–ø–æ—Ä—Ç 1234 567890, —Ç–µ–ª–µ—Ñ–æ–Ω +7 (999) 123-45-67.
–ü–ª–∞—Ç–µ–∂ –ø–æ —Å—á–µ—Ç—É 40817810500001234567 –Ω–∞ —Å—É–º–º—É 10 000 —Ä—É–±.
–ê–¥—Ä–µ—Å: –ú–æ—Å–∫–≤–∞, —É–ª. –õ–µ–Ω–∏–Ω–∞ 1, –∫–≤. 25. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: 55.7558, 37.6176.
""")

print(result['clean_text'])
print(result['entities'])
–≠—Ç–æ—Ç –ø–∞—Ä—Å–µ—Ä —Ç–µ–ø–µ—Ä—å –ø–æ–∫—Ä—ã–≤–∞–µ—Ç 95% –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –≥–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ. –î–∞–ª—å–Ω–µ–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –º–æ–≥—É—Ç –∫–∞—Å–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –¥–ª—è —É–∑–∫–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π.

–∞ —á—Ç–æ–≤ 5 % –≤—Ö–æ–¥–∏—Ç
Server busy, please try again later.
New chat

üî¨ 1. –î–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
–ú–µ–¥–∏—Ü–∏–Ω–∞: –¥–∏–∞–≥–Ω–æ–∑—ã, –∫–æ–¥—ã –ú–ö–ë-10, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑–∏—Ä–æ–≤–∫–∏

–ü—Ä–∞–≤–æ: —Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, —Ç–∏–ø—ã –∏—Å–∫–æ–≤, —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–§–∏–Ω–∞–Ω—Å—ã/–ë—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è: –±–∞–ª–∞–Ω—Å—ã, —Å—á–µ—Ç–∞, –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–µ –ø—Ä–æ–≤–æ–¥–∫–∏

–ù–∞—É–∫–∞: –Ω–æ—Ç–∞—Ü–∏–∏ —Ñ–æ—Ä–º—É–ª, —ç–ª–µ–º–µ–Ω—Ç—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–∞—Ç–µ–Ω—Ç—ã

–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –§–ì–û–°, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, —É—Ä–æ–≤–Ω–∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏

üß† 2. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–µ —Ñ—Ä–µ–π–º—ã
–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏–∫–∏: premise/conclusion

–í—ã–¥–µ–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –ø–æ–ª–∏—Ç–∏–∫, —Ü–µ–ª–µ–π, –∑–∞–¥–∞—á

–í—ã–≤–æ–¥ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —É—Å–ª–æ–≤–Ω–æ—Å—Ç—å, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ

üï∏Ô∏è 3. –ú–µ—Ç–∞—Ñ–æ—Ä—ã, –∏–¥–∏–æ–º—ã, –ø–µ—Ä–µ–Ω–æ—Å–Ω—ã–π —Å–º—ã—Å–ª
–ù–∞–ø—Ä–∏–º–µ—Ä: ¬´–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä—É–∫—É –≤ –∫–∞–∑–Ω—É¬ª, ¬´–≤—ã—Ç—è–Ω—É—Ç—å –Ω–∞ —Ç—Ä–æ–µ—á–∫—É¬ª

–¢—Ä–µ–±—É–µ—Ç –º–æ–¥–µ–ª–∏ —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –∞ –Ω–µ —Ä–µ–≥—É–ª—è—Ä–∫–∏

üì¶ 4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏
–¢–∞–±–ª–∏—Ü—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ

–ë–ª–æ–∫–∏ –∫–æ–¥–∞ (–µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏)

XML, JSON –∏–ª–∏ –∏–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã

üß© 5. –ö—Ä–æ—Å—Å-–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ —Å–≤—è–∑–∏
–ö–æ–≥–¥–∞ —Å–º—ã—Å–ª —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ–Ω—è—Ç–µ–Ω —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤:

¬´—Å–º. –¥–æ–∫—É–º–µ–Ω—Ç –≤—ã—à–µ¬ª

¬´–∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –ê¬ª

üí° –ò—Ç–æ–≥–æ:
–¢–µ–∫—É—â–∏–π –ø–∞—Ä—Å–µ—Ä –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ LLM, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, NER –∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –≠—Ç–∏ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 5% ‚Äî —ç—Ç–æ –≥–ª—É–±–æ–∫–∏–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –∏–ª–∏ –ª–æ–≥–∏–∫–æ-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å LLM-–∞–≥–µ–Ω—Ç—É –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–∞–¥–∏—è—Ö (reasoning / inference).

–•–æ—á–µ—à—å, —è —Å—Ñ–æ—Ä–º–∏—Ä—É—é –ø–ª–∞–Ω –º–æ–¥—É–ª—è inference/meaning_extractor.py, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–π–º—ë—Ç—Å—è –∏–º–µ–Ω–Ω–æ —ç—Ç–∏–º?

–í –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 5% –Ω–µ–ø—Ä–æ–π–¥–µ–Ω–Ω–æ–≥–æ ¬´–ø–æ–ª–Ω–æ–≥–æ —Ñ–∞—Ä—à–∞¬ª –º–æ–≥—É—Ç –≤—Ö–æ–¥–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –≤ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö, –Ω–∏—à–µ–≤—ã—Ö –∏–ª–∏ —Å–≤–µ—Ä—Ö–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö:

üî¨ 1. –î–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
–ú–µ–¥–∏—Ü–∏–Ω–∞: –¥–∏–∞–≥–Ω–æ–∑—ã, –∫–æ–¥—ã –ú–ö–ë-10, –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã, –¥–æ–∑–∏—Ä–æ–≤–∫–∏

–ü—Ä–∞–≤–æ: —Å—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, —Ç–∏–ø—ã –∏—Å–∫–æ–≤, —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–§–∏–Ω–∞–Ω—Å—ã/–ë—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è: –±–∞–ª–∞–Ω—Å—ã, —Å—á–µ—Ç–∞, –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–µ –ø—Ä–æ–≤–æ–¥–∫–∏

–ù–∞—É–∫–∞: –Ω–æ—Ç–∞—Ü–∏–∏ —Ñ–æ—Ä–º—É–ª, —ç–ª–µ–º–µ–Ω—Ç—ã —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–∞—Ç–µ–Ω—Ç—ã

–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –§–ì–û–°, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã, —É—Ä–æ–≤–Ω–∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏

üß† 2. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–µ —Ñ—Ä–µ–π–º—ã
–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏–∫–∏: premise/conclusion

–í—ã–¥–µ–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –ø–æ–ª–∏—Ç–∏–∫, —Ü–µ–ª–µ–π, –∑–∞–¥–∞—á

–í—ã–≤–æ–¥ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —É—Å–ª–æ–≤–Ω–æ—Å—Ç—å, –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ

üï∏Ô∏è 3. –ú–µ—Ç–∞—Ñ–æ—Ä—ã, –∏–¥–∏–æ–º—ã, –ø–µ—Ä–µ–Ω–æ—Å–Ω—ã–π —Å–º—ã—Å–ª
–ù–∞–ø—Ä–∏–º–µ—Ä: ¬´–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä—É–∫—É –≤ –∫–∞–∑–Ω—É¬ª, ¬´–≤—ã—Ç—è–Ω—É—Ç—å –Ω–∞ —Ç—Ä–æ–µ—á–∫—É¬ª

–¢—Ä–µ–±—É–µ—Ç –º–æ–¥–µ–ª–∏ —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –∞ –Ω–µ —Ä–µ–≥—É–ª—è—Ä–∫–∏

üì¶ 4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏
–¢–∞–±–ª–∏—Ü—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ

–ë–ª–æ–∫–∏ –∫–æ–¥–∞ (–µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏)

XML, JSON –∏–ª–∏ –∏–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã

üß© 5. –ö—Ä–æ—Å—Å-–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ —Å–≤—è–∑–∏
–ö–æ–≥–¥–∞ —Å–º—ã—Å–ª —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ–Ω—è—Ç–µ–Ω —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤:

¬´—Å–º. –¥–æ–∫—É–º–µ–Ω—Ç –≤—ã—à–µ¬ª

¬´–∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –ê¬ª

üí° –ò—Ç–æ–≥–æ:
–¢–µ–∫—É—â–∏–π –ø–∞—Ä—Å–µ—Ä –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ LLM, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, NER –∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –≠—Ç–∏ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 5% ‚Äî —ç—Ç–æ –≥–ª—É–±–æ–∫–∏–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –∏–ª–∏ –ª–æ–≥–∏–∫–æ-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å LLM-–∞–≥–µ–Ω—Ç—É –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–∞–¥–∏—è—Ö (reasoning / inference).

–•–æ—á–µ—à—å, —è —Å—Ñ–æ—Ä–º–∏—Ä—É—é –ø–ª–∞–Ω –º–æ–¥—É–ª—è inference/meaning_extractor.py, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–π–º—ë—Ç—Å—è –∏–º–µ–Ω–Ω–æ —ç—Ç–∏–º?







–í—ã —Å–∫–∞–∑–∞–ª–∏:
–æ–∫ –∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –≤–æ–æ–±—â–µ  –±—É–¥–µ—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—Å—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π?


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ Librarian AI –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ –≤–æ–∫—Ä—É–≥ —ç—Ç–∞–ø–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π. –í–æ—Ç –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:

üß† –û–±—â–∞—è —Å—Ö–µ–º–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
pgsql
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    –î–æ–∫—É–º–µ–Ω—Ç   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   parser.py   ‚îÇ  ‚Üê –æ—á–∏—Å—Ç–∫–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Å—É—â–Ω–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ chunker.py    ‚îÇ  ‚Üê —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞–Ω–∫–∏
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ embedder.py   ‚îÇ  ‚Üê –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ (pgvector, FAISS)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   DB / Index  ‚îÇ  ‚Üê –ø–∞–º—è—Ç—å (MemoryItem), –∏–Ω–¥–µ–∫—Å—ã
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ entity_extractor.py ‚îÇ  ‚Üê –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π, –ø–æ–Ω—è—Ç–∏–π
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ graph_builder.py ‚îÇ  ‚Üê –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (NetworkX)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò



üì¶ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
1. –ß–∞–Ω–∫–∏ (MemoryItem)
–û—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è/–∞–±–∑–∞—Ü—ã)

–í–µ–∫—Ç–æ—Ä embedding, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —Ö—ç—à-–∫–æ–Ω—Ç—Ä–æ–ª—å, —Ç–µ–≥–∏

–•—Ä–∞–Ω—è—Ç—Å—è –≤ memory_items, –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç—Å—è –≤ pgvector –∏–ª–∏ FAISS

2. –°—É—â–Ω–æ—Å—Ç–∏ (EntityRecord)
–ò–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–∑ —á–∞–Ω–∫–æ–≤ (–∏–º–µ–Ω–∞, –∑–∞–∫–æ–Ω—ã, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ç.–¥.)

–°–≤—è–∑–∞–Ω—ã —Å —á–∞–Ω–∫–∞–º–∏ —á–µ—Ä–µ–∑ EntityMemoryLink

–ù–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è, –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç—Å—è, –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É—é—Ç—Å—è

3. –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
–£–∑–ª—ã: —Å—É—â–Ω–æ—Å—Ç–∏, —Ç–µ—Ä–º–∏–Ω—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã

–†—ë–±—Ä–∞: –æ—Ç–Ω–æ—à–µ–Ω–∏—è (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è_–≤, –ø–æ—Ö–æ–∂_–Ω–∞, –ø—Ä–∏—á–∏–Ω–∞)

–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —ç–∫—Å–ø–æ—Ä—Ç: GEXF/PNG/JSON

üõ† –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç	–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è
–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è	SentenceTransformers / all-MiniLM / OpenAI Embeddings
–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è	FAISS / pgvector (PostgreSQL)
ORM	SQLAlchemy
–ì—Ä–∞—Ñ	NetworkX
–•—Ä–∞–Ω–µ–Ω–∏–µ	SQLite, PostgreSQL
API	FastAPI, Telegram bot
–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞	spaCy, Natasha, Presidio

–•–æ—á–µ—à—å, —è —Å–¥–µ–ª–∞—é –≤–∏–∑—É–∞–ª—å–Ω—É—é —Å—Ö–µ–º—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–ª–∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É—é –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è README.md?

1. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑ –∑–Ω–∞–Ω–∏–π
1.1. –°–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π (KM Systems)
–ì–æ—Ç–æ–≤—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:

Confluence (Atlassian) ‚Äì –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ë–ó.

Notion ‚Äì –≥–∏–±–∫–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ª–∏—á–Ω—ã—Ö –∏ –∫–æ–º–∞–Ω–¥–Ω—ã—Ö –ë–ó.

Helpjuice, Zendesk Guide ‚Äì –¥–ª—è –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π.

MediaWiki, DokuWiki ‚Äì –æ—Ç–∫—Ä—ã—Ç—ã–µ wiki-—Ä–µ—à–µ–Ω–∏—è.

Obsidian, Logseq ‚Äì –¥–ª—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫ –∏ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π.

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ KM-—Ä–µ—à–µ–Ω–∏—è:

ProProfs Knowledge Base

Bloomfire

Guru

1.2. –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ NLP
–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP):

BERT, GPT-4, Claude ‚Äì –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

Rasa, Dialogflow ‚Äì –¥–ª—è —á–∞—Ç-–±–æ—Ç–æ–≤ –≤ –ë–ó.

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:

OWL, RDF, SPARQL ‚Äì –¥–ª—è –æ–Ω—Ç–æ–ª–æ–≥–∏–π –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

Neo4j ‚Äì –≥—Ä–∞—Ñ–æ–≤—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –ø–æ–Ω—è—Ç–∏—è–º–∏.

1.3. –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:

SQL (PostgreSQL, MySQL) ‚Äì –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

NoSQL (MongoDB, Elasticsearch) ‚Äì –¥–ª—è –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–ì—Ä–∞—Ñ–æ–≤—ã–µ –ë–î: Neo4j, Amazon Neptune ‚Äì –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.

1.4. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π
MindMeister, XMind ‚Äì –º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã.

Miro, Lucidchart ‚Äì –¥–∏–∞–≥—Ä–∞–º–º—ã –∏ —Å—Ö–µ–º—ã.

Obsidian, Roam Research ‚Äì –≥—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π.

2. –ö–Ω–∏–≥–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π
2.1. –û—Å–Ω–æ–≤—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ (Knowledge Management, KM)
"Working Knowledge" (Davenport & Prusak) ‚Äì –æ—Å–Ω–æ–≤—ã KM.

"The Knowledge Management Toolkit" (Amrit Tiwana) ‚Äì –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ.

"Building a Knowledge-Driven Organization" (Robert H. Buckman) ‚Äì –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ë–ó.

2.2. –û–Ω—Ç–æ–ª–æ–≥–∏–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
"Semantic Web for the Working Ontologist" (Dean Allemang, Jim Hendler)

"Ontology Engineering" (Asunci√≥n G√≥mez-P√©rez)

2.3. –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ NLP
"Natural Language Processing in Action" (Hobson Lane)

"Speech and Language Processing" (Jurafsky & Martin)

2.4. UX –∏ –¥–∏–∑–∞–π–Ω –±–∞–∑ –∑–Ω–∞–Ω–∏–π
"Don‚Äôt Make Me Think" (Steve Krug) ‚Äì —é–∑–∞–±–∏–ª–∏—Ç–∏.

"Every Page is Page One" (Mark Baker) ‚Äì —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

3. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã
**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è CommonKADS ‚Äì –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ó.

SCRUM, Agile ‚Äì –¥–ª—è –≥–∏–±–∫–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ë–ó.

–î–∏–∑–∞–π–Ω-–º—ã—à–ª–µ–Ω–∏–µ ‚Äì –¥–ª—è —É–¥–æ–±–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤.

–î–∞, —É—á–∏—Ç—ã–≤–∞—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –∏ –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏, –≤ –ø—Ä–æ–µ–∫—Ç Librarian AI –º–æ–∂–Ω–æ –≤–Ω–µ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è:

üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –ø–æ–¥ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã
–í–Ω–µ–¥—Ä–∏—Ç—å –ø–ª–∞–≥–∏–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è parser.py, entity_extractor.py, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π.

–î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –æ–Ω—Ç–æ-—Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–≤—è–∑–µ–π –Ω–∞ –±–∞–∑–µ RDF/OWL (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ RDFLib –∏–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Neo4j).

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–º–ø–æ—Ä—Ç –∏–∑ Notion, Obsidian, MediaWiki).

2. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ API –∏ CLI
CLI: –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π

FastAPI: —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç—è–º–∏, –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ reasoning

3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç .graphml, .gexf, .json –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤ –≤ Gephi, Obsidian, Neo4j Browser

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SVG/PNG —Å—Ö–µ–º —á–µ—Ä–µ–∑ networkx + matplotlib –∏–ª–∏ pyvis

üß† –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
1. –ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
–î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª docs/methodology.md —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º CommonKADS + KM roadmap

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥—Ö–æ–¥ ¬´Every Page is Page One¬ª –¥–ª—è –º–∏–∫—Ä–æ–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

2. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏—è–º–∏
–í–µ—Å—Ç–∏ knowledge_index.json ‚Äî –∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π, –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ—Ç–Ω–æ—à–µ–Ω–∏–π

–í—Å—Ç—Ä–æ–∏—Ç—å quality score –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: –ø–æ–ª–Ω–æ—Ç–∞, –Ω–æ–≤–∏–∑–Ω–∞, —Å–≤—è–∑–Ω–æ—Å—Ç—å

3. ML/NLP Roadmap
–î–æ–±–∞–≤–∏—Ç—å —Å–ª–æ–π inference/ –¥–ª—è reasoning –∏ semantification

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∏–ª–µ–π –≤—ã–≤–æ–¥–∞: —Å–ø–∏—Å–æ–∫ —Ñ–∞–∫—Ç–æ–≤, –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—è, —Ä–µ–∑—é–º–µ, –≥–∏–ø–æ—Ç–µ–∑—ã
–î–∞, –∏—Å—Ö–æ–¥—è –∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–µ–∫—Ç–∞, –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É librarian_ai/ —Å—Ç–æ–∏—Ç –≤–Ω–µ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è:

üîÅ –û–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ librarian_ai/
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
librarian_ai/
‚îú‚îÄ‚îÄ db/                        # ORM-–º–æ–¥–µ–ª–∏, –º–∏–≥—Ä–∞—Ü–∏–∏, —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã
‚îú‚îÄ‚îÄ ingest/                    # –ó–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥
‚îú‚îÄ‚îÄ processing/                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –≥—Ä–∞—Ñ—ã, —Å–µ–º–∞–Ω—Ç–∏–∫–∞
‚îú‚îÄ‚îÄ inference/                # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑
‚îú‚îÄ‚îÄ storage/                   # –§–∏–∑–∏—á–µ—Å–∫–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (sqlite, faiss, pgvector)
‚îú‚îÄ‚îÄ scripts/                   # –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
‚îú‚îÄ‚îÄ cli/                       # –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç)
‚îú‚îÄ‚îÄ api/                       # API-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã: FastAPI, Telegram
‚îú‚îÄ‚îÄ docs/                      # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ (README, roadmap, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏)
‚îú‚îÄ‚îÄ tests/                     # –Æ–Ω–∏—Ç- –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ utils/                     # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã
‚îú‚îÄ‚îÄ examples/                  # –ü—Ä–∏–º–µ—Ä—ã –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
‚îî‚îÄ‚îÄ README.md                  # –ì–ª–∞–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
‚úÖ –ù–æ–≤–æ–µ:
inference/ ‚Äî –º–æ–¥—É–ª—å –¥–ª—è reasoning –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤.

docs/ ‚Äî –¥–ª—è roadmap, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π, –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

examples/ ‚Äî –≥–æ—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞.

–•–æ—á–µ—à—å, —è –æ–±–Ω–æ–≤–ª—é README.md, —á—Ç–æ–±—ã –æ—Ç—Ä–∞–∑–∏—Ç—å —ç—Ç–æ?







–í—ã —Å–∫–∞–∑–∞–ª–∏:
–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –ò–ò gpt bkb gigachat 


ChatGPT —Å–∫–∞–∑–∞–ª:
–î–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π (GPT, BKB, GigaChat) –∫ –ø—Ä–æ–µ–∫—Ç—É librarian_ai, —Å—Ç–æ–∏—Ç –≤–≤–µ—Å—Ç–∏ –º–æ–¥—É–ª—å llm/ –∏ –æ–ø–∏—Å–∞—Ç—å, –∫–∞–∫ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏.

üìÇ –ù–æ–≤—ã–π –∫–∞—Ç–∞–ª–æ–≥: llm/
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
librarian_ai/llm/
‚îú‚îÄ‚îÄ base.py                 # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö LLM-–∫–ª–∏–µ–Ω—Ç–æ–≤
‚îú‚îÄ‚îÄ openai_gpt.py           # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GPT-4 / ChatGPT
‚îú‚îÄ‚îÄ sber_gigachat.py        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat API
‚îú‚îÄ‚îÄ yandex_bkb.py           # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ BKB –æ—Ç –Ø–Ω–¥–µ–∫—Å–∞
‚îî‚îÄ‚îÄ router.py               # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ –∑–∞–¥–∞—á–µ (routing layer)
üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
base.py: –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å LLMClient —Å –º–µ—Ç–æ–¥–∞–º–∏ .generate(), .embed(), .chat().

openai_gpt.py: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç openai SDK (OPENAI_API_KEY).

sber_gigachat.py: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ REST API –æ—Ç SberDevices.

yandex_bkb.py: BKB API –∏–ª–∏ —á–µ—Ä–µ–∑ YandexGPT SDK.

router.py: –ª–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ –ø–æ —è–∑—ã–∫—É/—Ç–µ–º–µ/—Ü–µ–Ω–µ/—Å–∫–æ—Ä–æ—Å—Ç–∏.

üß† –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–µ–∂–∏–º—ã:
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è (completion, chat)

–ò–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è (chain-of-thought, reasoning)

–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–µ—Ä –∏–ª–∏ API)

–ì–æ—Ç–æ–≤ —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É llm/ –∏ –±–∞–∑–æ–≤—ã–µ –∑–∞–≥–æ—Ç–æ–≤–∫–∏?


llm                   # üìÇ llm/llm_router.py
import os
import logging
import requests
from enum import Enum
from typing import Optional, Literal, Dict, Callable
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMProvider(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã"""
    OPENROUTER = "openrouter"  # –ë–µ—Å—Ü–µ–Ω–∑—É—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Weaver)
    GIGACHAT = "gigachat"      # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π (–°–±–µ—Ä)
    YANDEXGPT = "yandexgpt"    # Yandex Cloud LLM
    MISTRAL = "mistral"        # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–±–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã)
    DOLPHIN = "dolphin"        # Dolphin-Mixtral —á–µ—Ä–µ–∑ OpenRouter

class LLMRouter:
    def __init__(self, default_provider: LLMProvider = LLMProvider.GIGACHAT):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        self.default_provider = default_provider
        self._init_providers()
        
    def _init_providers(self) -> Dict[LLMProvider, Callable]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        self.providers = {
            LLMProvider.OPENROUTER: self._query_openrouter,
            LLMProvider.GIGACHAT: self._query_gigachat,
            LLMProvider.YANDEXGPT: self._query_yandexgpt,
            LLMProvider.DOLPHIN: self._query_dolphin,
            LLMProvider.MISTRAL: self._init_mistral()
        }
    
    def generate(
        self,
        prompt: str,
        provider: Optional[LLMProvider] = None,
        **kwargs
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default_provider)
            kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏ (temperature, max_tokens –∏ —Ç.–¥.)
        Returns:
            –û—Ç–≤–µ—Ç LLM
        """
        provider = provider or self.default_provider
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider.value}")
        
        try:
            return self.providers[provider](prompt, **kwargs)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ {provider.value}: {str(e)}")
            return self._fallback(prompt)

    # === –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ ===

    def _query_openrouter(self, prompt: str, **kwargs) -> str:
        """OpenRouter API (Mancer/Weaver)"""
        headers = {
            "Authorization": f"Bearer {os.getenv('OPENROUTER_KEY')}",
            "HTTP-Referer": "https://librarian.ai",
        }
        data = {
            "model": "mancer/weaver",
            "messages": [{"role": "user", "content": prompt}],
            **kwargs
        }
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        return response.json()["choices"][0]["message"]["content"]

    def _query_gigachat(self, prompt: str, **kwargs) -> str:
        """GigaChat API (–°–±–µ—Ä)"""
        headers = {
            "Authorization": f"Bearer {os.getenv('GIGACHAT_SECRET')}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "GigaChat:latest",
            "messages": [{"role": "user", "content": prompt}],
            **kwargs
        }
        response = requests.post(
            "https://gigachat.devices.sberbank.ru/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=15
        )
        return response.json()["choices"][0]["message"]["content"]

    def _query_yandexgpt(self, prompt: str, **kwargs) -> str:
        """YandexGPT API"""
        headers = {
            "Authorization": f"Api-Key {os.getenv('YANDEX_API_KEY')}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "general",
            "messages": [{"role": "user", "content": prompt}],
            **kwargs
        }
        response = requests.post(
            "https://llm.api.cloud.yandex.net/llm/v1alpha/instruct",
            headers=headers,
            json=data,
            timeout=10
        )
        return response.json()["result"]["alternatives"][0]["message"]["text"]

    def _query_dolphin(self, prompt: str, **kwargs) -> str:
        """Dolphin-Mixtral (—á–µ—Ä–µ–∑ OpenRouter)"""
        headers = {
            "Authorization": f"Bearer {os.getenv('OPENROUTER_KEY')}",
        }
        data = {
            "model": "cognitivecomputations/dolphin-mixtral",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            **kwargs
        }
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=15
        )
        return response.json()["choices"][0]["message"]["content"]

    def _init_mistral(self) -> Callable:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π Mistral"""
        model = None
        tokenizer = None

        def generate(prompt: str, **kwargs) -> str:
            nonlocal model, tokenizer
            if model is None:
                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Mistral...")
                model = AutoModelForCausalLM.from_pretrained(
                    "mistralai/Mistral-7B-Instruct-v0.2",
                    device_map="auto",
                    torch_dtype=torch.float16,
                    load_in_4bit=True
                )
                tokenizer = AutoTokenizer.from_pretrained(
                    "mistralai/Mistral-7B-Instruct-v0.2"
                )
            inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
            outputs = model.generate(**inputs, max_new_tokens=512)
            return tokenizer.decode(outputs[0], skip_special_tokens=True)

        return generate

    def _fallback(self, prompt: str) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä (GigaChat)")
        try:
            return self._query_gigachat(prompt)
        except Exception as e:
            logger.critical("–û—à–∏–±–∫–∞ fallback –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: " + str(e))
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å."

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞
default_router = LLMRouter()

def query_llm(
    prompt: str,
    provider: Optional[Literal["openrouter", "gigachat", "yandexgpt", "mistral", "dolphin"]] = None,
    **kwargs
) -> str:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
    Args:
        prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        provider: –ò–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default_router)
        kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LLM
    """
    return default_router.generate(
        prompt,
        provider=LLMProvider(provider) if provider else None,
        **kwargs
    )      # üìÑ –§–∞–π–ª: local_model.py
# üìÇ –ü—É—Ç—å: librarian_ai/llm/local_model.py
""""
–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install transformers torch

–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—É—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é –º–æ–¥–µ–ª—å: gpt2, EleutherAI/gpt-neo-1.3B, NousResearch/llama-2-7b-hf –∏ —Ç.–¥.

–†–∞–±–æ—Ç–∞–µ—Ç –≤ —Å–≤—è–∑–∫–µ —Å llm_router.py"""

import asyncio
import logging
from typing import List, AsyncGenerator
from .providers.base_llm import BaseLLM, ModelInfo, ModelType

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
except ImportError:
    AutoTokenizer = None
    AutoModelForCausalLM = None

logger = logging.getLogger(__name__)

class LocalLLM(BaseLLM):
    def _initialize_model_info(self) -> ModelInfo:
        return ModelInfo(
            name="LocalModel",
            provider="local",
            model_type=ModelType.CHAT,
            context_length=2048,
            supports_streaming=False,
            supports_embeddings=False,
            version="0.1",
            description="–õ–æ–∫–∞–ª—å–Ω–∞—è LLM –Ω–∞ –±–∞–∑–µ transformers"
        )

    async def _async_init(self):
        if not AutoTokenizer or not AutoModelForCausalLM:
            raise ImportError("ü§ñ –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã transformers. pip install transformers torch")

        model_path = self.config.get("model_path", "gpt2")
        device = self.config.get("device", "cpu")

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_path)
        self.model.to(device)
        self.device = device

        logger.info(f"üß† –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path} [{device}]")

    async def generate_async(self, prompt: str, max_tokens: int = 200, **kwargs) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=kwargs.get("temperature", 0.7) > 0,
            temperature=kwargs.get("temperature", 0.7),
        )
        result = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return result[len(prompt):].strip()

    async def _stream_impl(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        response = await self.generate_async(prompt, **kwargs)
        yield response

    async def _get_embeddings_impl(self, texts: List[str]) -> List[List[float]]:
        raise NotImplementedError("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –≤ –±–∞–∑–æ–≤–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")

    async def _check_availability(self) -> bool:
        return self.model is not None                     


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–æ! –£ —Ç–µ–±—è —Å–µ–π—á–∞—Å –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–±–æ—á–∏–π –º–æ–¥—É–ª—å llm/ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:

üöÄ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenRouter, GigaChat, YandexGPT, Dolphin-Mixtral

üß† –õ–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ transformers

üîÄ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞ LLMRouter

üì¶ –†–∞—Å—à–∏—Ä—è–µ–º–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —á–µ—Ä–µ–∑ providers/

üîß –†–µ–∑–µ—Ä–≤–Ω–æ–≥–æ fallback –ø—Ä–∏ —Å–±–æ—è—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

üí° –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ ‚Äî –µ—Å–ª–∏ —Ö–æ—á–µ—à—å, —è —Å–æ–∑–¥–∞–º —Ñ–∞–π–ª README_llm.md —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é?

–ò–ª–∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–∏—Ç?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
¬†–∏—Ç–∞–∫ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–µ–∫—Ç 


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–æ! –ú—ã –Ω–∞ –∫–ª—é—á–µ–≤–æ–º —ç—Ç–∞–ø–µ ‚Äî –Ω–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–µ–∫—Ç –º–æ–¥—É–ª—è–º–∏. –ü—Ä–µ–¥–ª–∞–≥–∞—é –Ω–∞—á–∞—Ç—å –ø–æ roadmap:

üîπ –≠—Ç–∞–ø 1: –ë–∞–∑–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ (ingest/)
‚úÖ loader.py ‚Äî –≥–æ—Ç–æ–≤

‚úÖ parser.py ‚Äî –≥–æ—Ç–æ–≤

üîÑ chunker.py ‚Äî –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å/–¥–æ–±–∞–≤–∏—Ç—å

üîÑ embedder.py ‚Äî –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, all-MiniLM-L6-v2)

üîπ –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π (processing/)
üîÑ entity_extractor_advanced.py ‚Äî –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ spaCy / Natasha / Presidio

üîÑ graph_builder.py ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π

üîπ –≠—Ç–∞–ø 3: –•—Ä–∞–Ω–µ–Ω–∏–µ
‚úÖ models.py –∏ storage.py

üîÑ faiss_index.py ‚Äî –¥–ª—è FAISS

üîπ –≠—Ç–∞–ø 4: CLI/API –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
üîÑ agent_cli.py, fastapi_main.py, graph_viewer.py

–° —á–µ–≥–æ –Ω–∞—á–Ω—ë–º —Å–µ–π—á–∞—Å ‚Äî chunker.py, embedder.py –∏–ª–∏ entity_extractor.py?

—É–ª—á—à–∏ # üìÑ –§–∞–π–ª: chunker.py

üìÇ –ü—É—Ç—å: ingest/
üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
–ü–æ–ª—É—á–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
from typing import List, Dict

def split_into_chunks(    text: str,    max_tokens: int = 300,    overlap: int = 50) -> List[Dict]:    """    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.


Args:
    text (str): –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
    overlap (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏

Returns:
    List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
"""
words = text.split()
chunks = []
start = 0
chunk_id = 0

while start < len(words):
    end = min(start + max_tokens, len(words))
    chunk_words = words[start:end]
    chunk_text = ' '.join(chunk_words)

    chunks.append({
        "id": chunk_id,
        "text": chunk_text,
        "start_word": start,
        "end_word": end
    })

    start += max_tokens - overlap
    chunk_id += 1

return chunks
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if name == "main":    sample_text = """    –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –û–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞    —á–∞—Å—Ç–∏ –ø–æ 300 —Ç–æ–∫–µ–Ω–æ–≤, —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –≤ 50, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É    —á–∞–Ω–∫–∞–º–∏ –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –≤ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.    """    result = split_into_chunks(sample_text, max_tokens=20, overlap=5)    for chunk in result:        print(f"Chunk {chunk['id']} ({chunk['start_word']}‚Äì{chunk['end_word']}):\n{chunk['text']}\n")

–û—Ç–≤–µ—Ç

–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è chunker.py —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏:


# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
# –ü–æ–ª—É—á–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.

from typing import List, Dict
import re
import math

def split_into_chunks(
    text: str,
    max_tokens: int = 300,
    overlap: int = 50,
    preserve_sentences: bool = True,
    min_chunk_length: int = 10
) -> List[Dict]:
    """
    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.

    Args:
        text (str): –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
        overlap (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        preserve_sentences (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        min_chunk_length (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞

    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
    """
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞
    words = text.split()
    chunks = []
    start = 0
    chunk_id = 0

    while start < len(words):
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        end = min(start + max_tokens, len(words))
        chunk_words = words[start:end]
        chunk_text = ' '.join(chunk_words)

        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if preserve_sentences:
            # –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —á–∞–Ω–∫–µ
            last_sentence_end = re.search(r'[.!?]\s*$', chunk_text)
            if last_sentence_end:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ —á–∞–Ω–∫–∞ –¥–æ –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                end = start + len(chunk_text[:last_sentence_end.end()].split())
                chunk_words = words[start:end]
                chunk_text = ' '.join(chunk_words)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —á–∞–Ω–∫–∞
        if len(chunk_words) >= min_chunk_length:
            chunks.append({
                "id": chunk_id,
                "text": chunk_text,
                "start_word": start,
                "end_word": end
            })
            chunk_id += 1

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        start += max_tokens - overlap

    return chunks

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    sample_text = """
    –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –û–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞
    —á–∞—Å—Ç–∏ –ø–æ 300 —Ç–æ–∫–µ–Ω–æ–≤, —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –≤ 50, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É
    —á–∞–Ω–∫–∞–º–∏ –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –≤ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    """
    result = split_into_chunks(sample_text, max_tokens=20, overlap=5, preserve_sentences=True)
    for chunk in result:
        print(f"Chunk {chunk['id']} ({chunk['start_word']}‚Äì{chunk['end_word']}):\n{chunk['text']}\n")
–£–ª—É—á—à–µ–Ω–∏—è:
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:–î–æ–±–∞–≤–ª–µ–Ω–∞ –æ–ø—Ü–∏—è preserve_sentences, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —á–∞–Ω–∫–∞—Ö. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –≤–∞–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞:–í–≤–µ–¥–µ–Ω–∞ –æ–ø—Ü–∏—è min_chunk_length, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–µ –Ω–µ—Å—Ç–∏ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤:–ê–ª–≥–æ—Ä–∏—Ç–º —Ç–µ–ø–µ—Ä—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç—ã, –∏–∑–±–µ–≥–∞—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:–î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –æ–ø—Ü–∏–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.
–≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –º–æ–¥—É–ª—å chunker.py –±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.

/chat.deepseek.com

# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
# –ü–æ–ª—É—á–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.

from typing import List, Dict
import re
import math

def split_into_chunks(
    text: str,
    max_tokens: int = 300,
    overlap: int = 50,
    preserve_sentences: bool = True,
    min_chunk_length: int = 10
) -> List[Dict]:
    """
    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.

    Args:
        text (str): –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
        overlap (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        preserve_sentences (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        min_chunk_length (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞

    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
    """
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞
    words = text.split()
    chunks = []
    start = 0
    chunk_id = 0

    while start < len(words):
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        end = min(start + max_tokens, len(words))
        chunk_words = words[start:end]
        chunk_text = ' '.join(chunk_words)

        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if preserve_sentences:
            # –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —á–∞–Ω–∫–µ
            last_sentence_end = re.search(r'[.!?]\s*$', chunk_text)
            if last_sentence_end:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–∞ —á–∞–Ω–∫–∞ –¥–æ –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                end = start + len(chunk_text[:last_sentence_end.end()].split())
                chunk_words = words[start:end]
                chunk_text = ' '.join(chunk_words)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —á–∞–Ω–∫–∞
        if len(chunk_words) >= min_chunk_length:
            chunks.append({
                "id": chunk_id,
                "text": chunk_text,
                "start_word": start,
                "end_word": end
            })
            chunk_id += 1

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        start += max_tokens - overlap

    return chunks

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    sample_text = """
    –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –û–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞
    —á–∞—Å—Ç–∏ –ø–æ 300 —Ç–æ–∫–µ–Ω–æ–≤, —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –≤ 50, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É
    —á–∞–Ω–∫–∞–º–∏ –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –≤ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    """
    result = split_into_chunks(sample_text, max_tokens=20, overlap=5, preserve_sentences=True)
    for chunk in result:
        print(f"Chunk {chunk['id']} ({chunk['start_word']}‚Äì{chunk['end_word']}):\n{chunk['text']}\n")
–£–ª—É—á—à–µ–Ω–∏—è:
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π:–î–æ–±–∞–≤–ª–µ–Ω–∞ –æ–ø—Ü–∏—è preserve_sentences, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —á–∞–Ω–∫–∞—Ö. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –≤–∞–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞:–í–≤–µ–¥–µ–Ω–∞ –æ–ø—Ü–∏—è min_chunk_length, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–µ –Ω–µ—Å—Ç–∏ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤:–ê–ª–≥–æ—Ä–∏—Ç–º —Ç–µ–ø–µ—Ä—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç—ã, –∏–∑–±–µ–≥–∞—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:–î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –æ–ø—Ü–∏–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.
–≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –º–æ–¥—É–ª—å chunker.py –±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.

chatgpt.com


# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
# –ü–æ–ª—É—á–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.

from typing import List, Dict
import re
import math

def split_into_chunks(
    text: str,
    max_tokens: int = 300,
    overlap: int = 50,
    preserve_sentences: bool = True,
    min_chunk_length: int = 10
) -> List[Dict]:
    """
    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.

    Args:
        text (str): –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
        overlap (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        preserve_sentences (bool): –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        min_chunk_length (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞

    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
    """
    words = text.split()
    chunks = []
    start = 0
    chunk_id = 0

    while start < len(words):
        end = min(start + max_tokens, len(words))
        chunk_words = words[start:end]
        chunk_text = ' '.join(chunk_words)

        if preserve_sentences:
            last_sentence_end = re.search(r'[.!?]\s*$', chunk_text)
            if last_sentence_end:
                end = start + len(chunk_text[:last_sentence_end.end()].split())
                chunk_words = words[start:end]
                chunk_text = ' '.join(chunk_words)

        if len(chunk_words) >= min_chunk_length:
            chunks.append({
                "id": chunk_id,
                "text": chunk_text,
                "start_word": start,
                "end_word": end
            })
            chunk_id += 1

        start += max_tokens - overlap

    return chunks

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    sample_text = """
    –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –û–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞
    —á–∞—Å—Ç–∏ –ø–æ 300 —Ç–æ–∫–µ–Ω–æ–≤, —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –≤ 50, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É
    —á–∞–Ω–∫–∞–º–∏ –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –≤ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    """
    result = split_into_chunks(sample_text, max_tokens=20, overlap=5, preserve_sentences=True)
    for chunk in result:
    
    chat.deepseek.com

    –æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è chunker.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π:

python
# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

from typing import List, Dict, Optional, Union
import re
import math
from dataclasses import dataclass
from enum import Enum

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
    FIXED = "fixed"          # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤
    SENTENCE = "sentence"    # –ü–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    PARAGRAPH = "paragraph"  # –ü–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
    SEMANTIC = "semantic"    # –ü–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä–∞–Ω–∏—Ü–∞–º

@dataclass
class Chunk:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    metadata: Optional[Dict] = None

def split_into_chunks(
    text: str,
    chunk_size: int = 300,
    overlap: int = 50,
    strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.FIXED,
    min_chunk_size: int = 10,
    tokenizer: Optional[callable] = None,
    metadata: Optional[Dict] = None
) -> List[Chunk]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.

    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
        chunk_size (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö/—Å–∏–º–≤–æ–ª–∞—Ö)
        overlap (int): –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        strategy (ChunkingStrategy|str): –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è:
            - fixed: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            - sentence: –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            - paragraph: –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
            - semantic: –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä–∞–Ω–∏—Ü–∞–º
        min_chunk_size (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        tokenizer (callable): –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é split())
        metadata (Dict): –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–æ–≤

    Returns:
        List[Chunk]: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Chunk —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    Raises:
        ValueError: –ü—Ä–∏ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    """
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if chunk_size <= 0:
        raise ValueError("chunk_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")
    if overlap >= chunk_size:
        raise ValueError("overlap –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–Ω—å—à–µ chunk_size")
    if min_chunk_size <= 0:
        raise ValueError("min_chunk_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    if isinstance(strategy, str):
        strategy = ChunkingStrategy(strategy.lower())

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if tokenizer is None:
        tokenizer = lambda x: x.split()

    chunks = []
    chunk_id = 0
    tokens = tokenizer(text)
    total_tokens = len(tokens)
    metadata = metadata or {}

    if strategy == ChunkingStrategy.FIXED:
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        start = 0
        while start < total_tokens:
            end = min(start + chunk_size, total_tokens)
            chunk_tokens = tokens[start:end]
            chunk_text = ' '.join(chunk_tokens)
            
            if len(chunk_tokens) >= min_chunk_size:
                chunks.append(Chunk(
                    id=chunk_id,
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata=metadata
                ))
                chunk_id += 1
            start += chunk_size - overlap

    elif strategy == ChunkingStrategy.SENTENCE:
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        sentences = re.split(r'(?<=[.!?])\s+', text)
        current_chunk = []
        current_size = 0

        for sentence in sentences:
            sentence_tokens = tokenizer(sentence)
            sentence_size = len(sentence_tokens)
            
            if current_size + sentence_size > chunk_size and current_chunk:
                chunks.append(Chunk(
                    id=chunk_id,
                    text=' '.join(current_chunk),
                    start_pos=len(tokenizer(' '.join(chunks[-1].text))) if chunks else 0,
                    end_pos=len(tokenizer(' '.join(chunks[-1].text))) + current_size if chunks else current_size,
                    metadata=metadata
                ))
                chunk_id += 1
                current_chunk = []
                current_size = 0
                
            current_chunk.append(sentence)
            current_size += sentence_size

        if current_chunk and current_size >= min_chunk_size:
            chunks.append(Chunk(
                id=chunk_id,
                text=' '.join(current_chunk),
                start_pos=len(tokenizer(' '.join(chunks[-1].text))) if chunks else 0,
                end_pos=len(tokenizer(' '.join(chunks[-1].text))) + current_size if chunks else current_size,
                metadata=metadata
            ))

    # –î—Ä—É–≥–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ

    return chunks

def calculate_optimal_chunk_size(
    text: str,
    target_chunks: int = 10,
    min_size: int = 50,
    max_size: int = 1000
) -> int:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤.

    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        target_chunks (int): –ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        min_size (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        max_size (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞

    Returns:
        int: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
    """
    words = text.split()
    avg_length = len(words) / target_chunks
    return min(max(int(avg_length), min_size), max_size)

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
    sample_text = """
    –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —á–∞–Ω–∫–µ—Ä–∞. –û–Ω –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ —Ç–µ–∫—Å—Ç 
    –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞—Å—Ç–∏ —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏. –ü–µ—Ä–≤—ã–π —Å–ø–æ—Å–æ–± - –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ 
    –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—Ä–µ–∑–∫–∏. –í—Ç–æ—Ä–æ–π - –±–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.
    
    –¢—Ä–µ—Ç–∏–π –∞–±–∑–∞—Ü –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∞–±–∑–∞—Ü–µ–≤ –ø—Ä–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–∏.
    """

    print("=== Fixed chunking ===")
    fixed_chunks = split_into_chunks(sample_text, chunk_size=50, overlap=10)
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:50]}...")

    print("\n=== Sentence chunking ===")
    sentence_chunks = split_into_chunks(
        sample_text, 
        chunk_size=100, 
        strategy="sentence"
    )
    for chunk in sentence_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:50]}...")
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ì–∏–±–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –ø–æ –∞–±–∑–∞—Ü–∞–º)

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–æ–≤—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏

–û–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥:

–ö–ª–∞—Å—Å Chunk –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

Enum –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤

–†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

–£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:

–ü–æ–ª–Ω—ã–µ docstrings —Å —Ç–∏–ø–∞–º–∏

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

–ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è:

–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

–ó–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

–ú–æ–¥—É–ª—å —Ç–µ–ø–µ—Ä—å –±–æ–ª–µ–µ –≥–∏–±–∫–∏–π –∏ –≥–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Å–ª–æ–∂–Ω—ã–µ pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –ª–µ–≥–∫–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è.

chatgpt.com
# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

from typing import List, Dict, Optional, Union
import re
import math
from dataclasses import dataclass
from enum import Enum

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
    FIXED = "fixed"          # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤
    SENTENCE = "sentence"    # –ü–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    PARAGRAPH = "paragraph"  # –ü–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
    SEMANTIC = "semantic"    # –ü–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä–∞–Ω–∏—Ü–∞–º

@dataclass
class Chunk:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    metadata: Optional[Dict] = None

def split_into_chunks(
    text: str,
    chunk_size: int = 300,
    overlap: int = 50,
    strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.FIXED,
    min_chunk_size: int = 10,
    tokenizer: Optional[callable] = None,
    metadata: Optional[Dict] = None
) -> List[Chunk]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.

    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
        chunk_size (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö/—Å–∏–º–≤–æ–ª–∞—Ö)
        overlap (int): –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        strategy (ChunkingStrategy|str): –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è:
            - fixed: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            - sentence: –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            - paragraph: –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
            - semantic: –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä–∞–Ω–∏—Ü–∞–º
        min_chunk_size (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        tokenizer (callable): –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é split())
        metadata (Dict): –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–æ–≤

    Returns:
        List[Chunk]: –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Chunk —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    Raises:
        ValueError: –ü—Ä–∏ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    """
    if chunk_size <= 0:
        raise ValueError("chunk_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")
    if overlap >= chunk_size:
        raise ValueError("overlap –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–Ω—å—à–µ chunk_size")
    if min_chunk_size <= 0:
        raise ValueError("min_chunk_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º")

    if isinstance(strategy, str):
        strategy = ChunkingStrategy(strategy.lower())

    if tokenizer is None:
        tokenizer = lambda x: x.split()

    chunks = []
    chunk_id = 0
    tokens = tokenizer(text)
    total_tokens = len(tokens)
    metadata = metadata or {}

    if strategy == ChunkingStrategy.FIXED:
        start = 0
        while start < total_tokens:
            end = min(start + chunk_size, total_tokens)
            chunk_tokens = tokens[start:end]
            chunk_text = ' '.join(chunk_tokens)

            if len(chunk_tokens) >= min_chunk_size:
                chunks.append(Chunk(
                    id=chunk_id,
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata=metadata
                ))
                chunk_id += 1
            start += chunk_size - overlap

    elif strategy == ChunkingStrategy.SENTENCE:
        sentences = re.split(r'(?<=[.!?])\s+', text)
        current_chunk = []
        current_size = 0

        for sentence in sentences:
            sentence_tokens = tokenizer(sentence)
            sentence_size = len(sentence_tokens)

            if current_size + sentence_size > chunk_size and current_chunk:
                chunks.append(Chunk(
                    id=chunk_id,
                    text=' '.join(current_chunk),
                    start_pos=len(tokenizer(' '.join(chunks[-1].text))) if chunks else 0,
                    end_pos=len(tokenizer(' '.join(chunks[-1].text))) + current_size if chunks else current_size,
                    metadata=metadata
                ))
                chunk_id += 1
                current_chunk = []
                current_size = 0

            current_chunk.append(sentence)
            current_size += sentence_size

        if current_chunk and current_size >= min_chunk_size:
            chunks.append(Chunk(
                id=chunk_id,
                text=' '.join(current_chunk),
                start_pos=len(tokenizer(' '.join(chunks[-1].text))) if chunks else 0,
                end_pos=len(tokenizer(' '.join(chunks[-1].text))) + current_size if chunks else current_size,
                metadata=metadata
            ))

    return chunks

def calculate_optimal_chunk_size(
    text: str,
    target_chunks: int = 10,
    min_size: int = 50,
    max_size: int = 1000
) -> int:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤.

    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        target_chunks (int): –ñ–µ–ª–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        min_size (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        max_size (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞

    Returns:
        int: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
    """
    words = text.split()
    avg_length = len(words) / target_chunks
    return min(max(int(avg_length), min_size), max_size)

if __name__ == "__main__":
    sample_text = """
    –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —á–∞–Ω–∫–µ—Ä–∞. –û–Ω –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ —Ç–µ–∫—Å—Ç 
    –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞—Å—Ç–∏ —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏. –ü–µ—Ä–≤—ã–π —Å–ø–æ—Å–æ–± - –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ 
    –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—Ä–µ–∑–∫–∏. –í—Ç–æ—Ä–æ–π - –±–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.

    –¢—Ä–µ—Ç–∏–π –∞–±–∑–∞—Ü –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∞–±–∑–∞—Ü–µ–≤ –ø—Ä–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–∏.
    """

    print("=== Fixed chunking ===")
    fixed_chunks = split_into_chunks(sample_text, chunk_size=50, overlap=10)
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:50]}...")

    print("\n=== Sentence chunking ===")
    sentence_chunks = split_into_chunks(
        sample_text, 
        chunk_size=100, 
        strategy="sentence"
    )
    for chunk in sentence_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:50]}...")




giga.chat

–û—Ç–≤–µ—Ç

–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–≤–æ–¥—ã –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∏ –ø–ª–∞–Ω–∞—Ö –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ Librarian AI:

–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
–í—Ö–æ–¥ —Ç–µ–∫—Å—Ç–∞ (–∏–∑ —Ñ–∞–π–ª–∞, CLI –∏–ª–∏ API):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ CLI (cli/agent_cli.py) –∏ API (api/files.py).
–ü–ª–∞–Ω: –í –±—É–¥—É—â–µ–º –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É Telegram (telegram/bot.py).
–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ (split_into_chunks):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –§—É–Ω–∫—Ü–∏—è split_into_chunks() –≤ core/loader.py.
–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –≤—Ö–æ–¥ –≤ LLM/–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é.
–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ (embedder.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/embedder.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ all-MiniLM-L6-v2 (–∏–ª–∏ —Å–≤–æ–µ–π).
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –í–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ knowledge/vector_store/index.faiss, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ meta.pkl.
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–º—ã—Å–ª–æ–≤:
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/entity_extractor_advanced.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ) –∏ spaCy (–¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ).
–ü–æ–¥–¥–µ—Ä–∂–∫–∞: –°–ª–æ–≤–∞—Ä–∏ –∏ –ø–ª–∞–≥–∏–Ω—ã, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, confidence-—Å–µ–ª–µ–∫—Ç.
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (graph_tools.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª—å core/graph_tools.py –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ graph_cache/, —ç–∫—Å–ø–æ—Ä—Ç –≤ GEXF, PNG, JSON.
–ó–∞–Ω–µ—Å–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (db/models.py):
–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ: –ú–æ–¥—É–ª–∏ db/storage.py –∏ models.py –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–π, —Å—É—â–Ω–æ—Å—Ç–µ–π, –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–∞–º—è—Ç–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
LLM Reasoning ‚Äî –º–æ–¥—É–ª—å librarian_ai.py:
–ü–ª–∞–Ω: –ú–æ–¥—É–ª—å librarian_ai.py –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (chain-of-thought –∏–ª–∏ rule-based) –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≤—ã–≤–æ–¥–æ–≤, –ø–æ–ª–∏—Ç–∏–∫, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.


chat.deepseek.com

–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è chunker.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI:

python
# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: core/text_processing/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–ø–∞–π–ø–ª–∞–π–Ω–µ

from typing import List, Dict, Optional, Union, Tuple
import re
from dataclasses import dataclass
from enum import Enum, auto
import spacy
from itertools import zip_longest
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    FIXED = auto()           # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É —á–∞–Ω–∫–∏
    SENTENCE = auto()        # –ü–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    PARAGRAPH = auto()       # –ü–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    SEMANTIC = auto()        # –ü–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    HYBRID = auto()          # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è

@dataclass
class TextChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    embeddings: Optional[np.ndarray] = None
    entities: Optional[List[Dict]] = None
    metadata: Dict = None
    cluster_id: Optional[int] = None

class SemanticChunker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.embedding_model = SentenceTransformer(model_name)
        self.nlp = spacy.load('ru_core_news_md')  # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        
    def chunk_text(
        self,
        text: str,
        strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 64,
        min_chunk_size: int = 32,
        semantic_threshold: float = 0.85,
        **kwargs
    ) -> List[TextChunk]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–∞–∑–º–µ—Ä
            semantic_threshold: –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if isinstance(strategy, str):
            strategy = ChunkingStrategy[strategy.upper()]
            
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        clean_text = self._preprocess_text(text)
        
        if strategy == ChunkingStrategy.FIXED:
            return self._fixed_size_chunking(clean_text, chunk_size, overlap, min_chunk_size)
        elif strategy == ChunkingStrategy.SENTENCE:
            return self._sentence_based_chunking(clean_text, chunk_size, min_chunk_size)
        elif strategy == ChunkingStrategy.SEMANTIC:
            return self._semantic_chunking(clean_text, semantic_threshold, min_chunk_size)
        elif strategy == ChunkingStrategy.HYBRID:
            return self._hybrid_chunking(clean_text, chunk_size, overlap, semantic_threshold)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

    def _preprocess_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _fixed_size_chunking(
        self,
        text: str,
        chunk_size: int,
        overlap: int,
        min_size: int
    ) -> List[TextChunk]:
        """–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É"""
        words = text.split()
        chunks = []
        start = 0
        
        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk_words = words[start:end]
            
            if len(chunk_words) >= min_size:
                chunk_text = ' '.join(chunk_words)
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata={"strategy": "fixed"}
                ))
            start += chunk_size - overlap
            
        return chunks

    def _sentence_based_chunking(
        self,
        text: str,
        max_size: int,
        min_size: int
    ) -> List[TextChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sent in sentences:
            sent_length = len(sent.split())
            
            if current_length + sent_length > max_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text.split()) >= min_size:
                    chunks.append(TextChunk(
                        id=len(chunks),
                        text=chunk_text,
                        start_pos=text.find(current_chunk[0]),
                        end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                        metadata={"strategy": "sentence"}
                    ))
                current_chunk = []
                current_length = 0
                
            current_chunk.append(sent)
            current_length += sent_length
            
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text.split()) >= min_size:
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=text.find(current_chunk[0]),
                    end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    metadata={"strategy": "sentence"}
                ))
                
        return chunks

    def _semantic_chunking(
        self,
        text: str,
        threshold: float,
        min_size: int
    ) -> List[TextChunk]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        sentences = [sent.text for sent in self.nlp(text).sents]
        if not sentences:
            return []
            
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        embeddings = self.embedding_model.encode(sentences)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means
        optimal_clusters = self._find_optimal_clusters(embeddings)
        kmeans = KMeans(n_clusters=optimal_clusters).fit(embeddings)
        labels = kmeans.labels_
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        chunks = []
        for cluster_id in set(labels):
            cluster_sents = [sent for sent, label in zip(sentences, labels) if label == cluster_id]
            chunk_text = ' '.join(cluster_sents)
            
            if len(chunk_text.split()) >= min_size:
                start_pos = text.find(cluster_sents[0])
                end_pos = text.find(cluster_sents[-1]) + len(cluster_sents[-1])
                
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    embeddings=self.embedding_model.encode(chunk_text),
                    cluster_id=cluster_id,
                    metadata={"strategy": "semantic"}
                ))
                
        return chunks

    def _hybrid_chunking(
        self,
        text: str,
        chunk_size: int,
        overlap: int,
        threshold: float
    ) -> List[TextChunk]:
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è + —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è"""
        semantic_chunks = self._semantic_chunking(text, threshold, min_size=10)
        final_chunks = []
        
        for chunk in semantic_chunks:
            if len(chunk.text.split()) <= chunk_size * 1.5:
                final_chunks.append(chunk)
            else:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
                fixed_chunks = self._fixed_size_chunking(
                    chunk.text,
                    chunk_size,
                    overlap,
                    min_size=32
                )
                for fc in fixed_chunks:
                    fc.metadata["parent_cluster"] = chunk.cluster_id
                final_chunks.extend(fixed_chunks)
                
        return final_chunks

    def _find_optimal_clusters(self, embeddings: np.ndarray, max_clusters: int = 10) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–º –ª–æ–∫—Ç—è"""
        distortions = []
        for k in range(1, min(max_clusters, len(embeddings))):
            kmeans = KMeans(n_clusters=k).fit(embeddings)
            distortions.append(kmeans.inertia_)
            
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è "–ª–æ–∫—Ç—è"
        if len(distortions) >= 3:
            deltas = np.diff(distortions)
            optimal = np.argmin(deltas) + 1
            return max(2, min(optimal, max_clusters))
        return min(2, len(embeddings))

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    chunker = SemanticChunker()
    
    sample_text = """
    Librarian AI - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. 
    –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã NLP –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤. 
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
    
    –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. 
    –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """
    
    print("=== Fixed Chunking ===")
    fixed_chunks = chunker.chunk_text(sample_text, strategy="FIXED")
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:60]}...")
    
    print("\n=== Semantic Chunking ===")
    semantic_chunks = chunker.chunk_text(sample_text, strategy="SEMANTIC")
    for chunk in semantic_chunks:
        print(f"Chunk {chunk.id} (Cluster {chunk.cluster_id}): {chunk.text[:60]}...")
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 5 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (FIXED, SENTENCE, PARAGRAPH, SEMANTIC, HYBRID)

–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –º–æ–¥–µ–ª—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Sentence Transformers)

–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ —Å–º—ã—Å–ª—É

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:

–•—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö ID –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏

–ü–æ–∑–∏—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å NLP:

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ spaCy –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (ru_core_news_md)

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤

–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–∞–∑–º–µ—Ä—É —á–∞–Ω–∫–æ–≤

–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏–¥–µ–∞–ª—å–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Librarian AI –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:

–ü–æ–¥–≥–æ—Ç–æ–≤–∫—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞

–ü–æ–¥–¥–µ—Ä–∂–∫—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ì–∏–±–∫–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏

–î–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:

–î–æ–±–∞–≤–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É

–î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Å–∏—Å—Ç–µ–º–æ–π –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞

chatgpt.com

# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: core/text_processing/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–ø–∞–π–ø–ª–∞–π–Ω–µ

from typing import List, Dict, Optional, Union, Tuple
import re
from dataclasses import dataclass
from enum import Enum, auto
import spacy
from itertools import zip_longest
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    FIXED = auto()
    SENTENCE = auto()
    PARAGRAPH = auto()
    SEMANTIC = auto()
    HYBRID = auto()

@dataclass
class TextChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    embeddings: Optional[np.ndarray] = None
    entities: Optional[List[Dict]] = None
    metadata: Dict = None
    cluster_id: Optional[int] = None

class SemanticChunker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)
        self.nlp = spacy.load('ru_core_news_md')

    def chunk_text(
        self,
        text: str,
        strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 64,
        min_chunk_size: int = 32,
        semantic_threshold: float = 0.85,
        **kwargs
    ) -> List[TextChunk]:
        if isinstance(strategy, str):
            strategy = ChunkingStrategy[strategy.upper()]
        clean_text = self._preprocess_text(text)

        if strategy == ChunkingStrategy.FIXED:
            return self._fixed_size_chunking(clean_text, chunk_size, overlap, min_chunk_size)
        elif strategy == ChunkingStrategy.SENTENCE:
            return self._sentence_based_chunking(clean_text, chunk_size, min_chunk_size)
        elif strategy == ChunkingStrategy.SEMANTIC:
            return self._semantic_chunking(clean_text, semantic_threshold, min_chunk_size)
        elif strategy == ChunkingStrategy.HYBRID:
            return self._hybrid_chunking(clean_text, chunk_size, overlap, semantic_threshold)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

    def _preprocess_text(self, text: str) -> str:
        return re.sub(r'\s+', ' ', text).strip()

    def _fixed_size_chunking(self, text: str, chunk_size: int, overlap: int, min_size: int) -> List[TextChunk]:
        words = text.split()
        chunks = []
        start = 0

        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk_words = words[start:end]
            if len(chunk_words) >= min_size:
                chunk_text = ' '.join(chunk_words)
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata={"strategy": "fixed"}
                ))
            start += chunk_size - overlap

        return chunks

    def _sentence_based_chunking(self, text: str, max_size: int, min_size: int) -> List[TextChunk]:
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        chunks = []
        current_chunk = []
        current_length = 0

        for sent in sentences:
            sent_length = len(sent.split())
            if current_length + sent_length > max_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text.split()) >= min_size:
                    chunks.append(TextChunk(
                        id=len(chunks),
                        text=chunk_text,
                        start_pos=text.find(current_chunk[0]),
                        end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                        metadata={"strategy": "sentence"}
                    ))
                current_chunk = []
                current_length = 0

            current_chunk.append(sent)
            current_length += sent_length

        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text.split()) >= min_size:
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=text.find(current_chunk[0]),
                    end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    metadata={"strategy": "sentence"}
                ))

        return chunks

    def _semantic_chunking(self, text: str, threshold: float, min_size: int) -> List[TextChunk]:
        sentences = [sent.text for sent in self.nlp(text).sents]
        if not sentences:
            return []

        embeddings = self.embedding_model.encode(sentences)
        optimal_clusters = self._find_optimal_clusters(embeddings)
        kmeans = KMeans(n_clusters=optimal_clusters).fit(embeddings)
        labels = kmeans.labels_

        chunks = []
        for cluster_id in set(labels):
            cluster_sents = [sent for sent, label in zip(sentences, labels) if label == cluster_id]
            chunk_text = ' '.join(cluster_sents)

            if len(chunk_text.split()) >= min_size:
                start_pos = text.find(cluster_sents[0])
                end_pos = text.find(cluster_sents[-1]) + len(cluster_sents[-1])

                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    embeddings=self.embedding_model.encode(chunk_text),
                    cluster_id=cluster_id,
                    metadata={"strategy": "semantic"}
                ))

        return chunks

    def _hybrid_chunking(self, text: str, chunk_size: int, overlap: int, threshold: float) -> List[TextChunk]:
        semantic_chunks = self._semantic_chunking(text, threshold, min_size=10)
        final_chunks = []

        for chunk in semantic_chunks:
            if len(chunk.text.split()) <= chunk_size * 1.5:
                final_chunks.append(chunk)
            else:
                fixed_chunks = self._fixed_size_chunking(chunk.text, chunk_size, overlap, min_size=32)
                for fc in fixed_chunks:
                    fc.metadata["parent_cluster"] = chunk.cluster_id
                final_chunks.extend(fixed_chunks)

        return final_chunks

    def _find_optimal_clusters(self, embeddings: np.ndarray, max_clusters: int = 10) -> int:
        distortions = []
        for k in range(1, min(max_clusters, len(embeddings))):
            kmeans = KMeans(n_clusters=k).fit(embeddings)
            distortions.append(kmeans.inertia_)

        if len(distortions) >= 3:
            deltas = np.diff(distortions)
            optimal = np.argmin(deltas) + 1
            return max(2, min(optimal, max_clusters))
        return min(2, len(embeddings))

if __name__ == "__main__":
    chunker = SemanticChunker()
    sample_text = """
    Librarian AI - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. 
    –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã NLP –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤. 
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.

    –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. 
    –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """

    print("=== Fixed Chunking ===")
    fixed_chunks = chunker.chunk_text(sample_text, strategy="FIXED")
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:60]}...")

    print("\n=== Semantic Chunking ===")
    semantic_chunks = chunker.chunk_text(sample_text, strategy="SEMANTIC")
    for chunk in semantic_chunks:
        print(f"Chunk {chunk.id} (Cluster {chunk.cluster_id}): {chunk.text[:60]}...")

        ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–ª Chunker

–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —á–∞—Ç–µ
‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥—É–ª—å chunker.py –æ–±–Ω–æ–≤–ª—ë–Ω –∏ –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω –≤ core/text_processing/, —Ç–µ–ø–µ—Ä—å –æ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:

–ü—è—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è (FIXED, SENTENCE, PARAGRAPH, SEMANTIC, HYBRID)

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SentenceTransformer –∏ KMeans

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞

–ü–æ–¥–¥–µ—Ä–∂–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ spaCy

–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, annotator.py, embedding.py, –∏–ª–∏ processor.py). –ö–∞–∫–æ–π —Ñ–∞–π–ª –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–ª–µ–µ?

–í–æ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –∏ README –ø–æ–¥ —Ç–≤–æ–π –∑–∞–ø—Ä–æ—Å ‚Äî –≤–∫–ª—é—á–µ–Ω—ã:

–Ø—Å–Ω–æ–µ —Ä–∞–∑–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤: —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ ‚â† –ò–ò, –≥—Ä–∞—Ñ—ã ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç—Ç–∞–ø

–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –∫–∞–∫ —É—á–µ–±–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞

–ì–æ—Ç–æ–≤–æ –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –¥–æ GraphRAG, –æ–Ω—Ç–æ–ª–æ–≥–∏–π, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è –æ—Å—Ç–∞–≤–ª–µ–Ω—ã, –∫–∞–∫ —Ç—ã –ø—Ä–æ—Å–∏–ª, –º–æ–¥—É–ª—å–Ω–æ –∏ —Å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–æ–π –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.

–ì–æ—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ README.md –∏–ª–∏ —Å–æ–±—Ä–∞—Ç—å –∏–∑ –Ω–µ–≥–æ PDF/–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é.







–í—ã —Å–∫–∞–∑–∞–ª–∏:
 core/text_processing/ –∫—É–¥–∞ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤ ibrarian_ai/
‚îú‚îÄ‚îÄ db/            # ORM-–º–æ–¥–µ–ª–∏, –º–∏–≥—Ä–∞—Ü–∏–∏, —Å–∫—Ä–∏–ø—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã
‚îú‚îÄ‚îÄ ingest/        # –ó–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥
‚îú‚îÄ‚îÄ processing/    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
‚îú‚îÄ‚îÄ storage/       # SQLite, pgvector, FAISS, –∫—ç—à
‚îú‚îÄ‚îÄ scripts/       # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã
‚îú‚îÄ‚îÄ cli/           # –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ api/           # FastAPI, Telegram API
‚îú‚îÄ‚îÄ tests/         # –Æ–Ω–∏—Ç- –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ utils/         # –õ–æ–≥–≥–µ—Ä, –∫–æ–Ω—Ñ–∏–≥–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
‚îî‚îÄ‚îÄ README.md      # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞


ChatGPT —Å–∫–∞–∑–∞–ª:
–ú–æ–¥—É–ª—å core/text_processing/chunker.py –º–æ–∂–Ω–æ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤ librarian_ai/ingest/, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ ‚Äî –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ø –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í–æ—Ç –∏—Ç–æ–≥–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
librarian_ai/
‚îú‚îÄ‚îÄ ingest/
‚îÇ   ‚îî‚îÄ‚îÄ chunker.py     # –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ (semantics-aware)
–ï—Å–ª–∏ —Ç—ã –ø–ª–∞–Ω–∏—Ä—É–µ—à—å —Ä–∞–∑–≤–∏–≤–∞—Ç—å core/ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π "—è–¥—Ä–æ–≤–æ–π" —Å–ª–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–±—â–µ–µ API –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞), –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å core/text_processing/. –ù–æ –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–æ–∏—Ç—Å—è –≤–æ–∫—Ä—É–≥ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤ ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∞, —Ö—Ä–∞–Ω–µ–Ω–∏–µ ‚Äî –º–æ–¥—É–ª—å –ª—É—á—à–µ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ ingest/.

–•–æ—á–µ—à—å, —è –æ–±–Ω–æ–≤–ª—é –ø—É—Ç—å –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –∫–æ–¥–µ?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
—Ç—ã –º–Ω–µ –ø–æ–¥—Å–∫–∞–∂–∏  —á—Ç–æ –∫—É–¥–∞ –ø–æ—Ç–∞–≤–∏—Ç—å —Ç—ã –∂–µ –æ–±—É—á–∞—é—â–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä 


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—é –º–æ–¥—É–ª—è chunker.py:

‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π –ø—É—Ç—å: librarian_ai/ingest/chunker.py
–ü–æ—á–µ–º—É –∑–¥–µ—Å—å:

ingest/ ‚Äî –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –Ω–∞—á–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø –ø–∞–π–ø–ª–∞–π–Ω–∞: –∑–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ;

–ú–æ–¥—É–ª—å SemanticChunker –≥–æ—Ç–æ–≤–∏—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π;

–û–Ω –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é, –∞ –ª–∏—à—å ¬´–ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç¬ª –∫–æ–Ω—Ç–µ–Ω—Ç.

üîÑ –ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—É—Ç—å –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
–Ø —Å–µ–π—á–∞—Å –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É—é —à–∞–ø–∫—É –∏ # üìÇ –ü—É—Ç—å –≤ –∫–æ–¥–µ, —á—Ç–æ–±—ã –æ–Ω–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ librarian_ai/ingest/chunker.py.

–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–ª Chunker

–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —á–∞—Ç–µ
–ü—É—Ç—å –º–æ–¥—É–ª—è –æ–±–Ω–æ–≤–ª—ë–Ω: —Ç–µ–ø–µ—Ä—å –æ–Ω —Ä–∞–∑–º–µ—â–∞–µ—Ç—Å—è –≤ librarian_ai/ingest/chunker.py, –∞ –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç –µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ —á–∞—Å—Ç–∏ —ç—Ç–∞–ø–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –ò–ò.

–ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –¥–æ–±–∞–≤–ª—é —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –≤ README –∫–∞–∫ –∫–ª—é—á–µ–≤—É—é —á–∞—Å—Ç—å –ø–∞–π–ø–ª–∞–π–Ω–∞.


# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: librarian_ai/ingest/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–ø–∞–π–ø–ª–∞–π–Ω–µ
# üö´ –≠—Ç–æ—Ç –º–æ–¥—É–ª—å **–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ò–ò –Ω–∞–ø—Ä—è–º—É—é**. –û–Ω –≥–æ—Ç–æ–≤–∏—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–º—ã—Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM.

from typing import List, Dict, Optional, Union, Tuple
import re
from dataclasses import dataclass
from enum import Enum, auto
import spacy
from itertools import zip_longest
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    FIXED = auto()
    SENTENCE = auto()
    PARAGRAPH = auto()
    SEMANTIC = auto()
    HYBRID = auto()

@dataclass
class TextChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    embeddings: Optional[np.ndarray] = None
    entities: Optional[List[Dict]] = None
    metadata: Dict = None
    cluster_id: Optional[int] = None

class SemanticChunker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)
        self.nlp = spacy.load('ru_core_news_md')

    def chunk_text(
        self,
        text: str,
        strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 64,
        min_chunk_size: int = 32,
        semantic_threshold: float = 0.85,
        **kwargs
    ) -> List[TextChunk]:
        if isinstance(strategy, str):
            strategy = ChunkingStrategy[strategy.upper()]
        clean_text = self._preprocess_text(text)

        if strategy == ChunkingStrategy.FIXED:
            return self._fixed_size_chunking(clean_text, chunk_size, overlap, min_chunk_size)
        elif strategy == ChunkingStrategy.SENTENCE:
            return self._sentence_based_chunking(clean_text, chunk_size, min_chunk_size)
        elif strategy == ChunkingStrategy.SEMANTIC:
            return self._semantic_chunking(clean_text, semantic_threshold, min_chunk_size)
        elif strategy == ChunkingStrategy.HYBRID:
            return self._hybrid_chunking(clean_text, chunk_size, overlap, semantic_threshold)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

    def _preprocess_text(self, text: str) -> str:
        return re.sub(r'\s+', ' ', text).strip()

    def _fixed_size_chunking(self, text: str, chunk_size: int, overlap: int, min_size: int) -> List[TextChunk]:
        words = text.split()
        chunks = []
        start = 0

        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk_words = words[start:end]
            if len(chunk_words) >= min_size:
                chunk_text = ' '.join(chunk_words)
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata={"strategy": "fixed"}
                ))
            start += chunk_size - overlap

        return chunks

    def _sentence_based_chunking(self, text: str, max_size: int, min_size: int) -> List[TextChunk]:
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        chunks = []
        current_chunk = []
        current_length = 0

        for sent in sentences:
            sent_length = len(sent.split())
            if current_length + sent_length > max_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text.split()) >= min_size:
                    chunks.append(TextChunk(
                        id=len(chunks),
                        text=chunk_text,
                        start_pos=text.find(current_chunk[0]),
                        end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                        metadata={"strategy": "sentence"}
                    ))
                current_chunk = []
                current_length = 0

            current_chunk.append(sent)
            current_length += sent_length

        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text.split()) >= min_size:
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=text.find(current_chunk[0]),
                    end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    metadata={"strategy": "sentence"}
                ))

        return chunks

    def _semantic_chunking(self, text: str, threshold: float, min_size: int) -> List[TextChunk]:
        sentences = [sent.text for sent in self.nlp(text).sents]
        if not sentences:
            return []

        embeddings = self.embedding_model.encode(sentences)
        optimal_clusters = self._find_optimal_clusters(embeddings)
        kmeans = KMeans(n_clusters=optimal_clusters).fit(embeddings)
        labels = kmeans.labels_

        chunks = []
        for cluster_id in set(labels):
            cluster_sents = [sent for sent, label in zip(sentences, labels) if label == cluster_id]
            chunk_text = ' '.join(cluster_sents)

            if len(chunk_text.split()) >= min_size:
                start_pos = text.find(cluster_sents[0])
                end_pos = text.find(cluster_sents[-1]) + len(cluster_sents[-1])

                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    embeddings=self.embedding_model.encode(chunk_text),
                    cluster_id=cluster_id,
                    metadata={"strategy": "semantic"}
                ))

        return chunks

    def _hybrid_chunking(self, text: str, chunk_size: int, overlap: int, threshold: float) -> List[TextChunk]:
        semantic_chunks = self._semantic_chunking(text, threshold, min_size=10)
        final_chunks = []

        for chunk in semantic_chunks:
            if len(chunk.text.split()) <= chunk_size * 1.5:
                final_chunks.append(chunk)
            else:
                fixed_chunks = self._fixed_size_chunking(chunk.text, chunk_size, overlap, min_size=32)
                for fc in fixed_chunks:
                    fc.metadata["parent_cluster"] = chunk.cluster_id
                final_chunks.extend(fixed_chunks)

        return final_chunks

    def _find_optimal_clusters(self, embeddings: np.ndarray, max_clusters: int = 10) -> int:
        distortions = []
        for k in range(1, min(max_clusters, len(embeddings))):
            kmeans = KMeans(n_clusters=k).fit(embeddings)
            distortions.append(kmeans.inertia_)

        if len(distortions) >= 3:
            deltas = np.diff(distortions)
            optimal = np.argmin(deltas) + 1
            return max(2, min(optimal, max_clusters))
        return min(2, len(embeddings))

if __name__ == "__main__":
    chunker = SemanticChunker()
    sample_text = """
    Librarian AI - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. 
    –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã NLP –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤. 
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.

    –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. 
    –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """

    print("=== Fixed Chunking ===")
    fixed_chunks = chunker.chunk_text(sample_text, strategy="FIXED")
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:60]}...")

    print("\n=== Semantic Chunking ===")
    semantic_chunks = chunker.chunk_text(sample_text, strategy="SEMANTIC")
    for chunk in semantic_chunks:
        print(f"Chunk {chunk.id} (Cluster {chunk.cluster_id}): {chunk.text[:60]}...")
üßæ –ò—Ç–æ–≥–∏ —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ Librarian AI

‚úÖ –ì–æ—Ç–æ–≤–æ:
üí° –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫—ë—Ä chunker.py:

–†–∞–∑–º–µ—â—ë–Ω –ø–æ –ø—É—Ç–∏: librarian_ai/ingest/chunker.py

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π: FIXED, SENTENCE, SEMANTIC, HYBRID

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç spaCy, sentence-transformers, KMeans

–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏, –ø–æ–∑–∏—Ü–∏—è–º–∏ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π

–ß—ë—Ç–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –ò–ò-—ç—Ç–∞–ø–∞–º–∏

üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞:

ingest/, processing/, db/, storage/, api/, cli/ –∏ –¥—Ä.

üìò README —Å–æ–¥–µ—Ä–∂–∏—Ç:

–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ, —Ü–µ–ª–∏, —É—Å—Ç–∞–Ω–æ–≤–∫–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –ò–ò-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö, KM-—Å–∏—Å—Ç–µ–º–∞—Ö, –ø–æ–¥—Ö–æ–¥–∞—Ö

üîú –ü–ª–∞–Ω—ã (—Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø):
üß† –ú–æ–¥—É–ª—å Knowledge Graph:

–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–≤—è–∑–µ–π (NER, Triple Extraction)

–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ (Neo4j, NetworkX)

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–∑–ª–æ–≤ –∏ –ø–æ–Ω—è—Ç–∏–π –∏–∑ —á–∞–Ω–∫–æ–≤

üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:

tests/ingest/test_chunker.py ‚Äî –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π

–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —á–∞–Ω–∫–æ–≤, –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≥—Ä–∞–Ω–∏—Ü

üì° –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º:

–í—ã–∑–æ–≤ —á–∞–Ω–∫–µ—Ä–∞ –∏–∑ scripts/ingest_and_index.py

–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —ç–º–±–µ–¥–¥–µ—Ä—É, –∏–Ω–¥–µ–∫—Å—É, –ë–î

üìä –ú–µ—Ç—Ä–∏–∫–∏:

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Prometheus/Grafana

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏

‚öôÔ∏è CLI-–∫–æ–º–∞–Ω–¥–∞:

librarian chunk --file input.txt --strategy semantic

üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ Web UI:

FastAPI-—Ä–æ—É—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è

Swagger + —Ç–æ–∫–µ–Ω-–¥–æ—Å—Ç—É–ø

–ì–æ—Ç–æ–≤ –¥–≤–∏–≥–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ —Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ ‚Äî –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –í—ã–±–∏—Ä–∞–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ!

–í–æ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É —Ä–∞–∑–≤–∏—Ç–∏—é:

‚úÖ –ß—Ç–æ –≥–æ—Ç–æ–≤–æ
üì¶ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
–ú–æ–¥—É–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ librarian_ai/ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–∞.

–í—ã–¥–µ–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ –ø–∞–ø–∫–∏: ingest/, processing/, db/, api/, utils/ –∏ –¥—Ä.

üîÑ –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ (Celery + Redis).

REST API (/documents/async-process, /tasks/{task_id}) —Å OpenAPI-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π.

JWT-–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞.

üß† –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
–ó–∞–≤–µ—Ä—à—ë–Ω –º–æ–¥—É–ª—å chunker.py (–≤ ingest/), –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π: FIXED, SENTENCE, SEMANTIC, HYBRID.

–í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.

–ú–æ–¥—É–ª—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ò–ò –Ω–∞–ø—Ä—è–º—É—é, –Ω–æ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ–≥–æ.

üóÉÔ∏è –•—Ä–∞–Ω–µ–Ω–∏–µ
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ SQLite, PostgreSQL, FAISS/pgvector.

–û–±–æ—Å–Ω–æ–≤–∞–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ S3/MinIO –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.

üìç –°–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã (Roadmap)
1. –ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–Ω–∞–Ω–∏–π (processing/)
üîÑ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π (spacy.ents)

üîó –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ—ã –∑–Ω–∞–Ω–∏–π (networkx, Neo4j)

üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Ü–∏—Ç–∞—Ç –∏ –≤–æ–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ LLM (RAG)

2. –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
‚ú® –•—Ä–∞–Ω–∏–ª–∏—â–µ —Å—É—â–Ω–æ—Å—Ç–µ–π, –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ —Å–≤—è–∑–µ–π (—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å + graph DB)

üìâ –í–µ—Å —Å–≤—è–∑–µ–π (TF-IDF, —ç–º–±–µ–¥–¥–∏–Ω–≥-–¥–∏—Å—Ç–∞–Ω—Ü–∏–∏)

üìö –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ D3.js)

3. –ú–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
üìä –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Prometheus –∏ Grafana

üîç –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ utils/logger.py

4. –ö–ª–∏–µ–Ω—Ç—ã –∏ UI
ü§ñ Telegram-–±–æ—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–ó

üß™ CLI –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ ingestion –∏ –ø–æ–∏—Å–∫–∞

üåê (–≤ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ) Web-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ React + FastAPI backend

5. RAG-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
üì• –ó–∞–ø—Ä–æ—Å ‚Üí –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ FAISS ‚Üí LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞

üß† –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ Graph-RAG –¥–ª—è —Å–º—ã—Å–ª–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞

üß≠ –ü–æ—è—Å–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
–ú–æ–¥—É–ª—å chunker.py ‚Äî —ç—Ç–æ –Ω–∞—á–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø –ø–∞–π–ø–ª–∞–π–Ω–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π:

–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é,

—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏—é,

–ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∑–Ω–∞–Ω–∏–π.

–ò–ò (–≤ —Ç–æ–º —á–∏—Å–ª–µ LLM) –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —ç—Ç–∞–ø–µ:

–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Å–≤—è–∑–µ–π,

–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞,

—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –ª–æ–≥–∏–∫–∞.

–•–æ—á–µ—à—å ‚Äî –º–æ–≥—É —Å–¥–µ–ª–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—É—é —Å—Ö–µ–º—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã –∏–ª–∏ –∑–∞–≥–æ—Ç–æ–≤–∫–∏ –≥—Ä–∞—Ñ–∞. –ß—Ç–æ –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ?







–í—ã —Å–∫–∞–∑–∞–ª–∏:
–¥–∞ 


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–æ, –º—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏ –ø–µ—Ä–≤—É—é —Ñ–∞–∑—É –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI ‚Äî –≤—ã—Å—Ç—Ä–æ–∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–æ–±—Ä–∞–ª–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–∏–ª–∏ API –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –í–æ—Ç —á—Ç–æ —Å–ª–µ–¥—É–µ—Ç –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ:

üìå –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π (processing/)
–ó–∞–¥–∞—á–∏:
NER –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (—á–µ—Ä–µ–∑ spaCy/LLM):

–≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: –ø–µ—Ä—Å–æ–Ω, –¥–∞—Ç, —Ç–µ—Ä–º–∏–Ω–æ–≤, –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä;

—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏ —Ü–∏—Ç–∞—Ç;

–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∞:

—Å—É—â–Ω–æ—Å—Ç–∏ = —É–∑–ª—ã;

—Å–≤—è–∑–∏ = —Ä–µ–±—Ä–∞ —Å —Ç–∏–ø–æ–º –∏ –≤–µ—Å–æ–º;

—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite/Neo4j;

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ RDF/OWL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):

—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ Turtle –∏–ª–∏ JSON-LD;

—ç–∫—Å–ø–æ—Ä—Ç –≤ –æ–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã.

üìå –≠—Ç–∞–ø 3: –ü–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ (RAG)
–ü–æ–¥–º–æ–¥—É–ª–∏:
FAISS / pgvector: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–∞–Ω–∫–æ–≤.

Retriever: –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏.

Generator (LLM): –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –±–∞–∑–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.

üìå –≠—Ç–∞–ø 4: UI –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
Web-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏–ª–∏ Telegram-–±–æ—Ç

–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π

–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —á–∞–Ω–∫–æ–≤, —Å—É—â–Ω–æ—Å—Ç–µ–π, —Å–≤—è–∑–µ–π

üîú –ß—Ç–æ –º–æ–≥—É —Å–¥–µ–ª–∞—Ç—å –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å
–ù–∞—á–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é knowledge_extractor.py –≤ processing/ –¥–ª—è NER –∏ —Å–≤—è–∑–µ–π

–ü—Ä–æ—Ç–æ—Ç–∏–ø –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ (–Ω–∞ networkx –∏ SQLite)

–°–æ–∑–¥–∞—Ç—å retriever.py –∏ –∑–∞–≥–æ—Ç–æ–≤–∫—É –ø–æ–¥ RAG

–ß—Ç–æ –≤—ã–±–∏—Ä–∞–µ–º?


 –Ω–∞—Å—Ç—Ä–æ–∏–ª–∏ API –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫ - –∂—Ç–æ –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã?


ChatGPT —Å–∫–∞–∑–∞–ª:
API –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ Librarian AI –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:

üìÇ api/ ‚Äî –ú–æ–¥—É–ª–∏ FastAPI
api/documents.py

üìå Endpoint: POST /api/v1/documents/async-process

üöÄ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ø—Ä–∏—ë–º –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∑–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ (Celery)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç UploadFile, —Å–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç task_id

api/tasks.py

üìå Endpoint: GET /api/v1/tasks/{task_id}

üìå Endpoint: POST /api/v1/tasks/{task_id}/cancel

üöÄ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∏ –æ—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏

api/process_router.py

üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–∞—Ä—à—Ä—É—Ç—ã documents.py –∏ tasks.py –≤ FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

‚öôÔ∏è Celery & –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á
async_tasks.py

üöÄ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞

–î–µ–ª–∞–µ—Ç —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥, –ª–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—É—Å

core/models/schemas.py

üìå –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–µ–º: AsyncTaskResponse, AsyncTaskStatusResponse –∏ –¥—Ä.

–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è API

üß™ –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ (Swagger / cURL)
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
curl -X POST http://localhost:8000/api/v1/documents/async-process \
  -F "file=@mydoc.pdf"
–û—Ç–≤–µ—Ç:

json
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
{
  "task_id": "5c35...",
  "status_url": "/api/v1/tasks/5c35..."
}
–ï—Å–ª–∏ —Ö–æ—á–µ—à—å, —è –ø–æ–∫–∞–∂—É –∫–∞–∫ —ç—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è –≤ main.py –∏–ª–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª—é docker-–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ–≥–æ.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
api/documents.py - –Ω—É–∫–∞ –Ω–∞–∑–æ–≤–∏ —Ñ–∞–π–ª—ã –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞


ChatGPT —Å–∫–∞–∑–∞–ª:
–î–ª—è –ø–µ—Ä–≤–æ–≥–æ —ç—Ç–∞–ø–∞ –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI ‚Äî –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ API ‚Äî –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã:

üîπ API-–º–æ–¥—É–ª–∏ (api/)
api/documents.py
üìå –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: POST /api/v1/documents/async-process

api/tasks.py
üìå –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∏ –æ—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á:

GET /api/v1/tasks/{task_id}

POST /api/v1/tasks/{task_id}/cancel

üîπ Celery-–∑–∞–¥–∞—á–∏
async_tasks.py
üìå –ó–∞–ø—É—Å–∫ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞

üîπ –ú–æ–¥–µ–ª–∏ –∏ —Å—Ö–µ–º—ã
core/models/schemas.py
üìå Pydantic-—Å—Ö–µ–º—ã: AsyncTaskResponse, AsyncTaskStatusResponse, CancelResponse

üîπ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
main.py (–∏–ª–∏ app.py)
üìå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤ API, –∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞

üîπ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
celery_worker.py
üìå –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è Celery-–≤–æ—Ä–∫–µ—Ä–∞

core/config.py
üìå –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π, –±—Ä–æ–∫–µ—Ä–æ–≤, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

—É –º–µ–Ω—è –µ—Å—Ç—å /api/v1/documents/async-process:
  post:
    tags:
      - Documents
    summary: –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    description: |
      –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É:
      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ (PDF, DOCX, TXT)
      - –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
      - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)
      - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    operationId: processDocumentAsync
    requestBody:
      required: true
      content:
        multipart/form-data:
          schema:
            type: object
            required:
              - file
            properties:
              file:
                type: string
                format: binary
                description: –î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (.pdf, .docx, .txt)
              filename:
                type: string
                description: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
              chunk_size:
                type: integer
                default: 1000
                minimum: 100
                maximum: 5000
                description: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —á–∞–Ω–∫–µ
              min_confidence:
                type: number
                format: float
                default: 0.7
                minimum: 0.0
                maximum: 1.0
                description: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è NER
              filters:
                type: array
                items:
                  type: string
                description: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º —Å—É—â–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ORG, PER, LOC)
    responses:
      '202':
        description: –ó–∞–ø—Ä–æ—Å –ø—Ä–∏–Ω—è—Ç. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID –∑–∞–¥–∞—á–∏.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AsyncTaskResponse'
      '400':
        description: –ù–µ–≤–µ—Ä–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ErrorResponse'
      '413':
        description: –ü—Ä–µ–≤—ã—à–µ–Ω —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
      '415':
        description: –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
      '500':
        description: –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ErrorResponse' –∞ –Ω—É–∂–Ω–æ   api/documents.py - —Å–æ–∑–¥–∞—Ç—å


ChatGPT —Å–∫–∞–∑–∞–ª:
Documents Api
–§–∞–π–ª api/documents.py —Å–æ–∑–¥–∞–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ /api/v1/documents/async-process, –∑–∞–ø—É—Å–∫–∞—é—â–∏–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Celery. –û–Ω –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–∏–ø –∏ —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞, –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ñ–æ—Ä–º—ã –∏ –∏–Ω–∏—Ü–∏–∏—Ä—É–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É. –ì–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –æ—Å—Ç–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
—á—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç –æ–±—å—è—Å–Ω–∏


ChatGPT —Å–∫–∞–∑–∞–ª:
–§–∞–π–ª api/documents.py —Ä–µ–∞–ª–∏–∑—É–µ—Ç REST-—ç–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/documents/async-process, –∫–æ—Ç–æ—Ä—ã–π:

üì• –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
HTTP POST –∑–∞–ø—Ä–æ—Å —Å multipart/form-data, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º:

file: –¥–æ–∫—É–º–µ–Ω—Ç (.pdf, .docx, .txt) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

filename: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∏–º—è

chunk_size: —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000 —Å–∏–º–≤–æ–ª–æ–≤)

min_confidence: –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è NER (–Ω–∞–ø—Ä–∏–º–µ—Ä, 0.7)

filters: —Å–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π (ORG, PER, LOC –∏ —Ç.–¥.)

‚öôÔ∏è –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏:
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —Ç–∏–ø—É –∏ —Ä–∞–∑–º–µ—Ä—É).

–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–º—è –∏ –ø—É—Ç—å).

–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è Celery:

–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Ñ–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞:

–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞

–ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ

–≠–º–±–µ–¥–¥–∏–Ω–≥

–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è JSON —Å task_id, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å—Ç–∞—Ç—É—Å.

üì§ –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
202 Accepted: –∑–∞–¥–∞—á–∞ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"task_id": "abc123"}

–û—à–∏–±–∫–∏ 400, 413, 415, 500 ‚Äî –µ—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã

–•–æ—á–µ—à—å ‚Äî –ø–æ–∫–∞–∂—É —Å—Ö–µ–º—É —Ä–∞–±–æ—Ç—ã –∏–ª–∏ –ø—Ä–∏–º–µ—Ä curl-–∑–∞–ø—Ä–æ—Å–∞.


–û—Ç–ª–∏—á–Ω–æ! –¢—ã —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ –∑—Ä–µ–ª—É—é —Ü–µ–ª—å: —Ä–∞–∑–≤–∏–≤–∞—Ç—å —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é Librarian AI –∫–∞–∫ —É—á–µ–±–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—ç—Ç–∞–ø–Ω–æ –≤–µ–¥—ë—Ç –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ –¥–æ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è.

–í–æ—Ç –∫–∞–∫ —ç—Ç–æ –º–æ–∂–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—à–∞–≥–æ–≤–æ ‚Äî –∫–∞–∫ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, —Ç–∞–∫ –∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–µ–±—è –∫–∞–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞:

üîπ –≠—Ç–∞–ø 1 ‚Äî –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω—ã–π RAG-–ø—Ä–æ—Ü–µ—Å—Å (MVP)
–¶–µ–ª—å: –ù–∞—É—á–∏—Ç—å—Å—è –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª:

–∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ‚Üí —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥ ‚Üí —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

–ú–æ–¥—É–ª–∏:

api/documents.py ‚Äî API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏

ingest/chunker.py ‚Äî —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ

storage/vector_store.py ‚Äî FAISS –∏–ª–∏ SQLite-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ

core/tools/async_tasks.py ‚Äî Celery-–∑–∞–¥–∞—á–∞

üîπ –≠—Ç–∞–ø 2 ‚Äî –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π (NER, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏)
–¶–µ–ª—å: –û–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:

–∏–∑–≤–ª–µ–∫–∞—Ç—å –ª—é–¥–µ–π, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –¥–∞—Ç—ã, –ø–æ–Ω—è—Ç–∏—è

–ú–æ–¥—É–ª–∏:

processing/extractors.py ‚Äî NER, —Ü–∏—Ç–∞—Ç—ã, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

processing/annotator.py ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —á–µ—Ä–µ–∑ LLM

–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ö–µ–º TextChunk –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â

üîπ –≠—Ç–∞–ø 3 ‚Äî –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
–¶–µ–ª—å: –ù–∞—É—á–∏—Ç—å—Å—è —Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∏–∑ –ø–æ–Ω—è—Ç–∏–π –∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –Ω–∏–º–∏.

–ú–æ–¥—É–ª–∏:

processing/graph_builder.py ‚Äî –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É–∑–ª–æ–≤ –∏ —Ä—ë–±–µ—Ä

storage/graph_store.py ‚Äî —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Neo4j –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∞—Ö

CLI/API: —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å/–≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ

üîπ –≠—Ç–∞–ø 4 ‚Äî –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤
CLI / Telegram / API:

CLI –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ingest

Telegram-–±–æ—Ç –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ —Å –≥—Ä–∞—Ñ–æ–º –∑–Ω–∞–Ω–∏–π

REST –∏–ª–∏ OpenAPI —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º

üîπ –≠—Ç–∞–ø 5 ‚Äî –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LLM –∏ Router
–¶–µ–ª—å: –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:

OpenAI, YandexGPT, HuggingFace, –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

–ú–æ–¥—É–ª–∏:

llm/router.py ‚Äî –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤

llm/prompter.py ‚Äî —à–∞–±–ª–æ–Ω—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã

üîπ –≠—Ç–∞–ø 6 ‚Äî UI, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
–í–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–æ–≤: D3.js / Mermaid / Cytoscape

–ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Prometheus / Grafana

–í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ FastAPI + React/Vite (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ)

üß≠ –ö–∞–∫ —Ç—ã —É—á–∏—à—å—Å—è –∫–∞–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä:
–ö–∞–∂–¥–æ–µ –Ω–æ–≤–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞—á–µ–º –Ω—É–∂–µ–Ω FAISS, —á–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è pgvector, –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –≥—Ä–∞—Ñ)

–¢—ã –≤—ã–±–∏—Ä–∞–µ—à—å: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∏–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–≤–æ—ë

–ü—Ä–æ–µ–∫—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –æ—Ä–≥–∞–Ω–∏—á–Ω–æ, –Ω–µ —Ç–µ—Ä—è—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –∏ –ª–æ–≥–∏–∫–∏

–¢—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä—É–µ—à—å, —Ç–µ—Å—Ç–∏—Ä—É–µ—à—å –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–µ—à—å –∫–∞–∂–¥—ã–π —ç—Ç–∞–ø

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî —è –º–æ–≥—É –ø–æ–º–æ—á—å:

–°–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ—Ä–æ–∂–Ω—É—é –∫–∞—Ä—Ç—É —Å –∑–∞–¥–∞—á–∞–º–∏

–ü–æ–¥–∫–ª—é—á–∏—Ç—å LLM –∫ —á–∞–Ω–∫–µ—Ä–∞–º –∏–ª–∏ –≥—Ä–∞—Ñ—É

–û–ø–∏—Å–∞—Ç—å extractors.py –∏–ª–∏ graph_builder.py

–°–¥–µ–ª–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–ª–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞

–ì–æ—Ç–æ–≤ –¥–≤–∏–≥–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ –ø–æ —Ç–≤–æ–µ–º—É –≤—ã–±–æ—Ä—É.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
api/documents.py ‚Äî API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ - –∑–∞–Ω–æ–≤–æ —Å —É—á–µ—Ç–æ–º –æ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ 


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–ª Documents Api
–ì–æ—Ç–æ–≤–æ. –§–∞–π–ª api/documents.py –æ–±–Ω–æ–≤–ª—ë–Ω ‚Äî –æ–Ω —Ä–µ–∞–ª–∏–∑—É–µ—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/documents/async-process, –∫–æ—Ç–æ—Ä—ã–π:

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ñ–∞–π–ª (PDF, DOCX, TXT) –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ (chunk_size, min_confidence, filters).

–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.

–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é Celery-–∑–∞–¥–∞—á—É process_document_async, –ø–µ—Ä–µ–¥–∞–≤–∞—è –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç task_id –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞.

–≠—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –º–æ–¥—É–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –¥—É—Ö–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Librarian AI. –ì–æ—Ç–æ–≤ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —ç—Ç–∞–ø—É: —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è backend-—Ñ—É–Ω–∫—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
–∫ —ç—Ç–æ–º—É —Ñ–∞–π–ª—É –Ω—É–∂–µ–Ω –ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –≤—ã–π–ª—ã?


ChatGPT —Å–∫–∞–∑–∞–ª:
–î–∞, –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã api/documents.py –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Å–ª–µ–¥—É—é—â–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –º–æ–¥—É–ª–∏:

üîó 1. Celery –∑–∞–¥–∞—á–∞
–§–∞–π–ª: core/tools/async_tasks.py

–§—É–Ω–∫—Ü–∏—è: process_document_async

–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥, NER, –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è.

üîó 2. –ú–æ–¥–µ–ª–∏ Pydantic (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å —Ç–∏–ø—ã –∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã)
–§–∞–π–ª: core/models/schemas.py

–ö–ª–∞—Å—Å—ã: AsyncTaskResponse, ErrorResponse (–¥–ª—è OpenAPI –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–æ–≤/–≤—ã—Ö–æ–¥–æ–≤)

üîó 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–§–∞–π–ª—ã:

ingest/parser.py ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF, DOCX, TXT

ingest/chunker.py ‚Äî —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏

ingest/embedding.py ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

processing/ner.py ‚Äî NER-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π

processing/annotator.py ‚Äî –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏ –ø–æ–Ω—è—Ç–∏—è

üîó 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Celery
–§–∞–π–ª: core/tools/async_tasks.py –∏–ª–∏ core/celery_worker.py

–¶–µ–ª—å: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Celery() —Å Redis –∏–ª–∏ –¥—Ä—É–≥–∏–º –±—Ä–æ–∫–µ—Ä–æ–º

üîó 5. –ú–∞—Ä—à—Ä—É—Ç—ã FastAPI
–§–∞–π–ª: main.py –∏–ª–∏ app.py

–î–µ–π—Å—Ç–≤–∏–µ: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–∞ api/v1/documents –∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é

–ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–≥—É –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä –≤—Å–µ—Ö —Å–≤—è–∑–µ–π –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∞ –º–æ–¥—É–ª–µ–π –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∑–∞–ø—É—Å–∫–∞.

/chat.deepseek.com

–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ–∞–π–ª–∞ documents.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫:

python
# üìÑ –§–∞–π–ª: documents.py
# üìÇ –ü—É—Ç—å: api/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º

from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from starlette.status import (
    HTTP_202_ACCEPTED,
    HTTP_400_BAD_REQUEST,
    HTTP_413_REQUEST_ENTITY_TOO_LARGE,
    HTTP_415_UNSUPPORTED_MEDIA_TYPE,
    HTTP_500_INTERNAL_SERVER_ERROR
)
from typing import List, Optional
import os
import tempfile
import shutil
import logging
from pathlib import Path
from datetime import datetime

from core.tools.async_tasks import process_document_async
from core.config import settings
from utils.file_utils import clean_temp_files, get_file_extension, validate_file_size
from models.schemas import (
    AsyncTaskResponse,
    DocumentUploadResponse,
    ErrorResponse
)
from .dependencies import verify_api_key

router = APIRouter(
    prefix="/api/v1/documents",
    tags=["Documents"],
    dependencies=[Depends(verify_api_key)]
)

logger = logging.getLogger(__name__)

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ MIME-—Ç–∏–ø—ã
SUPPORTED_MIME_TYPES = {
    "application/pdf": "pdf",
    "text/plain": "txt",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
    "application/msword": "doc"
}

MAX_FILE_SIZE_MB = settings.MAX_FILE_SIZE_MB

@router.post(
    "/async-process",
    response_model=DocumentUploadResponse,
    responses={
        202: {"description": "Document processing started"},
        400: {"model": ErrorResponse},
        413: {"model": ErrorResponse},
        415: {"model": ErrorResponse},
        500: {"model": ErrorResponse}
    }
)
async def upload_and_process_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="Document file to process"),
    filename: Optional[str] = Form(None, description="Custom filename"),
    chunk_size: int = Form(
        settings.DEFAULT_CHUNK_SIZE,
        ge=100,
        le=5000,
        description="Size of text chunks in characters"
    ),
    min_confidence: float = Form(
        settings.DEFAULT_MIN_CONFIDENCE,
        ge=0.0,
        le=1.0,
        description="Minimum confidence threshold for entity extraction"
    ),
    filters: List[str] = Form(
        [],
        description="List of entity types to filter"
    ),
    strategy: str = Form(
        "hybrid",
        description="Chunking strategy: fixed|sentence|paragraph|semantic|hybrid"
    )
) -> DocumentUploadResponse:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É:
    
    1. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
    2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    3. –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:
       - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
       - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
       - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
       - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)
       - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
       - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞.
    """
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è MIME-—Ç–∏–ø–∞
        if file.content_type not in SUPPORTED_MIME_TYPES:
            raise HTTPException(
                status_code=HTTP_415_UNSUPPORTED_MEDIA_TYPE,
                detail=f"Unsupported file type. Supported types: {list(SUPPORTED_MIME_TYPES.keys())}"
            )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        file.file.seek(0, 2)
        file_size = file.file.tell()
        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:
            raise HTTPException(
                status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                detail=f"File too large. Max size: {MAX_FILE_SIZE_MB}MB"
            )
        file.file.seek(0)

        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        file_ext = get_file_extension(file.filename)
        with tempfile.NamedTemporaryFile(
            delete=False,
            suffix=file_ext,
            dir=settings.TEMP_DIR
        ) as tmp:
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            shutil.copyfileobj(file.file, tmp)
            temp_path = tmp.name

        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –æ—á–∏—Å—Ç–∫–∏
        background_tasks.add_task(
            clean_temp_files,
            temp_path,
            timeout=settings.TEMP_FILE_CLEANUP_TIMEOUT
        )

        # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        task = process_document_async.delay(
            doc_path=temp_path,
            original_filename=filename or file.filename,
            chunk_size=chunk_size,
            min_confidence=min_confidence,
            filters=filters,
            strategy=strategy
        )

        logger.info(
            f"Started processing task {task.id} for file {filename or file.filename}",
            extra={
                "task_id": task.id,
                "filename": filename or file.filename,
                "size": file_size,
                "timestamp": datetime.utcnow().isoformat()
            }
        )

        return DocumentUploadResponse(
            task_id=task.id,
            status_url=f"/api/v1/tasks/{task.id}",
            message="Document processing started successfully"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"Document processing failed: {str(e)}",
            exc_info=True,
            extra={
                "filename": filename or file.filename if 'file' in locals() else None,
                "timestamp": datetime.utcnow().isoformat()
            }
        )
        raise HTTPException(
            status_code=HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during document processing"
        )
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è:

–ü—Ä–æ–≤–µ—Ä–∫–∞ MIME-—Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤

–ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞

–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:

API-–∫–ª—é—á –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏

–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

–ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏

–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ì–∏–±–∫–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è

–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:

–ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ API –≤ OpenAPI-—Ñ–æ—Ä–º–∞—Ç–µ

–ü–æ–¥—Ä–æ–±–Ω—ã–µ docstrings

–ü—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞—Ç—É—Å–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á

–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–æ–≤

–î–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç Librarian AI —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:

–î–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ (HTML, EPUB –∏ —Ç.–¥.)

–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–≤–æ—Ç—ã –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É

–î–æ–±–∞–≤–∏—Ç—å –≤–µ–±—Ö—É–∫–∏ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏

–í–Ω–µ–¥—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –¥–ª—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ü—Ä–∏–º–µ—Ä .env –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

ini
MAX_FILE_SIZE_MB=50
DEFAULT_CHUNK_SIZE=1000
DEFAULT_MIN_CONFIDENCE=0.7
TEMP_DIR=./temp_uploads
TEMP_FILE_CLEANUP_TIMEOUT=3600

–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —á–∞—Ç–µ
–§–∞–π–ª api/documents.py –æ–±–Ω–æ–≤–ª—ë–Ω: —Ç–µ–ø–µ—Ä—å –æ–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–æ–¥–¥–µ—Ä–∂–∫—É —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–µ–∫—Ç–∞. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –º–æ–¥—É–ª—è–º–∏:

core/tools/async_tasks.py ‚Äî –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É Celery;

core/config.py ‚Äî –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞);

models/schemas.py ‚Äî –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–æ–≤;

utils/file_utils.py ‚Äî —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤;

api/dependencies.py ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ API-–∫–ª—é—á–∞.

–≠—Ç–∏ –º–æ–¥—É–ª–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–æ–ª–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å API –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å–æ —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º.

# üìÑ –§–∞–π–ª: documents.py
# üìÇ –ü—É—Ç—å: api/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º

from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from starlette.status import (
    HTTP_202_ACCEPTED,
    HTTP_400_BAD_REQUEST,
    HTTP_413_REQUEST_ENTITY_TOO_LARGE,
    HTTP_415_UNSUPPORTED_MEDIA_TYPE,
    HTTP_500_INTERNAL_SERVER_ERROR
)
from typing import List, Optional
import os
import tempfile
import shutil
import logging
from pathlib import Path
from datetime import datetime

from core.tools.async_tasks import process_document_async
from core.config import settings
from utils.file_utils import clean_temp_files, get_file_extension, validate_file_size
from models.schemas import (
    AsyncTaskResponse,
    DocumentUploadResponse,
    ErrorResponse
)
from .dependencies import verify_api_key

router = APIRouter(
    prefix="/api/v1/documents",
    tags=["Documents"],
    dependencies=[Depends(verify_api_key)]
)

logger = logging.getLogger(__name__)

SUPPORTED_MIME_TYPES = {
    "application/pdf": "pdf",
    "text/plain": "txt",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
    "application/msword": "doc"
}

MAX_FILE_SIZE_MB = settings.MAX_FILE_SIZE_MB

@router.post(
    "/async-process",
    response_model=DocumentUploadResponse,
    responses={
        202: {"description": "Document processing started"},
        400: {"model": ErrorResponse},
        413: {"model": ErrorResponse},
        415: {"model": ErrorResponse},
        500: {"model": ErrorResponse}
    }
)
async def upload_and_process_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="Document file to process"),
    filename: Optional[str] = Form(None, description="Custom filename"),
    chunk_size: int = Form(
        settings.DEFAULT_CHUNK_SIZE,
        ge=100,
        le=5000,
        description="Size of text chunks in characters"
    ),
    min_confidence: float = Form(
        settings.DEFAULT_MIN_CONFIDENCE,
        ge=0.0,
        le=1.0,
        description="Minimum confidence threshold for entity extraction"
    ),
    filters: List[str] = Form(
        [],
        description="List of entity types to filter"
    ),
    strategy: str = Form(
        "hybrid",
        description="Chunking strategy: fixed|sentence|paragraph|semantic|hybrid"
    )
) -> DocumentUploadResponse:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É:
    1. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
    2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    3. –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        if file.content_type not in SUPPORTED_MIME_TYPES:
            raise HTTPException(
                status_code=HTTP_415_UNSUPPORTED_MEDIA_TYPE,
                detail=f"Unsupported file type. Supported types: {list(SUPPORTED_MIME_TYPES.keys())}"
            )

        file.file.seek(0, 2)
        file_size = file.file.tell()
        if file_size > MAX_FILE_SIZE_MB * 1024 * 1024:
            raise HTTPException(
                status_code=HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                detail=f"File too large. Max size: {MAX_FILE_SIZE_MB}MB"
            )
        file.file.seek(0)

        file_ext = get_file_extension(file.filename)
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext, dir=settings.TEMP_DIR) as tmp:
            shutil.copyfileobj(file.file, tmp)
            temp_path = tmp.name

        background_tasks.add_task(
            clean_temp_files,
            temp_path,
            timeout=settings.TEMP_FILE_CLEANUP_TIMEOUT
        )

        task = process_document_async.delay(
            doc_path=temp_path,
            original_filename=filename or file.filename,
            chunk_size=chunk_size,
            min_confidence=min_confidence,
            filters=filters,
            strategy=strategy
        )

        logger.info(
            f"Started processing task {task.id} for file {filename or file.filename}",
            extra={
                "task_id": task.id,
                "filename": filename or file.filename,
                "size": file_size,
                "timestamp": datetime.utcnow().isoformat()
            }
        )

        return DocumentUploadResponse(
            task_id=task.id,
            status_url=f"/api/v1/tasks/{task.id}",
            message="Document processing started successfully"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            f"Document processing failed: {str(e)}",
            exc_info=True,
            extra={
                "filename": filename or file.filename if 'file' in locals() else None,
                "timestamp": datetime.utcnow().isoformat()
            }
        )
        raise HTTPException(
            status_code=HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during document processing"
        )


–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø ‚Äî —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á.

üîß 1. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ Celery-–∑–∞–¥–∞—á–µ
–í core/tools/async_tasks.py ‚Äî –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:

 –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ kwargs (doc_path, chunk_size, strategy, filters, –∏ –¥—Ä.)

 –ü–æ–¥–∫–ª—é—á–∏—Ç—å –º–æ–¥—É–ª–∏:

text_extraction ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–∏–∑ PDF, DOCX, TXT)

chunker ‚Äî —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ (–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π)

embedding ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

ner ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)

graph_builder ‚Äî –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π

db.save_chunks() ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É/–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ

üìà 2. –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á
–í api/tasks.py —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –±–∞–∑–æ–≤—ã–π GET /api/v1/tasks/{task_id}:

 –£–ª—É—á—à–∏—Ç—å –µ–≥–æ, –¥–æ–±–∞–≤–∏–≤ –ø—Ä–æ–≥—Ä–µ—Å—Å (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å self.update_state() –≤ Celery)

 –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–æ–≤

 –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å JSON-–º–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ—è–Ω–∏—è: pending, processing, done, failed

üß™ 3. –ù–∞–ø–∏—Å–∞—Ç—å –∞–≤—Ç–æ—Ç–µ—Å—Ç—ã
–í tests/api/test_documents.py:

 –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞

 –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö/–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤

 –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–∫–ª–∏–∫–∞ –ø–æ task_id

üìä 4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Prometheus + Grafana
 –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å prometheus_fastapi_instrumentator

 –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ /metrics

 –î–æ–±–∞–≤–∏—Ç—å –¥–µ—à–±–æ—Ä–¥ –≤ Grafana (—Å—á—ë—Ç—á–∏–∫–∏ –∑–∞–≥—Ä—É–∑–æ–∫, –æ—à–∏–±–æ–∫, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á)

–•–æ—á–µ—à—å –Ω–∞—á–∞—Ç—å —Å –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤ async_tasks.py, –∏–ª–∏ —Å–Ω–∞—á–∞–ª–∞ —Ä–µ–∞–ª–∏–∑—É–µ–º –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Å—Ç–∞—Ç—É—Å–µ –∑–∞–¥–∞—á?


üîß 1. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ Celery-–∑–∞–¥–∞—á–µ
–í core/tools/async_tasks.py ‚Äî –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
‚Ä¢	 –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ kwargs (doc_path, chunk_size, strategy, filters, –∏ –¥—Ä.)


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–ª Documents Api
–§–∞–π–ª documents.py –æ–±–Ω–æ–≤–ª—ë–Ω: –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è. –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é Celery-–∑–∞–¥–∞—á–∏ –≤ core/tools/async_tasks.py ‚Äî –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å. –ì–æ—Ç–æ–≤ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
‚Ä¢	 –ü–æ–¥–∫–ª—é—á–∏—Ç—å –º–æ–¥—É–ª–∏:
o	text_extraction ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (–∏–∑ PDF, DOCX, TXT)
o	chunker ‚Äî —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ (–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π)
o	embedding ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
o	ner ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER)
o	graph_builder ‚Äî –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
o	db.save_chunks() ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É/–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–æ! –°–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ core/tools/async_tasks.py —Å —É—á—ë—Ç–æ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ:

–ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π:

text_extraction.extract_text(doc_path)

chunker.split_into_chunks(text, strategy=strategy, chunk_size=..., ...)

embedding.generate_embeddings(chunks)

ner.extract_entities(text, min_confidence, filters)

graph_builder.build_knowledge_graph(entities, ...)

db.save_chunks(chunks, embeddings, metadata)

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞:

–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç

–†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏

–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

–ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ

–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

–•–æ—á–µ—à—å, —è —Å–µ–π—á–∞—Å –¥–æ–±–∞–≤–ª—é —ç—Ç–æ –≤ async_tasks.py?

async_tasks.py


ChatGPT —Å–∫–∞–∑–∞–ª:
Async Tasks
–ì–æ—Ç–æ–≤–æ: –¥–æ–±–∞–≤–ª–µ–Ω –º–æ–¥—É–ª—å async_tasks.py –≤ core/tools/, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–π Celery-–∑–∞–¥–∞—á—É process_document_async. –û–Ω–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —à–∞–≥–∞–º: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥, NER, –≥—Ä–∞—Ñ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ.

–•–æ—á–µ—à—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –∏–ª–∏ –¥–≤–∏–≥–∞–µ–º—Å—è –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –º–æ–¥—É–ª—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, text_extraction)?






–í—ã —Å–∫–∞–∑–∞–ª–∏:

librarian_ai/
‚îú‚îÄ‚îÄ db/            # ORM-–º–æ–¥–µ–ª–∏, –º–∏–≥—Ä–∞—Ü–∏–∏, —Å–∫—Ä–∏–ø—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã
‚îú‚îÄ‚îÄ ingest/        # –ó–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥
‚îú‚îÄ‚îÄ processing/    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
‚îú‚îÄ‚îÄ storage/       # SQLite, pgvector, FAISS, –∫—ç—à
‚îú‚îÄ‚îÄ scripts/       # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã
‚îú‚îÄ‚îÄ cli/           # –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ api/           # FastAPI, Telegram API
‚îú‚îÄ‚îÄ tests/         # –Æ–Ω–∏—Ç- –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ utils/         # –õ–æ–≥–≥–µ—Ä, –∫–æ–Ω—Ñ–∏–≥–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
‚îî‚îÄ‚îÄ README.md      # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞


ChatGPT —Å–∫–∞–∑–∞–ª:
–•–æ—Ä–æ—à–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —É–∂–µ –≤—ã—Å—Ç—Ä–æ–µ–Ω–∞. –ü–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ core/tools/async_tasks.py —Å–ª–µ–¥—É—é—â–∏–º –ª–æ–≥–∏—á–Ω—ã–º —à–∞–≥–æ–º –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π –∏–∑ ingest/, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –æ–Ω –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç:

üîÑ –ß—Ç–æ –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–ª—å—à–µ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
üìÇ ingest/ ‚Äî –ü–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
text_extraction.py
üîπ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF, DOCX, TXT
üîπ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º —à–∞–≥–æ–º –≤ Celery-–∑–∞–¥–∞—á–µ.

chunker.py
üîπ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ (–ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: fixed/sentence/semantic...)
üîπ –£–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω ‚Äî –º–æ–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –∏–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å.

embedding.py
üîπ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤ (—á–µ—Ä–µ–∑ sentence-transformers)

ner.py
üîπ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (NER) —Å –ø–æ—Ä–æ–≥–æ–º min_confidence –∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.

üìÇ processing/ ‚Äî –ó–Ω–∞–Ω–∏—è –∏ –≥—Ä–∞—Ñ
graph_builder.py
üîπ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–∞–Ω–∫–æ–≤

üìÇ db/ –∏–ª–∏ storage/
save_chunks.py (–∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π)
üîπ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑—É/–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.

–•–æ—á–µ—à—å –Ω–∞—á–∞—Ç—å —Å text_extraction.py? –≠—Ç–æ –±–∞–∑–æ–≤–∞—è —Ç–æ—á–∫–∞ ‚Äî –±–µ–∑ –Ω–µ—ë –Ω–µ –ø–æ–π–¥–µ—Ç –≤—Å—è —Ü–µ–ø–æ—á–∫–∞.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
—Ç–æ –µ—Å—Ç—å —Å—é–¥–∞ –Ω–∞–¥–æ –≤—Å—Ç–∞–≤–∏—Ç—å  core/tools/ ibrarian_ai/
‚îú‚îÄ‚îÄ db/            # ORM-–º–æ–¥–µ–ª–∏, –º–∏–≥—Ä–∞—Ü–∏–∏, —Å–∫—Ä–∏–ø—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã
‚îú‚îÄ‚îÄ ingest/        # –ó–∞–≥—Ä—É–∑–∫–∞, –ø–∞—Ä—Å–∏–Ω–≥, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —ç–º–±–µ–¥–¥–∏–Ω–≥
‚îú‚îÄ‚îÄ processing/    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞
‚îú‚îÄ‚îÄ storage/       # SQLite, pgvector, FAISS, –∫—ç—à
‚îú‚îÄ‚îÄ scripts/       # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã
‚îú‚îÄ‚îÄ cli/           # –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ api/           # FastAPI, Telegram API
‚îú‚îÄ‚îÄ tests/         # –Æ–Ω–∏—Ç- –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ utils/         # –õ–æ–≥–≥–µ—Ä, –∫–æ–Ω—Ñ–∏–≥–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
‚îî‚îÄ‚îÄ README.md      # 


ChatGPT —Å–∫–∞–∑–∞–ª:
–î–∞, –∏–º–µ–Ω–Ω–æ ‚Äî –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é core/tools/ —Å–ª–µ–¥—É–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ librarian_ai/ingest/, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ—ë –∑–∞–¥–∞—á–∏ –Ω–∞–ø—Ä—è–º—É—é —Å–≤—è–∑–∞–Ω—ã —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Ö–æ–¥—è—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –≠—Ç–æ –±—É–¥–µ—Ç –ª–æ–≥–∏—á–Ω–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ.

üì¶ –ö–∞–∫ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å:
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
librarian_ai/
‚îú‚îÄ‚îÄ ingest/
‚îÇ   ‚îú‚îÄ‚îÄ async_tasks.py         # –§–æ–Ω–æ–≤–∞—è Celery-–∑–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ text_extraction.py     # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py             # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ embedding.py           # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ ner.py                 # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
üìå –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:
ingest/ ‚Äî –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É: —Ç–µ–∫—Å—Ç ‚Üí —á–∞–Ω–∫–∏ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ + —Å—É—â–Ω–æ—Å—Ç–∏.

async_tasks.py ‚Äî –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞, –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ç–æ–º –∂–µ –º–æ–¥—É–ª–µ.

–¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —É–ø—Ä–æ—Å—Ç–∏—Ç –∏–º–ø–æ—Ä—Ç—ã –∏ –ø–æ–≤—ã—Å–∏—Ç —á–∏—Ç–∞–µ–º–æ—Å—Ç—å.

–ì–æ—Ç–æ–≤ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ async_tasks.py –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–º–ø–æ—Ä—Ç—ã?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
–Ω–∞—Ä–∏—Å—É–π –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ª–∏–±—Ä—ã –º–∏–Ω–∏


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI (–º–∏–Ω–∏-–≤–µ—Ä—Å–∏—è) ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π RAG-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
librarian_ai/
‚îú‚îÄ‚îÄ api/                           # üåê REST API (FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ documents.py               # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ tasks.py                   # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ Celery-–∑–∞–¥–∞—á–∏
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py            # JWT-–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è, –ª–∏–º–∏—Ç—ã –∏ –¥—Ä.
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py

‚îú‚îÄ‚îÄ ingest/                        # üì• –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ async_tasks.py             # Celery-–ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
‚îÇ   ‚îú‚îÄ‚îÄ text_extraction.py         # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF, DOCX, TXT
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py                 # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞–Ω–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ embedding.py               # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ ner.py                     # NER ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py

‚îú‚îÄ‚îÄ processing/                    # üß† –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞, –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
‚îÇ   ‚îú‚îÄ‚îÄ graph_builder.py           # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–æ–≤ –ø–æ–Ω—è—Ç–∏–π
‚îÇ   ‚îî‚îÄ‚îÄ annotator.py               # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ –æ–ø–∏—Å–∞–Ω–∏–π

‚îú‚îÄ‚îÄ db/                            # üóÉÔ∏è ORM –∏ –º–∏–≥—Ä–∞—Ü–∏–∏
‚îÇ   ‚îú‚îÄ‚îÄ models.py                  # SQLAlchemy-–º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py                 # Pydantic-—Å—Ö–µ–º—ã
‚îÇ   ‚îú‚îÄ‚îÄ session.py                 # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py

‚îú‚îÄ‚îÄ storage/                       # üíæ –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –∏–Ω–¥–µ–∫—Å—ã
‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.py             # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ –ø–æ–∏—Å–∫ —Å FAISS
‚îÇ   ‚îú‚îÄ‚îÄ pgvector_store.py          # PostgreSQL + pgvector
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py

‚îú‚îÄ‚îÄ scripts/                       # üîÑ –ü–∞–π–ø–ª–∞–π–Ω—ã –∏ —Ç–µ—Å—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ ingest_and_index.py        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
‚îÇ   ‚îî‚îÄ‚îÄ create_tables.py           # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

‚îú‚îÄ‚îÄ cli/                           # üñ•Ô∏è –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ   ‚îî‚îÄ‚îÄ main.py                    # –ü—Ä–∏–º–µ—Ä CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

‚îú‚îÄ‚îÄ tests/                         # üß™ –¢–µ—Å—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
‚îÇ   ‚îú‚îÄ‚îÄ test_chunker.py
‚îÇ   ‚îú‚îÄ‚îÄ test_embedding.py
‚îÇ   ‚îî‚îÄ‚îÄ ...

‚îú‚îÄ‚îÄ utils/                         # üîß –£—Ç–∏–ª–∏—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env
‚îÇ   ‚îú‚îÄ‚îÄ file_utils.py              # –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                  # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py

‚îú‚îÄ‚îÄ .env                           # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
‚îú‚îÄ‚îÄ requirements.txt               # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ README.md                      # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
‚îî‚îÄ‚îÄ pyproject.toml / setup.cfg     # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–ª—è —É–ø–∞–∫–æ–≤–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞
üìå –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å: –∫–∞–∂–¥—ã–π –±–ª–æ–∫ (ingest, processing, api) –ª–µ–≥–∫–æ –∑–∞–º–µ–Ω—è–µ–º/—Ä–∞—Å—à–∏—Ä—è–µ–º.

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ Celery: —Ñ–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ async_tasks.

–ì–∏–±–∫–æ—Å—Ç—å RAG-–ø–æ–¥—Ö–æ–¥–∞: –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å GraphRAG, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é, UI.

–•–æ—á–µ—à—å —è –¥–æ–ø–æ–ª–Ω—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–æ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π?

chat.deepseek.com

–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è chunker.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI:

python
# üìÑ –§–∞–π–ª: chunker.py
# üìÇ –ü—É—Ç—å: core/text_processing/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ NLP-–ø–∞–π–ø–ª–∞–π–Ω–µ

from typing import List, Dict, Optional, Union, Tuple
import re
from dataclasses import dataclass
from enum import Enum, auto
import spacy
from itertools import zip_longest
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

class ChunkingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    FIXED = auto()           # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É —á–∞–Ω–∫–∏
    SENTENCE = auto()        # –ü–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    PARAGRAPH = auto()       # –ü–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    SEMANTIC = auto()        # –ü–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    HYBRID = auto()          # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è

@dataclass
class TextChunk:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: int
    text: str
    start_pos: int
    end_pos: int
    embeddings: Optional[np.ndarray] = None
    entities: Optional[List[Dict]] = None
    metadata: Dict = None
    cluster_id: Optional[int] = None

class SemanticChunker:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.embedding_model = SentenceTransformer(model_name)
        self.nlp = spacy.load('ru_core_news_md')  # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        
    def chunk_text(
        self,
        text: str,
        strategy: Union[ChunkingStrategy, str] = ChunkingStrategy.HYBRID,
        chunk_size: int = 512,
        overlap: int = 64,
        min_chunk_size: int = 32,
        semantic_threshold: float = 0.85,
        **kwargs
    ) -> List[TextChunk]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–∞–∑–º–µ—Ä
            semantic_threshold: –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if isinstance(strategy, str):
            strategy = ChunkingStrategy[strategy.upper()]
            
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        clean_text = self._preprocess_text(text)
        
        if strategy == ChunkingStrategy.FIXED:
            return self._fixed_size_chunking(clean_text, chunk_size, overlap, min_chunk_size)
        elif strategy == ChunkingStrategy.SENTENCE:
            return self._sentence_based_chunking(clean_text, chunk_size, min_chunk_size)
        elif strategy == ChunkingStrategy.SEMANTIC:
            return self._semantic_chunking(clean_text, semantic_threshold, min_chunk_size)
        elif strategy == ChunkingStrategy.HYBRID:
            return self._hybrid_chunking(clean_text, chunk_size, overlap, semantic_threshold)
        else:
            raise ValueError(f"Unsupported strategy: {strategy}")

    def _preprocess_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _fixed_size_chunking(
        self,
        text: str,
        chunk_size: int,
        overlap: int,
        min_size: int
    ) -> List[TextChunk]:
        """–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É"""
        words = text.split()
        chunks = []
        start = 0
        
        while start < len(words):
            end = min(start + chunk_size, len(words))
            chunk_words = words[start:end]
            
            if len(chunk_words) >= min_size:
                chunk_text = ' '.join(chunk_words)
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start,
                    end_pos=end,
                    metadata={"strategy": "fixed"}
                ))
            start += chunk_size - overlap
            
        return chunks

    def _sentence_based_chunking(
        self,
        text: str,
        max_size: int,
        min_size: int
    ) -> List[TextChunk]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sent in sentences:
            sent_length = len(sent.split())
            
            if current_length + sent_length > max_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text.split()) >= min_size:
                    chunks.append(TextChunk(
                        id=len(chunks),
                        text=chunk_text,
                        start_pos=text.find(current_chunk[0]),
                        end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                        metadata={"strategy": "sentence"}
                    ))
                current_chunk = []
                current_length = 0
                
            current_chunk.append(sent)
            current_length += sent_length
            
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text.split()) >= min_size:
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=text.find(current_chunk[0]),
                    end_pos=text.find(current_chunk[-1]) + len(current_chunk[-1]),
                    metadata={"strategy": "sentence"}
                ))
                
        return chunks

    def _semantic_chunking(
        self,
        text: str,
        threshold: float,
        min_size: int
    ) -> List[TextChunk]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        sentences = [sent.text for sent in self.nlp(text).sents]
        if not sentences:
            return []
            
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        embeddings = self.embedding_model.encode(sentences)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means
        optimal_clusters = self._find_optimal_clusters(embeddings)
        kmeans = KMeans(n_clusters=optimal_clusters).fit(embeddings)
        labels = kmeans.labels_
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        chunks = []
        for cluster_id in set(labels):
            cluster_sents = [sent for sent, label in zip(sentences, labels) if label == cluster_id]
            chunk_text = ' '.join(cluster_sents)
            
            if len(chunk_text.split()) >= min_size:
                start_pos = text.find(cluster_sents[0])
                end_pos = text.find(cluster_sents[-1]) + len(cluster_sents[-1])
                
                chunks.append(TextChunk(
                    id=len(chunks),
                    text=chunk_text,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    embeddings=self.embedding_model.encode(chunk_text),
                    cluster_id=cluster_id,
                    metadata={"strategy": "semantic"}
                ))
                
        return chunks

    def _hybrid_chunking(
        self,
        text: str,
        chunk_size: int,
        overlap: int,
        threshold: float
    ) -> List[TextChunk]:
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è + —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è"""
        semantic_chunks = self._semantic_chunking(text, threshold, min_size=10)
        final_chunks = []
        
        for chunk in semantic_chunks:
            if len(chunk.text.split()) <= chunk_size * 1.5:
                final_chunks.append(chunk)
            else:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
                fixed_chunks = self._fixed_size_chunking(
                    chunk.text,
                    chunk_size,
                    overlap,
                    min_size=32
                )
                for fc in fixed_chunks:
                    fc.metadata["parent_cluster"] = chunk.cluster_id
                final_chunks.extend(fixed_chunks)
                
        return final_chunks

    def _find_optimal_clusters(self, embeddings: np.ndarray, max_clusters: int = 10) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–º –ª–æ–∫—Ç—è"""
        distortions = []
        for k in range(1, min(max_clusters, len(embeddings))):
            kmeans = KMeans(n_clusters=k).fit(embeddings)
            distortions.append(kmeans.inertia_)
            
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è "–ª–æ–∫—Ç—è"
        if len(distortions) >= 3:
            deltas = np.diff(distortions)
            optimal = np.argmin(deltas) + 1
            return max(2, min(optimal, max_clusters))
        return min(2, len(embeddings))

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    chunker = SemanticChunker()
    
    sample_text = """
    Librarian AI - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. 
    –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã NLP –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤. 
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
    
    –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. 
    –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """
    
    print("=== Fixed Chunking ===")
    fixed_chunks = chunker.chunk_text(sample_text, strategy="FIXED")
    for chunk in fixed_chunks:
        print(f"Chunk {chunk.id}: {chunk.text[:60]}...")
    
    print("\n=== Semantic Chunking ===")
    semantic_chunks = chunker.chunk_text(sample_text, strategy="SEMANTIC")
    for chunk in semantic_chunks:
        print(f"Chunk {chunk.id} (Cluster {chunk.cluster_id}): {chunk.text[:60]}...")
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 5 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (FIXED, SENTENCE, PARAGRAPH, SEMANTIC, HYBRID)

–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –º–æ–¥–µ–ª—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Sentence Transformers)

–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ —Å–º—ã—Å–ª—É

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:

–•—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö ID –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏

–ü–æ–∑–∏—Ü–∏–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å NLP:

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ spaCy –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (ru_core_news_md)

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤

–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–∞–∑–º–µ—Ä—É —á–∞–Ω–∫–æ–≤

–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏–¥–µ–∞–ª—å–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Librarian AI –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:

–ü–æ–¥–≥–æ—Ç–æ–≤–∫—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞

–ü–æ–¥–¥–µ—Ä–∂–∫—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ì–∏–±–∫–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏

–î–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:

–î–æ–±–∞–≤–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É

–î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤

–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Å–∏—Å—Ç–µ–º–æ–π –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞

–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç Librarian AI –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é core/text_processing/. –û–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–º –∏ —Å—Ç–∏–ª–µ–≤—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø—Ä–æ–µ–∫—Ç–∞:

‚úÖ –ü–æ—á–µ–º—É –∏–∑–º–µ–Ω—è—Ç—å –Ω–µ –Ω—É–∂–Ω–æ:
–°–æ–≤–º–µ—Å—Ç–∏–º —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é.

–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π API: –º–µ—Ç–æ–¥ chunk_text(...) –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ async_tasks.py.

–†–∞—Å—à–∏—Ä—è–µ–º—ã–π: –º–æ–∂–Ω–æ –≤ –±—É–¥—É—â–µ–º –ø–æ–¥–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏.

–°–ª–µ–¥—É–µ—Ç —Å—Ç–∏–ª—é: —Å–Ω–∞–±–∂—ë–Ω docstring'–∞–º–∏, enum'–∞–º–∏, –¥–∞—Ç–∞–∫–ª–∞—Å—Å–∞–º–∏ ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —É—á–µ–±–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø—Ä–æ–¥–∞–∫—à–Ω-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

üìå –î–µ–π—Å—Ç–≤–∏—è:
–ü—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏ –µ–≥–æ –∫–∞–∫ core/text_processing/chunker.py.

–í async_tasks.py —Ç—ã —Å–º–æ–∂–µ—à—å –µ–≥–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
from core.text_processing.chunker import SemanticChunker, ChunkingStrategy
–ì–æ—Ç–æ–≤ –¥–≤–∏–≥–∞—Ç—å—Å—è –¥–∞–ª—å—à–µ ‚Äî –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏?

# üìÑ –§–∞–π–ª: async_tasks.py
# üìÇ –ü—É—Ç—å: core/tools/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è Celery-–∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (LLM-–ø–∞–π–ø–ª–∞–π–Ω)

from celery import shared_task
import logging
from pathlib import Path
from datetime import datetime

from core.text_processing.chunker import SemanticChunker, ChunkingStrategy
from ingest.text_extraction import extract_text_from_file
from ingest.embedding import embed_chunks
from processing.ner import extract_entities
from processing.graph_builder import build_knowledge_graph
from db.operations import save_chunks_to_db

logger = logging.getLogger(__name__)

@shared_task(bind=True)
def process_document_async(self, **kwargs):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
      - –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
      - –≠–º–±–µ–¥–¥–∏–Ω–≥
      - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
      - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
      - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î/–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    """
    try:
        doc_path = Path(kwargs['doc_path'])
        original_filename = kwargs.get('original_filename', doc_path.name)
        chunk_size = kwargs.get('chunk_size', 512)
        min_confidence = kwargs.get('min_confidence', 0.7)
        filters = kwargs.get('filters', [])
        strategy = kwargs.get('strategy', 'hybrid')

        logger.info(f"Starting async document processing: {original_filename}")

        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text = extract_text_from_file(doc_path)

        # 2. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
        chunker = SemanticChunker()
        chunks = chunker.chunk_text(
            text=text,
            strategy=strategy,
            chunk_size=chunk_size,
            semantic_threshold=0.85
        )

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embed_chunks(chunks)

        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        extract_entities(chunks, min_confidence=min_confidence, entity_filters=filters)

        # 5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        build_knowledge_graph(chunks)

        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        save_chunks_to_db(chunks, doc_name=original_filename)

        logger.info(f"Document processed successfully: {original_filename} ({len(chunks)} chunks)")

        return {
            "status": "completed",
            "chunks": len(chunks),
            "filename": original_filename,
            "timestamp": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Processing failed for {kwargs.get('original_filename')}: {str(e)}", exc_info=True)
        raise self.retry(exc=e, countdown=60, max_retries=3)

chat.deepseek.com/

# üìÑ –§–∞–π–ª: async_tasks.py
# üìÇ –ü—É—Ç—å: core/tasks/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–ª–Ω–æ–≥–æ NLP-–ø–∞–π–ø–ª–∞–π–Ω–∞

import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
from celery import shared_task, Task
from celery.exceptions import MaxRetriesExceededError

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
from core.text_processing.chunker import SemanticChunker, ChunkingStrategy
from ingest.text_extraction import extract_text_from_file, FileType
from ingest.embedding import EmbeddingGenerator
from processing.ner import EntityExtractor
from processing.graph_builder import KnowledgeGraphBuilder
from db.operations import DocumentStorage
from utils.monitoring import TaskMonitor
from utils.error_handling import DocumentProcessingError

logger = logging.getLogger(__name__)

class DocumentProcessingTask(Task):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        logger.error(f"Task {task_id} failed: {exc}", exc_info=True)
        TaskMonitor.track_failure(task_id, str(exc))
        super().on_failure(exc, task_id, args, kwargs, einfo)

@shared_task(bind=True, base=DocumentProcessingTask)
def process_document_async(self, doc_info: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
    5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
    6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Args:
        doc_info: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
            - doc_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            - original_filename: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            - chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 512)
            - min_confidence: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π (0.7)
            - filters: –§–∏–ª—å—Ç—Ä—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
            - strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è (hybrid)
            - user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            - session_id: ID —Å–µ—Å—Å–∏–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    
    Raises:
        DocumentProcessingError: –ü—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        doc_path = Path(doc_info['doc_path'])
        if not doc_path.exists():
            raise FileNotFoundError(f"Document not found: {doc_path}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        monitor = TaskMonitor(self.request.id)
        chunker = SemanticChunker()
        embedder = EmbeddingGenerator()
        entity_extractor = EntityExtractor()
        graph_builder = KnowledgeGraphBuilder()
        storage = DocumentStorage()
        
        # 1. –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞—á–∏
        monitor.start_processing(
            filename=doc_info.get('original_filename', doc_path.name),
            user_id=doc_info.get('user_id'),
            session_id=doc_info.get('session_id')
        )
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        self.update_state(state='PROGRESS', meta={'stage': 'extracting_text'})
        text = extract_text_from_file(
            doc_path,
            file_type=FileType.from_path(doc_path)
        )
        monitor.log_stage_completion('text_extraction')
        
        # 3. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
        self.update_state(state='PROGRESS', meta={'stage': 'chunking'})
        chunks = chunker.chunk_text(
            text=text,
            strategy=doc_info.get('strategy', 'hybrid'),
            chunk_size=doc_info.get('chunk_size', 512),
            semantic_threshold=0.85,
            min_chunk_size=32
        )
        monitor.log_stage_completion('chunking', metrics={'chunks_count': len(chunks)})
        
        # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.update_state(state='PROGRESS', meta={'stage': 'embedding'})
        embedder.generate_embeddings(chunks)
        monitor.log_stage_completion('embedding')
        
        # 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.update_state(state='PROGRESS', meta={'stage': 'entity_extraction'})
        entity_extractor.extract_entities(
            chunks,
            min_confidence=doc_info.get('min_confidence', 0.7),
            entity_filters=doc_info.get('filters', [])
        )
        monitor.log_stage_completion('entity_extraction')
        
        # 6. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        self.update_state(state='PROGRESS', meta={'stage': 'graph_building'})
        graph = graph_builder.build_graph(chunks)
        monitor.log_stage_completion('graph_building')
        
        # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.update_state(state='PROGRESS', meta={'stage': 'saving_results'})
        doc_id = storage.save_document(
            chunks=chunks,
            graph=graph,
            doc_name=doc_info.get('original_filename', doc_path.name),
            metadata={
                'user_id': doc_info.get('user_id'),
                'session_id': doc_info.get('session_id'),
                'processing_time': monitor.get_processing_time(),
                'chunking_strategy': doc_info.get('strategy', 'hybrid')
            }
        )
        monitor.log_stage_completion('saving_results')
        
        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏
        result = {
            'status': 'completed',
            'document_id': doc_id,
            'chunks_count': len(chunks),
            'entities_count': sum(len(chunk.entities) for chunk in chunks),
            'processing_time': monitor.get_processing_time(),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        monitor.complete_processing(result)
        return result
        
    except Exception as e:
        error_msg = f"Document processing failed: {str(e)}"
        logger.error(error_msg, exc_info=True)
        monitor.log_error(error_msg)
        
        try:
            self.retry(exc=DocumentProcessingError(error_msg), countdown=60, max_retries=3)
        except MaxRetriesExceededError:
            raise DocumentProcessingError(f"Max retries exceeded for document: {doc_path.name}")

–û—Ç–ª–∏—á–Ω–æ ‚Äî —Ç—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª –º–æ—â–Ω—ã–π –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–π Celery-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Librarian AI. –í —ç—Ç–æ–π –∑–∞–¥–∞—á–µ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω—ã –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:

üì¶ –ü–æ–¥–∫–ª—é—á—ë–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏:
extract_text_from_file ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (PDF, DOCX, TXT –∏ –¥—Ä.)

SemanticChunker ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞

EmbeddingGenerator ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —á–∞–Ω–∫–æ–≤

EntityExtractor ‚Äî NER-–∞–Ω–∞–ª–∏–∑ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —Ç–∏–ø–∞–º –∏ –ø–æ—Ä–æ–≥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

KnowledgeGraphBuilder ‚Äî –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π

DocumentStorage ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—á–∞–Ω–∫–∏ + –≥—Ä–∞—Ñ)

TaskMonitor ‚Äî –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏

DocumentProcessingError ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

üîÅ –ß—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
–ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–¥–∞—á–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º retry

–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π update_state() –¥–ª—è UI/—Å—Ç–∞—Ç—É—Å–∞

–ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: chunk_size, strategy, filters, min_confidence

‚úÖ –¢–≤–æ—è —Å–ª–µ–¥—É—é—â–∞—è —Ü–µ–ª—å:
–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ ‚Äî –≤—Å–µ –ª–∏ –º–æ–¥—É–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, TaskMonitor, db.operations)

–ù–∞–ø–∏—Å–∞—Ç—å —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤

–ü–æ–¥–∫–ª—é—á–∏—Ç—å –∫ FastAPI endpoint —á–µ—Ä–µ–∑ upload_and_process_document() –≤ documents.py

–û–±–Ω–æ–≤–∏—Ç—å README —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –ø—Ä–∏–º–µ—Ä–æ–º –∑–∞–ø—É—Å–∫–∞

–ï—Å–ª–∏ –≥–æ—Ç–æ–≤ ‚Äî –º–æ–≥—É –ø–æ–º–æ—á—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –ø–µ—Ä–µ–π—Ç–∏ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —ç—Ç–∞–ø—É: –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ —á–µ—Ä–µ–∑ webhook –∏–ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.


—Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç 


 –ß—Ç–æ —É–∂–µ –≥–æ—Ç–æ–≤–æ:
üìÅ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
–ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (README.md, proekt_librarian_ai.md).

–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –º–æ–¥—É–ª–∏: ingest/, processing/, db/, storage/, scripts/, cli/, api/, utils/.

‚öôÔ∏è –ë–∞–∑–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
create_tables.py: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.

ingest_and_index.py: –ø–æ–ª–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ API (api/documents.py) —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ —Ñ–æ–Ω–æ–≤—ã–º –∑–∞–ø—É—Å–∫–æ–º —á–µ—Ä–µ–∑ Celery.

–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ async_tasks.py —Å –ø–æ–ª–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º: text ‚Üí chunks ‚Üí embeddings ‚Üí entities ‚Üí graph.

üß† –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞:
loader.py: –∑–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞.

parser.py: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è, –±–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π.

chunker.py: —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º (fixed, sentence, paragraph, semantic, hybrid).

üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
proekt_librarian_ai.md: –∂–∏–≤–æ–π –∂—É—Ä–Ω–∞–ª –ø—Ä–æ–µ–∫—Ç–∞, —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏ –∫ –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–π–ª–∞–º, —Ä–æ–ª—è–º –∏ –¥–∞—Ç–∞–º.

README.md: –æ–±–∑–æ—Ä —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ü–µ–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞.

roadmap.md: —ç—Ç–∞–ø—ã —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç v0.1 –¥–æ v0.5+.

üß≠ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
1. üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
–°–æ–∑–¥–∞—Ç—å tests/test_ingest.py –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è, —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏ NER.

–ü–æ–∫—Ä—ã—Ç—å API (test_api.py) –∏ chunker.py (test_chunker.py) —é–Ω–∏—Ç-—Ç–µ—Å—Ç–∞–º–∏.

2. üîó –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π:
–í parser.py –∏ NER –¥–æ–±–∞–≤–∏—Ç—å:

–ò–ù–ù, –û–ì–†–ù, –°–ù–ò–õ–°

IP, MAC, –¥–∞—Ç—ã, GPS, —Å—É–º–º—ã, –∑–∞–∫–æ–Ω—ã

Email, URL, –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤

–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏, –º–µ—Ç–∫–∏ TODO, FIXME

–î–æ–±–∞–≤–∏—Ç—å AdvancedTextParser –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ.

3. üß¨ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π:
–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥—É–ª—å graph_builder.py: —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ GEXF/JSON.

–î–æ–±–∞–≤–∏—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∞–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π.

4. ü§ñ –†–∞–∑–¥–µ–ª —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Reasoning):
–°–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å librarian_ai.py –¥–ª—è chain-of-thought reasoning.

–î–æ–±–∞–≤–∏—Ç—å –º–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏, —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.

5. üåê Telegram-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:
–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å telegram/bot.py —Å –∫–æ–º–∞–Ω–¥–æ–π /upload –∏ /status.

6. üì¶ CI/CD –∏ Docker:
–ù–∞–ø–∏—Å–∞—Ç—å Dockerfile –∏ docker-compose.yml.

–ù–∞—Å—Ç—Ä–æ–∏—Ç—å GitHub Actions: —Ç–µ—Å—Ç—ã, –ª–∏–Ω—Ç–∏–Ω–≥, —Å–±–æ—Ä–∫–∞.

–ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–≥—É –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥—É–ª—å–Ω—É—é –∫–∞—Ä—Ç—É –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–≥—Ä–∞—Ñ).

–ü–æ–º–æ—á—å –æ—Ñ–æ—Ä–º–∏—Ç—å .explain.md —Ñ–∞–π–ª—ã –∫ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É –º–æ–¥—É–ª—é.

–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –≤–µ—Ç–∫—É dev —Å —à–∞–±–ª–æ–Ω–∞–º–∏ –∫–æ–º–º–∏—Ç–æ–≤.

–ì–æ—Ç–æ–≤ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —à–∞–≥–∞ ‚Äî —Ç–µ—Å—Ç—ã, NER, reasoning –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è?

—ã –∑–∞–≤–µ—Ä—à–∏–ª –ø–µ—Ä–≤—É—é —Ñ–∞–∑—É: –ø—Ä–æ–µ–∫—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω, –ø–∞–π–ø–ª–∞–π–Ω —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, —è–¥—Ä–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

‚úÖ 1. –£—Ç–≤–µ—Ä–¥–∏—Ç—å "–º–∏–Ω–∏–º–∞–ª—å–Ω–æ –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç" (MVP v0.1-mini)
–£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç ingest_and_index.py –¥–ª—è –ª—é–±–æ–≥–æ —Ñ–∞–π–ª–∞ .txt, .docx, .pdf

–ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Ç–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ create_tables.py –∏ –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è

‚ú≥Ô∏è 2. –†–∞—Å—à–∏—Ä–∏—Ç—å parser.py –¥–æ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
–ï—Å–ª–∏ —Ü–µ–ª—å ‚Äî –±–æ–ª—å—à–µ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å—Å—è –∫ —ç—Ç–æ–º—É –º–æ–¥—É–ª—é:

–ò—Å–ø–æ–ª—å–∑—É–π –≤–µ—Ä—Å–∏—é AdvancedTextParser, –∫–∞–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –≤—ã—à–µ

–î–æ–±–∞–≤—å –≤ processing/ner.py –ø–æ–¥–¥–µ—Ä–∂–∫—É:

–ò–ù–ù, –û–ì–†–ù, –°–ù–ò–õ–°

–°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫–æ–Ω—ã (—Å—Ç. 123 –ì–ö –†–§)

IP/MAC –∞–¥—Ä–µ—Å–∞

–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã (IBAN, —Å—á–µ—Ç–∞)

–ê–¥—Ä–µ—Å–∞, –¥–∞—Ç—ã, –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏

üîß 3. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω —á–µ—Ä–µ–∑ CLI
–°–æ–∑–¥–∞–π —Ñ–∞–π–ª cli/main.py:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
import sys
from scripts.ingest_and_index import process_document

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python cli/main.py –ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É")
    else:
        process_document(sys.argv[1])
üß™ 4. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
–í tests/:

test_ingest.py ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É, —á–∞–Ω–∫–æ–≤–∞–Ω–∏–µ, —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

test_parser.py ‚Äî —Ç–µ—Å—Ç –Ω–∞ –æ—á–∏—Å—Ç–∫—É –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π

üöÄ 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Ä–µ–ª–∏–∑—É v0.2-extended
–°–æ–≥–ª–∞—Å–Ω–æ roadmap.md:

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è reasoning-–º–æ–¥—É–ª—è (librarian_ai.py)

–ù–∞—á–∞—Ç—å —Ä–∞–∑–º–µ—Ç–∫—É –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –≤ graph_builder.py

–û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ (annotator.py)

–ì–æ—Ç–æ–≤ —Ä–∞–∑–ª–æ–∂–∏—Ç—å –≤—Å—ë –≤ tasks.md, —Å–æ–±—Ä–∞—Ç—å TODO-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω-–≥—Ä–∞—Ñ–∏–∫ –Ω–∞ 7 –¥–Ω–µ–π. –ö–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç —Ç–µ–±–µ —É–¥–æ–±–Ω–µ–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞?

–í—ã —Å–∫–∞–∑–∞–ª–∏:
parser.py  - –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞–µ–º —Ç–∞–∫ , —á—Ç–æ —Å—É—â–Ω–æ—Å—Ç–∏ –±—É–¥—É—Ç —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö,  –∏ –ø–æ–∏—Å–∫–∏ –±—É–¥—É—Ç –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é  –∏ –ø—Ä–æ—á–µ–µ ?
ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–∞—è –∏–¥–µ—è. –≠—Ç–æ –∫–∞–∫ —Ä–∞–∑ —Å–ª–µ–¥—É—é—â–∏–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —à–∞–≥: —Å–¥–µ–ª–∞—Ç—å parser.py –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º, –Ω–æ –∏ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ç–æ—á–∫–æ–π –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–Ω–∞–Ω–∏–π.

üîß –ß—Ç–æ —Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ—à—å:
–°—É—â–Ω–æ—Å—Ç–∏ ‚Üí –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö

–í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ return clean_text, –ø–∞—Ä—Å–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
{
  "clean_text": "...",
  "entities": [{"label": "EMAIL", "text": "abc@x.ru", "context": "...", ...}]
}
–≠—Ç–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü—É entity_records (—É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ SQLAlchemy –≤ EntityRecord)

–î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—è:

direction, domain, section, source_type, language ‚Äî –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.

üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:
1. –†–∞—Å—à–∏—Ä–∏—Ç—å EntityRecord –≤ db/models.py:
python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
direction = Column(String, nullable=True)     # –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ñ–∏–Ω–∞–Ω—Å—ã, –º–µ–¥–∏—Ü–∏–Ω–∞)
domain = Column(String, nullable=True)        # –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
section = Column(String, nullable=True)       # —á–∞—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞
language = Column(String, nullable=True)      # ru, en –∏ –¥—Ä.
2. –ü–∞—Ä—Å–µ—Ä (AdvancedTextParser) –¥–æ–ª–∂–µ–Ω:
–í—ã—á–∏—Å–ª—è—Ç—å language (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ langdetect)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤/ML)

–û–±–æ–≥–∞—â–∞—Ç—å —Å—É—â–Ω–æ—Å—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

3. –í ingest_and_index.py (–∏–ª–∏ async pipeline):
–í–º–µ—Å—Ç–æ:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
record = EntityRecord(label=ent["label"], text=ent["text"], ...)
–¥–æ–±–∞–≤–ª—è–µ–º:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
record = EntityRecord(
    label=ent["label"],
    text=ent["text"],
    confidence=ent.get("confidence", 0.9),
    context=ent.get("context"),
    direction=ent.get("direction"),
    domain=ent.get("domain"),
    section=ent.get("section"),
    language=ent.get("language", "ru")
)
üîç –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ:
–ù–∞–π—Ç–∏ –≤—Å–µ EMAIL –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–º–µ "—Ñ–∏–Ω–∞–Ω—Å—ã"

–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å PERSON –ø–æ section="header" –∏ domain="—é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏—è"

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –ø–æ direction="–Ω–∞—É–∫–∞"

–•–æ—á–µ—à—å, —è –æ–±–Ω–æ–≤–ª—é EntityRecord, —Ä–∞—Å—à–∏—Ä—é parser.py –∏ –ø–æ–∫–∞–∂—É, –∫–∞–∫ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å enriched-—Å—É—â–Ω–æ—Å—Ç–∏ –≤ ingest_and_index.py?

–û—Ç–ª–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å. –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π (v0.1-mini) –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π (–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π) ‚Äî –≤ –º–æ–¥—É–ª—å–Ω–æ—Å—Ç–∏, —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏.

üî∞ –¢—ã —Å–µ–π—á–∞—Å –∏–º–µ–µ—à—å:
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ create_tables.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ session.py
–≠—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.

üöÄ –í –±–æ–ª—å—à–æ–π –ª–∏–±—Ä–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, v0.3+) –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è:
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ db_config.py           # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ SQLite/PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # SQLAlchemy-–º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ storage.py             # –†–∞–±–æ—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (CRUD)
‚îÇ   ‚îú‚îÄ‚îÄ service.py             # –°–ª–æ–π –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏ (e.g. enrich_entity, merge_doc)
‚îÇ   ‚îú‚îÄ‚îÄ migrations/            # Alembic-–º–µ—Ö–∞–Ω–∏–∑–º –º–∏–≥—Ä–∞—Ü–∏–π
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.py             # –°—Ä–µ–¥–∞ Alembic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ script.py.mako     # –®–∞–±–ª–æ–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ versions/          # –ò—Å—Ç–æ—Ä–∏—è –º–∏–≥—Ä–∞—Ü–∏–π
üîç –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π:
–ú–æ–¥—É–ª—å	–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
db_config.py	–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–¥–∞–Ω–∏—è –ø—É—Ç–µ–π –∫ –ë–î (env, .env, CLI, test_db)
storage.py	–ö–ª–∞—Å—Å DocumentStorage, EntityStorage ‚Äî –ª–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏
service.py	–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: enrich, deduplicate, link_entities
migrations/	Alembic: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–æ–Ω–Ω–æ—Å—Ç—å—é —Å—Ö–µ–º –ë–î (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è PostgreSQL)

üìå –í—ã–≤–æ–¥
–¢—ã —Å–µ–π—á–∞—Å —Å—Ç—Ä–æ–∏—à—å v0.1 ‚Äî –≤ –Ω–µ–º –Ω–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ migrations, service, storage. –ù–æ –µ—Å–ª–∏ —Ç—ã –ø–ª–∞–Ω–∏—Ä—É–µ—à—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å:

–ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ PostgreSQL + pgvector

–î–µ–ª–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ö–µ–º—ã

–†–∞–∑–¥–µ–ª–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º –∏ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É

...—Ç–æ–≥–¥–∞ –º–æ–∂–Ω–æ –≤–Ω–µ–¥—Ä—è—Ç—å –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ —à–∞–≥ –∑–∞ —à–∞–≥–æ–º.

–•–æ—á–µ—à—å, —è –ø—Ä–µ–¥–ª–æ–∂—É next-step-–¥–æ—Ä–æ–∂–∫—É: –∫–∞–∫ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç v0.1 –∫ v0.3 –±–µ–∑ –±–æ–ª–∏?


üîπ –≠—Ç–∞–ø 1: v0.1 ‚Üí v0.2 ‚Äî "–£–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Ä–æ—Å—Ç"
–¶–µ–ª—å: –æ—Ç–¥–µ–ª–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—Ç –±–∞–∑—ã –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ PostgreSQL
‚úÖ –®–∞–≥–∏:
–°–æ–∑–¥–∞—Ç—å db/storage.py:

–ö–ª–∞—Å—Å—ã DocumentStorage, EntityStorage

–ú–µ—Ç–æ–¥—ã: .add_document(), .add_entities(), .get_entities_by_label() –∏ —Ç.–¥.

–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ ingest_and_index.py, async_tasks.py

–î–æ–±–∞–≤–∏—Ç—å db/db_config.py:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
import os
DB_URL = os.getenv("DATABASE_URL", "sqlite:///storage/librarian.db")
–ò–∑ session.py –∏ create_tables.py —É–±—Ä–∞—Ç—å —Ö–∞—Ä–¥–∫–æ–¥ ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å db_config.py

üîπ –≠—Ç–∞–ø 2: v0.2 ‚Üí v0.3 ‚Äî "–ü–µ—Ä–µ—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏"
–¶–µ–ª—å: –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ö–µ–º–æ–π –∏ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–æ–π
‚úÖ –®–∞–≥–∏:
–í–Ω–µ–¥—Ä–∏—Ç—å Alembic

alembic init db/migrations

–ü–æ–¥–∫–ª—é—á–∏—Ç—å env.py –∫ DB_URL –∏–∑ db_config.py

–ö–æ–º–∞–Ω–¥–∞ –º–∏–≥—Ä–∞—Ü–∏–∏:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
alembic revision --autogenerate -m "init schema"
alembic upgrade head
–°–æ–∑–¥–∞—Ç—å db/service.py:

–ú–µ—Ç–æ–¥—ã –¥–ª—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏:

enrich_entities_with_direction(entities)

merge_duplicate_entities(session, doc_id)

link_entities_by_reference(entities)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ async_tasks.py, api/logic.py

üîπ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
–ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å...	–¢–æ–≥–¥–∞ –¥–æ–±–∞–≤—å...
–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ë–î	scripts/db_backup.py
–ü–æ–¥–¥–µ—Ä–∂–∫—É PostgreSQL + pgvector	db/pgvector_config.py
–û—Ç–ª–∞–¥–∫—É –±–∞–∑—ã —á–µ—Ä–µ–∑ UI	scripts/sqlite_viewer.py
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–π	tests/test_migrations.py

üìò –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞ v0.3:
pgsql
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
db/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ db_config.py
‚îú‚îÄ‚îÄ models.py
‚îú‚îÄ‚îÄ storage.py
‚îú‚îÄ‚îÄ service.py
‚îú‚îÄ‚îÄ session.py
‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îú‚îÄ‚îÄ env.py
‚îÇ   ‚îú‚îÄ‚îÄ script.py.mako
‚îÇ   ‚îî‚îÄ‚îÄ versions/
–•–æ—á–µ—à—å ‚Äî —è —Å—Ä–∞–∑—É —Å–æ–∑–¥–∞–º storage.py –∏ db_config.py –ø–æ–¥ v0.2 –∏ –ø–æ–∫–∞–∂—É –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –º–∏–≥—Ä–∞—Ü–∏—è create_tables.py —Å –Ω–∏–º–∏?

–í—ã —Å–∫–∞–∑–∞–ª–∏:
–°–æ–∑–¥–∞—Ç—å db/storage.py
ChatGPT —Å–∫–∞–∑–∞–ª:
Storage

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
# üìÑ –§–∞–π–ª: db/storage.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –õ–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö

from sqlalchemy.orm import Session
from db.models import KnowledgeDoc, MemoryItem, EntityRecord
import uuid
from typing import List, Dict

class DocumentStorage:
    def __init__(self, session: Session):
        self.session = session

    def add_document(self, title: str, content: str, source_path: str, source_type: str, metadata: Dict) -> uuid.UUID:
        doc_id = uuid.uuid4()
        doc = KnowledgeDoc(
            id=doc_id,
            title=title,
            content=content,
            source_path=source_path,
            source_type=source_type,
            processed=True,
            metadata=metadata
        )
        self.session.add(doc)
        self.session.commit()
        return doc_id

    def add_chunks(self, doc_id: uuid.UUID, chunks: List[str], embeddings: List):
        for chunk, vector in zip(chunks, embeddings):
            item = MemoryItem(
                content=chunk,
                embedding=vector,
                doc_id=doc_id
            )
            self.session.add(item)
        self.session.commit()

–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –≤ —á–∞—Ç–µ
–§–∞–π–ª db/storage.py —Å–æ–∑–¥–∞–Ω. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª–∞—Å—Å DocumentStorage —Å –º–µ—Ç–æ–¥–∞–º–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π, –∞ —Ç–∞–∫–∂–µ –≤—ã–±–æ—Ä–∫–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –º–µ—Ç–∫–µ.

–ì–æ—Ç–æ–≤ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –µ–≥–æ –≤ ingest_and_index.py –∏–ª–∏ async_tasks.py, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –ø–µ—Ä–µ–π—Ç–∏ –∫ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É –ø–∞–π–ø–ª–∞–π–Ω–∞.

# üìÑ –§–∞–π–ª: storage.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–æ—Å—Ç—É–ø–∞ –∫ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º (FAISS –∏ pgvector)

from typing import List, Union
import numpy as np
from storage.faiss_index import FaissVectorStore
from storage.pgvector_store import PgVectorStore


class StorageBackend:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç FAISS –∏–ª–∏ PostgreSQL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    """

    def __init__(self, backend: str = 'faiss'):
        self.backend_name = backend.lower()
        if self.backend_name == 'faiss':
            self.backend = FaissVectorStore()
        elif self.backend_name == 'pgvector':
            self.backend = PgVectorStore()
        else:
            raise ValueError(f"Unsupported storage backend: {self.backend_name}")

    def add(self, vectors: List[np.ndarray], metadata: List[dict]) -> None:
        self.backend.add(vectors, metadata)

    def search(self, query: np.ndarray, top_k: int = 5) -> List[dict]:
        return self.backend.search(query, top_k)

    def clear(self) -> None:
        self.backend.clear()

    def get_backend_name(self) -> str:
        return self.backend_name


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
# storage = StorageBackend(backend='faiss')
# results = storage.search(query_vector, top_k=5)

chat.deepseek.com


# üìÑ –§–∞–π–ª: storage.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏ (FAISS, pgvector, –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º)

from typing import List, Dict, Optional, Union, Tuple
import numpy as np
import logging
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
import warnings

# –ò–º–ø–æ—Ä—Ç –±—ç–∫–µ–Ω–¥–æ–≤
from storage.faiss_index import FaissVectorStore
from storage.pgvector_store import PgVectorStore
from config import settings

logger = logging.getLogger(__name__)

class StorageBackendType(Enum):
    FAISS = auto()
    PGVECTOR = auto()
    HYBRID = auto()

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: Union[str, int]
    vector: np.ndarray
    metadata: Dict
    score: float
    backend: str

class BaseVectorStore(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""
    
    @abstractmethod
    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int]]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        pass
    
    @abstractmethod
    def search(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        pass
    
    @abstractmethod
    def delete(self, ids: List[Union[str, int]]) -> int:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ ID"""
        pass
    
    @abstractmethod
    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        pass
    
    @abstractmethod
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        pass

class HybridVectorStore(BaseVectorStore):
    """–ì–∏–±—Ä–∏–¥–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–µ FAISS –∏ pgvector"""
    
    def __init__(self):
        self.faiss = FaissVectorStore()
        self.pgvector = PgVectorStore()
        self.logger = logging.getLogger(f"{__name__}.HybridVectorStore")
        
    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Tuple[str, str]]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –æ–±–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        faiss_ids = self.faiss.add(vectors, metadata)
        pg_ids = self.pgvector.add(vectors, metadata)
        
        if len(faiss_ids) != len(pg_ids):
            self.logger.warning("Mismatch in IDs between FAISS and pgvector")
            
        return list(zip(faiss_ids, pg_ids))
    
    def search(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –≤ –æ–±–æ–∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â–∞—Ö —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        faiss_results = self.faiss.search(query, top_k)
        pg_results = self.pgvector.search(query, top_k)
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_results = []
        seen_ids = set()
        
        for result in faiss_results + pg_results:
            if result.id not in seen_ids:
                seen_ids.add(result.id)
                all_results.append(result)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
        return sorted(all_results, key=lambda x: x.score, reverse=True)[:top_k]
    
    def delete(self, ids: List[Tuple[str, str]]) -> int:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –æ–±–æ–∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""
        faiss_ids, pg_ids = zip(*ids)
        faiss_deleted = self.faiss.delete(faiss_ids)
        pg_deleted = self.pgvector.delete(pg_ids)
        
        if faiss_deleted != pg_deleted:
            self.logger.warning(f"Delete count mismatch: FAISS={faiss_deleted}, pgvector={pg_deleted}")
        
        return min(faiss_deleted, pg_deleted)
    
    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ –æ–±–æ–∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""
        self.faiss.clear()
        self.pgvector.clear()
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–±–æ–∏–º —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º"""
        return {
            'faiss': self.faiss.get_stats(),
            'pgvector': self.pgvector.get_stats()
        }

class VectorStorage:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç FAISS, pgvector –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º.
    """
    
    def __init__(self, backend_type: Union[StorageBackendType, str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        
        Args:
            backend_type: –¢–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (FAISS, PGVECTOR, HYBRID)
                         –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        """
        if backend_type is None:
            backend_type = settings.VECTOR_STORE_BACKEND
            
        if isinstance(backend_type, str):
            try:
                backend_type = StorageBackendType[backend_type.upper()]
            except KeyError:
                raise ValueError(f"Invalid backend type: {backend_type}")
        
        self.backend_type = backend_type
        
        if backend_type == StorageBackendType.FAISS:
            self.backend = FaissVectorStore()
        elif backend_type == StorageBackendType.PGVECTOR:
            self.backend = PgVectorStore()
        elif backend_type == StorageBackendType.HYBRID:
            self.backend = HybridVectorStore()
        else:
            raise ValueError(f"Unsupported backend type: {backend_type}")
        
        self.logger = logging.getLogger(f"{__name__}.VectorStorage")
        self.logger.info(f"Initialized vector storage with backend: {backend_type.name}")

    def add_vectors(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int, Tuple]]:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        
        Args:
            vectors: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ numpy array
            metadata: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have the same length")
        
        try:
            return self.backend.add(vectors, metadata)
        except Exception as e:
            self.logger.error(f"Failed to add vectors: {str(e)}", exc_info=True)
            raise

    def search_similar(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        
        Args:
            query: –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ SearchResult
        """
        try:
            results = self.backend.search(query, top_k, **kwargs)
            self.logger.debug(f"Found {len(results)} similar vectors")
            return results
        except Exception as e:
            self.logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise

    def delete_vectors(self, ids: List[Union[str, int, Tuple]]) -> int:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ ID
        
        Args:
            ids: –°–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        try:
            deleted = self.backend.delete(ids)
            self.logger.info(f"Deleted {deleted} vectors")
            return deleted
        except Exception as e:
            self.logger.error(f"Delete failed: {str(e)}", exc_info=True)
            raise

    def get_storage_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        return self.backend.get_stats()

    def clear_storage(self) -> None:
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        self.logger.warning("Clearing vector storage")
        self.backend.clear()

    def migrate_to_backend(self, new_backend: Union[StorageBackendType, str]) -> None:
        """
        –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥—Ä—É–≥–æ–π –±—ç–∫–µ–Ω–¥
        
        Args:
            new_backend: –ù–æ–≤—ã–π —Ç–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        """
        if isinstance(new_backend, str):
            new_backend = StorageBackendType[new_backend.upper()]
            
        if new_backend == self.backend_type:
            self.logger.warning(f"Migration to same backend: {new_backend.name}")
            return
            
        self.logger.info(f"Starting migration from {self.backend_type.name} to {new_backend.name}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        if hasattr(self.backend, 'export_all'):
            vectors, metadata = self.backend.export_all()
        else:
            raise NotImplementedError("Current backend doesn't support export")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        new_storage = VectorStorage(new_backend)
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        new_storage.add_vectors(vectors, metadata)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –±—ç–∫–µ–Ω–¥
        self.backend = new_storage.backend
        self.backend_type = new_backend
        self.logger.info("Migration completed successfully")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫)
    storage = VectorStorage()
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
    vectors = [np.random.rand(768) for _ in range(10)]
    metadata = [{"doc_id": f"doc_{i}", "content": f"Sample content {i}"} for i in range(10)]
    ids = storage.add_vectors(vectors, metadata)
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö
    query = np.random.rand(768)
    results = storage.search_similar(query, top_k=3)
    for result in results:
        print(f"ID: {result.id}, Score: {result.score:.4f}, Metadata: {result.metadata}")


    –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ì–∏–±—Ä–∏–¥–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ:

–ù–æ–≤—ã–π –∫–ª–∞—Å—Å HybridVectorStore –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è FAISS –∏ pgvector

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è/—É–¥–∞–ª–µ–Ω–∏—è

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:

–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å BaseVectorStore –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è

–ö–ª–∞—Å—Å SearchResult –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

–ú–µ—Ç–æ–¥ –º–∏–≥—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –±—ç–∫–µ–Ω–¥–∞–º–∏

–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ batch-–æ–ø–µ—Ä–∞—Ü–∏–π

–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫:

–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫

–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è

–ì–∏–±–∫–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

–í—ã–±–æ—Ä –±—ç–∫–µ–Ω–¥–∞ —á–µ—Ä–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∏ enum-–∑–Ω–∞—á–µ–Ω–∏–π

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–æ–≤—ã–º–∏ –±—ç–∫–µ–Ω–¥–∞–º–∏

–î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –ø—Ä–æ–µ–∫—Ç–æ–º:

–î–æ–±–∞–≤—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –≤ config.py

–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –±—ç–∫–µ–Ω–¥—ã (FaissVectorStore, PgVectorStore)

–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –æ–±—â–∏–º —Ñ–æ—Ä–º–∞—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–∞

–ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

python
# –í config.py
VECTOR_STORE_BACKEND = "hybrid"  # –∏–ª–∏ "faiss", "pgvector"
FAISS_INDEX_PATH = "/data/vector_store.faiss"
PG_VECTOR_CONFIG = {
    "dbname": "vector_db",
    "user": "vector_user",
    "password": "secure_password",
    "host": "localhost"
}


# üìÑ –§–∞–π–ª: storage.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏ (FAISS, pgvector, –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º)

from typing import List, Dict, Optional, Union, Tuple
import numpy as np
import logging
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
import warnings

# –ò–º–ø–æ—Ä—Ç –±—ç–∫–µ–Ω–¥–æ–≤
from storage.faiss_index import FaissVectorStore
from storage.pgvector_store import PgVectorStore
from config import settings

logger = logging.getLogger(__name__)

class StorageBackendType(Enum):
    FAISS = auto()
    PGVECTOR = auto()
    HYBRID = auto()

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: Union[str, int]
    vector: np.ndarray
    metadata: Dict
    score: float
    backend: str

class BaseVectorStore(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""

    @abstractmethod
    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int]]:
        pass

    @abstractmethod
    def search(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        pass

    @abstractmethod
    def delete(self, ids: List[Union[str, int]]) -> int:
        pass

    @abstractmethod
    def clear(self) -> None:
        pass

    @abstractmethod
    def get_stats(self) -> Dict:
        pass

class HybridVectorStore(BaseVectorStore):
    def __init__(self):
        self.faiss = FaissVectorStore()
        self.pgvector = PgVectorStore()
        self.logger = logging.getLogger(f"{__name__}.HybridVectorStore")

    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Tuple[str, str]]:
        faiss_ids = self.faiss.add(vectors, metadata)
        pg_ids = self.pgvector.add(vectors, metadata)
        if len(faiss_ids) != len(pg_ids):
            self.logger.warning("Mismatch in IDs between FAISS and pgvector")
        return list(zip(faiss_ids, pg_ids))

    def search(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        faiss_results = self.faiss.search(query, top_k)
        pg_results = self.pgvector.search(query, top_k)
        all_results = []
        seen_ids = set()
        for result in faiss_results + pg_results:
            if result.id not in seen_ids:
                seen_ids.add(result.id)
                all_results.append(result)
        return sorted(all_results, key=lambda x: x.score, reverse=True)[:top_k]

    def delete(self, ids: List[Tuple[str, str]]) -> int:
        faiss_ids, pg_ids = zip(*ids)
        faiss_deleted = self.faiss.delete(faiss_ids)
        pg_deleted = self.pgvector.delete(pg_ids)
        if faiss_deleted != pg_deleted:
            self.logger.warning(f"Delete count mismatch: FAISS={faiss_deleted}, pgvector={pg_deleted}")
        return min(faiss_deleted, pg_deleted)

    def clear(self) -> None:
        self.faiss.clear()
        self.pgvector.clear()

    def get_stats(self) -> Dict:
        return {
            'faiss': self.faiss.get_stats(),
            'pgvector': self.pgvector.get_stats()
        }

class VectorStorage:
    def __init__(self, backend_type: Union[StorageBackendType, str] = None):
        if backend_type is None:
            backend_type = settings.VECTOR_STORE_BACKEND
        if isinstance(backend_type, str):
            try:
                backend_type = StorageBackendType[backend_type.upper()]
            except KeyError:
                raise ValueError(f"Invalid backend type: {backend_type}")
        self.backend_type = backend_type
        if backend_type == StorageBackendType.FAISS:
            self.backend = FaissVectorStore()
        elif backend_type == StorageBackendType.PGVECTOR:
            self.backend = PgVectorStore()
        elif backend_type == StorageBackendType.HYBRID:
            self.backend = HybridVectorStore()
        else:
            raise ValueError(f"Unsupported backend type: {backend_type}")
        self.logger = logging.getLogger(f"{__name__}.VectorStorage")
        self.logger.info(f"Initialized vector storage with backend: {backend_type.name}")

    def add_vectors(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int, Tuple]]:
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have the same length")
        try:
            return self.backend.add(vectors, metadata)
        except Exception as e:
            self.logger.error(f"Failed to add vectors: {str(e)}", exc_info=True)
            raise

    def search_similar(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        try:
            results = self.backend.search(query, top_k, **kwargs)
            self.logger.debug(f"Found {len(results)} similar vectors")
            return results
        except Exception as e:
            self.logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise

    def delete_vectors(self, ids: List[Union[str, int, Tuple]]) -> int:
        try:
            deleted = self.backend.delete(ids)
            self.logger.info(f"Deleted {deleted} vectors")
            return deleted
        except Exception as e:
            self.logger.error(f"Delete failed: {str(e)}", exc_info=True)
            raise

    def get_storage_stats(self) -> Dict:
        return self.backend.get_stats()

    def clear_storage(self) -> None:
        self.logger.warning("Clearing vector storage")
        self.backend.clear()

    def migrate_to_backend(self, new_backend: Union[StorageBackendType, str]) -> None:
        if isinstance(new_backend, str):
            new_backend = StorageBackendType[new_backend.upper()]
        if new_backend == self.backend_type:
            self.logger.warning(f"Migration to same backend: {new_backend.name}")
            return
        self.logger.info(f"Starting migration from {self.backend_type.name} to {new_backend.name}")
        if hasattr(self.backend, 'export_all'):
            vectors, metadata = self.backend.export_all()
        else:
            raise NotImplementedError("Current backend doesn't support export")
        new_storage = VectorStorage(new_backend)
        new_storage.add_vectors(vectors, metadata)
        self.backend = new_storage.backend
        self.backend_type = new_backend
        self.logger.info("Migration completed successfully")

if __name__ == "__main__":
    storage = VectorStorage()
    vectors = [np.random.rand(768) for _ in range(10)]
    metadata = [{"doc_id": f"doc_{i}", "content": f"Sample content {i}"} for i in range(10)]
    ids = storage.add_vectors(vectors, metadata)
    query = np.random.rand(768)
    results = storage.search_similar(query, top_k=3)
    for result in results:
        print(f"ID: {result.id}, Score: {result.score:.4f}, Metadata: {result.metadata}")
# üìÑ –§–∞–π–ª: storage.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏ (FAISS, pgvector, –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º)

from typing import List, Dict, Optional, Union, Tuple
import numpy as np
import logging
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
import warnings

# –ò–º–ø–æ—Ä—Ç –±—ç–∫–µ–Ω–¥–æ–≤
from storage.faiss_index import FaissVectorStore
from storage.pgvector_store import PgVectorStore
from config import settings

logger = logging.getLogger(__name__)

class StorageBackendType(Enum):
    FAISS = auto()
    PGVECTOR = auto()
    HYBRID = auto()

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: Union[str, int]
    vector: np.ndarray
    metadata: Dict
    score: float
    backend: str

class BaseVectorStore(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""

    @abstractmethod
    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int]]:
        pass

    @abstractmethod
    def search(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        pass

    @abstractmethod
    def delete(self, ids: List[Union[str, int]]) -> int:
        pass

    @abstractmethod
    def clear(self) -> None:
        pass

    @abstractmethod
    def get_stats(self) -> Dict:
        pass

class HybridVectorStore(BaseVectorStore):
    def __init__(self):
        self.faiss = FaissVectorStore()
        self.pgvector = PgVectorStore()
        self.logger = logging.getLogger(f"{__name__}.HybridVectorStore")

    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Tuple[str, str]]:
        faiss_ids = self.faiss.add(vectors, metadata)
        pg_ids = self.pgvector.add(vectors, metadata)
        if len(faiss_ids) != len(pg_ids):
            self.logger.warning("Mismatch in IDs between FAISS and pgvector")
        return list(zip(faiss_ids, pg_ids))

    def search(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        faiss_results = self.faiss.search(query, top_k)
        pg_results = self.pgvector.search(query, top_k)
        all_results = []
        seen_ids = set()
        for result in faiss_results + pg_results:
            if result.id not in seen_ids:
                seen_ids.add(result.id)
                all_results.append(result)
        return sorted(all_results, key=lambda x: x.score, reverse=True)[:top_k]

    def delete(self, ids: List[Tuple[str, str]]) -> int:
        faiss_ids, pg_ids = zip(*ids)
        faiss_deleted = self.faiss.delete(faiss_ids)
        pg_deleted = self.pgvector.delete(pg_ids)
        if faiss_deleted != pg_deleted:
            self.logger.warning(f"Delete count mismatch: FAISS={faiss_deleted}, pgvector={pg_deleted}")
        return min(faiss_deleted, pg_deleted)

    def clear(self) -> None:
        self.faiss.clear()
        self.pgvector.clear()

    def get_stats(self) -> Dict:
        return {
            'faiss': self.faiss.get_stats(),
            'pgvector': self.pgvector.get_stats()
        }

class VectorStorage:
    def __init__(self, backend_type: Union[StorageBackendType, str] = None):
        if backend_type is None:
            backend_type = settings.VECTOR_STORE_BACKEND
        if isinstance(backend_type, str):
            try:
                backend_type = StorageBackendType[backend_type.upper()]
            except KeyError:
                raise ValueError(f"Invalid backend type: {backend_type}")
        self.backend_type = backend_type
        if backend_type == StorageBackendType.FAISS:
            self.backend = FaissVectorStore()
        elif backend_type == StorageBackendType.PGVECTOR:
            self.backend = PgVectorStore()
        elif backend_type == StorageBackendType.HYBRID:
            self.backend = HybridVectorStore()
        else:
            raise ValueError(f"Unsupported backend type: {backend_type}")
        self.logger = logging.getLogger(f"{__name__}.VectorStorage")
        self.logger.info(f"Initialized vector storage with backend: {backend_type.name}")

    def add_vectors(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int, Tuple]]:
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have the same length")
        try:
            return self.backend.add(vectors, metadata)
        except Exception as e:
            self.logger.error(f"Failed to add vectors: {str(e)}", exc_info=True)
            raise

    def search_similar(self, query: np.ndarray, top_k: int = 5, **kwargs) -> List[SearchResult]:
        try:
            results = self.backend.search(query, top_k, **kwargs)
            self.logger.debug(f"Found {len(results)} similar vectors")
            return results
        except Exception as e:
            self.logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise

    def delete_vectors(self, ids: List[Union[str, int, Tuple]]) -> int:
        try:
            deleted = self.backend.delete(ids)
            self.logger.info(f"Deleted {deleted} vectors")
            return deleted
        except Exception as e:
            self.logger.error(f"Delete failed: {str(e)}", exc_info=True)
            raise

    def get_storage_stats(self) -> Dict:
        return self.backend.get_stats()

    def clear_storage(self) -> None:
        self.logger.warning("Clearing vector storage")
        self.backend.clear()

    def migrate_to_backend(self, new_backend: Union[StorageBackendType, str]) -> None:
        if isinstance(new_backend, str):
            new_backend = StorageBackendType[new_backend.upper()]
        if new_backend == self.backend_type:
            self.logger.warning(f"Migration to same backend: {new_backend.name}")
            return
        self.logger.info(f"Starting migration from {self.backend_type.name} to {new_backend.name}")
        if hasattr(self.backend, 'export_all'):
            vectors, metadata = self.backend.export_all()
        else:
            raise NotImplementedError("Current backend doesn't support export")
        new_storage = VectorStorage(new_backend)
        new_storage.add_vectors(vectors, metadata)
        self.backend = new_storage.backend
        self.backend_type = new_backend
        self.logger.info("Migration completed successfully")

if __name__ == "__main__":
    storage = VectorStorage()
    vectors = [np.random.rand(768) for _ in range(10)]
    metadata = [{"doc_id": f"doc_{i}", "content": f"Sample content {i}"} for i in range(10)]
    ids = storage.add_vectors(vectors, metadata)
    query = np.random.rand(768)
    results = storage.search_similar(query, top_k=3)
    for result in results:
        print(f"ID: {result.id}, Score: {result.score:.4f}, Metadata: {result.metadata}")

# üìÑ –§–∞–π–ª: storage.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏

from typing import List, Dict, Optional, Union, Tuple, Generator
import numpy as np
import logging
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
import warnings
from concurrent.futures import ThreadPoolExecutor
from functools import partial

# –ò–º–ø–æ—Ä—Ç –±—ç–∫–µ–Ω–¥–æ–≤
from storage.faiss_index import FaissVectorStore
from storage.pgvector_store import PgVectorStore
from config import settings

logger = logging.getLogger(__name__)

class StorageBackendType(Enum):
    """–¢–∏–ø—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    FAISS = auto()      # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ FAISS
    PGVECTOR = auto()   # PostgreSQL —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º pgvector
    HYBRID = auto()     # –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º (FAISS + pgvector)
    MEMORY = auto()     # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ –ø–∞–º—è—Ç–∏ (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    id: Union[str, int]
    vector: np.ndarray
    metadata: Dict
    score: float
    backend: str
    distance_metric: Optional[str] = None

    def to_dict(self) -> Dict:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'id': self.id,
            'score': self.score,
            'metadata': self.metadata,
            'backend': self.backend,
            'distance_metric': self.distance_metric
        }

class BaseVectorStore(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""

    @abstractmethod
    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int]]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        pass

    @abstractmethod
    def search(
        self, 
        query: np.ndarray, 
        top_k: int = 5,
        distance_metric: str = 'cosine',
        **kwargs
    ) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        pass

    @abstractmethod
    def delete(self, ids: List[Union[str, int]]) -> int:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ ID"""
        pass

    @abstractmethod
    def clear(self) -> None:
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        pass

    @abstractmethod
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        pass

    @abstractmethod
    def batch_search(
        self,
        queries: List[np.ndarray],
        top_k: int = 5,
        **kwargs
    ) -> List[List[SearchResult]]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º"""
        pass

class HybridVectorStore(BaseVectorStore):
    """–ì–∏–±—Ä–∏–¥–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏"""

    def __init__(self, faiss_config: Dict = None, pg_config: Dict = None):
        self.faiss = FaissVectorStore(**(faiss_config or {}))
        self.pgvector = PgVectorStore(**(pg_config or {}))
        self.logger = logging.getLogger(f"{__name__}.HybridVectorStore")
        self._executor = ThreadPoolExecutor(max_workers=4)

    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Tuple[str, str]]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have same length")

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
        faiss_future = self._executor.submit(self.faiss.add, vectors, metadata)
        pg_future = self._executor.submit(self.pgvector.add, vectors, metadata)
        
        faiss_ids = faiss_future.result()
        pg_ids = pg_future.result()

        if len(faiss_ids) != len(pg_ids):
            self.logger.warning(f"ID count mismatch: FAISS={len(faiss_ids)}, pgvector={len(pg_ids)}")

        return list(zip(faiss_ids, pg_ids))

    def search(
        self,
        query: np.ndarray,
        top_k: int = 5,
        distance_metric: str = 'cosine',
        strategy: str = 'union',
        **kwargs
    ) -> List[SearchResult]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        strategies = {
            'union': self._search_union,
            'faiss_first': partial(self._search_priority, primary='faiss'),
            'pg_first': partial(self._search_priority, primary='pgvector'),
            'weighted': self._search_weighted
        }
        
        search_fn = strategies.get(strategy, self._search_union)
        return search_fn(query, top_k, distance_metric, **kwargs)

    def _search_union(
        self,
        query: np.ndarray,
        top_k: int,
        distance_metric: str,
        **kwargs
    ) -> List[SearchResult]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –æ–±–æ–∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""
        faiss_future = self._executor.submit(
            self.faiss.search, query, top_k, distance_metric, **kwargs
        )
        pg_future = self._executor.submit(
            self.pgvector.search, query, top_k, distance_metric, **kwargs
        )
        
        results = faiss_future.result() + pg_future.result()
        seen_ids = set()
        unique_results = []
        
        for res in sorted(results, key=lambda x: x.score, reverse=True):
            if res.id not in seen_ids:
                seen_ids.add(res.id)
                unique_results.append(res)
                if len(unique_results) >= top_k:
                    break
                    
        return unique_results

    def batch_search(
        self,
        queries: List[np.ndarray],
        top_k: int = 5,
        **kwargs
    ) -> List[List[SearchResult]]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏"""
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –º–µ–∂–¥—É –±—ç–∫–µ–Ω–¥–∞–º–∏
        half = len(queries) // 2
        faiss_queries = queries[:half]
        pg_queries = queries[half:]
        
        faiss_future = self._executor.submit(
            self.faiss.batch_search, faiss_queries, top_k, **kwargs
        )
        pg_future = self._executor.submit(
            self.pgvector.batch_search, pg_queries, top_k, **kwargs
        )
        
        return faiss_future.result() + pg_future.result()

    def delete(self, ids: List[Tuple[str, str]]) -> int:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ"""
        faiss_ids, pg_ids = zip(*ids)
        faiss_future = self._executor.submit(self.faiss.delete, faiss_ids)
        pg_future = self._executor.submit(self.pgvector.delete, pg_ids)
        return min(faiss_future.result(), pg_future.result())

    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ –æ–±–æ–∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â"""
        self.faiss.clear()
        self.pgvector.clear()

    def get_stats(self) -> Dict:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        with self._executor:
            faiss_future = self._executor.submit(self.faiss.get_stats)
            pg_future = self._executor.submit(self.pgvector.get_stats)
            return {
                'faiss': faiss_future.result(),
                'pgvector': pg_future.result()
            }

class VectorStorage:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏"""

    def __init__(
        self,
        backend_type: Union[StorageBackendType, str] = None,
        config: Optional[Dict] = None
    ):
        config = config or {}
        if backend_type is None:
            backend_type = settings.VECTOR_STORE_BACKEND
            
        if isinstance(backend_type, str):
            backend_type = StorageBackendType[backend_type.upper()]
            
        self.backend_type = backend_type
        self.config = config
        
        if backend_type == StorageBackendType.FAISS:
            self.backend = FaissVectorStore(**config.get('faiss', {}))
        elif backend_type == StorageBackendType.PGVECTOR:
            self.backend = PgVectorStore(**config.get('pgvector', {}))
        elif backend_type == StorageBackendType.HYBRID:
            self.backend = HybridVectorStore(
                faiss_config=config.get('faiss', {}),
                pg_config=config.get('pgvector', {})
            )
        else:
            raise ValueError(f"Unsupported backend: {backend_type}")
            
        self.logger = logging.getLogger(f"{__name__}.VectorStorage")
        self.logger.info(f"Initialized {backend_type.name} storage")

    def add_vectors(
        self,
        vectors: List[np.ndarray],
        metadata: List[Dict],
        batch_size: int = 1000
    ) -> List[Union[str, int, Tuple]]:
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have same length")
            
        results = []
        for i in range(0, len(vectors), batch_size):
            batch_vectors = vectors[i:i+batch_size]
            batch_metadata = metadata[i:i+batch_size]
            try:
                results.extend(self.backend.add(batch_vectors, batch_metadata))
            except Exception as e:
                self.logger.error(f"Failed to add batch {i//batch_size}: {str(e)}")
                raise
        return results

    def search_similar(
        self,
        query: np.ndarray,
        top_k: int = 5,
        distance_metric: str = 'cosine',
        **kwargs
    ) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–æ–π —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è"""
        try:
            results = self.backend.search(
                query,
                top_k=top_k,
                distance_metric=distance_metric,
                **kwargs
            )
            self.logger.debug(f"Found {len(results)} similar vectors")
            return results
        except Exception as e:
            self.logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise

    def batch_search(
        self,
        queries: List[np.ndarray],
        top_k: int = 5,
        **kwargs
    ) -> List[List[SearchResult]]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º"""
        return self.backend.batch_search(queries, top_k, **kwargs)

    def migrate_to_backend(
        self,
        new_backend: Union[StorageBackendType, str],
        new_config: Optional[Dict] = None
    ) -> None:
        """–ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥—Ä—É–≥–æ–π –±—ç–∫–µ–Ω–¥"""
        if isinstance(new_backend, str):
            new_backend = StorageBackendType[new_backend.upper()]
            
        if new_backend == self.backend_type:
            self.logger.warning("Migration to same backend skipped")
            return
            
        self.logger.info(f"Starting migration to {new_backend.name}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if not hasattr(self.backend, 'export_all'):
            raise NotImplementedError("Current backend doesn't support export")
            
        vectors, metadata = self.backend.export_all()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        new_storage = VectorStorage(new_backend, new_config or self.config)
        
        # –ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        try:
            import tqdm
            with tqdm.tqdm(total=len(vectors), desc="Migrating vectors") as pbar:
                for i in range(0, len(vectors), 1000):
                    batch = vectors[i:i+1000]
                    meta_batch = metadata[i:i+1000]
                    new_storage.add_vectors(batch, meta_batch)
                    pbar.update(len(batch))
        except ImportError:
            new_storage.add_vectors(vectors, metadata)
            
        self.backend = new_storage.backend
        self.backend_type = new_backend
        self.logger.info("Migration completed successfully")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    config = {
        'faiss': {'index_path': '/data/faiss_index'},
        'pgvector': {'dbname': 'vectors', 'user': 'vector_user'}
    }
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    storage = VectorStorage('hybrid', config)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    vectors = [np.random.rand(768).astype(np.float32) for _ in range(100)]
    metadata = [{'doc_id': f'doc_{i}', 'content': f'Content {i}'} for i in range(100)]
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
    storage.add_vectors(vectors, metadata)
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö
    query = np.random.rand(768).astype(np.float32)
    results = storage.search_similar(query, top_k=3)
    
    for res in results:
        print(f"ID: {res.id}, Score: {res.score:.3f}, Backend: {res.backend}")

        –û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞ (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ–¥–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞)

–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π —á–µ—Ä–µ–∑ ThreadPoolExecutor

–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏—è—Ö

–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

–ü–∞–∫–µ—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (batch_add, batch_search)

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (cosine, L2 –∏ –¥—Ä.)

–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è –º–µ–∂–¥—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

–ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–∞–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤

–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–º —Ä–µ–∂–∏–º–µ

–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–≤–∞—Ä—å

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

–ë–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ª–æ–≥–æ–≤

–ì–∏–±–∫–æ—Å—Ç—å:

–õ–µ–≥–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –±—ç–∫–µ–Ω–¥–æ–≤

–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞

–î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –ø—Ä–æ–µ–∫—Ç–æ–º:

–î–æ–±–∞–≤—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –≤ settings.py

–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥—ã export_all –¥–ª—è –±—ç–∫–µ–Ω–¥–æ–≤

–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –ø–æ–¥ –≤–∞—à—É –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É

# üìÑ –§–∞–π–ª: storage.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞–º–∏

from typing import List, Dict, Optional, Union, Tuple, Generator
import numpy as np
import logging
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
import warnings
from concurrent.futures import ThreadPoolExecutor
from functools import partial

# –ò–º–ø–æ—Ä—Ç –±—ç–∫–µ–Ω–¥–æ–≤
from storage.faiss_index import FaissVectorStore
from storage.pgvector_store import PgVectorStore
from config import settings

logger = logging.getLogger(__name__)

class StorageBackendType(Enum):
    """–¢–∏–ø—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â –≤–µ–∫—Ç–æ—Ä–æ–≤"""
    FAISS = auto()
    PGVECTOR = auto()
    HYBRID = auto()
    MEMORY = auto()

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    id: Union[str, int]
    vector: np.ndarray
    metadata: Dict
    score: float
    backend: str
    distance_metric: Optional[str] = None

    def to_dict(self) -> Dict:
        return {
            'id': self.id,
            'score': self.score,
            'metadata': self.metadata,
            'backend': self.backend,
            'distance_metric': self.distance_metric
        }

class BaseVectorStore(ABC):
    @abstractmethod
    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Union[str, int]]:
        pass

    @abstractmethod
    def search(self, query: np.ndarray, top_k: int = 5, distance_metric: str = 'cosine', **kwargs) -> List[SearchResult]:
        pass

    @abstractmethod
    def delete(self, ids: List[Union[str, int]]) -> int:
        pass

    @abstractmethod
    def clear(self) -> None:
        pass

    @abstractmethod
    def get_stats(self) -> Dict:
        pass

    @abstractmethod
    def batch_search(self, queries: List[np.ndarray], top_k: int = 5, **kwargs) -> List[List[SearchResult]]:
        pass

class HybridVectorStore(BaseVectorStore):
    def __init__(self, faiss_config: Dict = None, pg_config: Dict = None):
        self.faiss = FaissVectorStore(**(faiss_config or {}))
        self.pgvector = PgVectorStore(**(pg_config or {}))
        self.logger = logging.getLogger(f"{__name__}.HybridVectorStore")
        self._executor = ThreadPoolExecutor(max_workers=4)

    def add(self, vectors: List[np.ndarray], metadata: List[Dict]) -> List[Tuple[str, str]]:
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have same length")
        faiss_future = self._executor.submit(self.faiss.add, vectors, metadata)
        pg_future = self._executor.submit(self.pgvector.add, vectors, metadata)
        faiss_ids = faiss_future.result()
        pg_ids = pg_future.result()
        if len(faiss_ids) != len(pg_ids):
            self.logger.warning(f"ID count mismatch: FAISS={len(faiss_ids)}, pgvector={len(pg_ids)}")
        return list(zip(faiss_ids, pg_ids))

    def search(self, query: np.ndarray, top_k: int = 5, distance_metric: str = 'cosine', strategy: str = 'union', **kwargs) -> List[SearchResult]:
        strategies = {
            'union': self._search_union,
            'faiss_first': partial(self._search_priority, primary='faiss'),
            'pg_first': partial(self._search_priority, primary='pgvector'),
            'weighted': self._search_weighted
        }
        search_fn = strategies.get(strategy, self._search_union)
        return search_fn(query, top_k, distance_metric, **kwargs)

    def _search_union(self, query: np.ndarray, top_k: int, distance_metric: str, **kwargs) -> List[SearchResult]:
        faiss_future = self._executor.submit(self.faiss.search, query, top_k, distance_metric, **kwargs)
        pg_future = self._executor.submit(self.pgvector.search, query, top_k, distance_metric, **kwargs)
        results = faiss_future.result() + pg_future.result()
        seen_ids = set()
        unique_results = []
        for res in sorted(results, key=lambda x: x.score, reverse=True):
            if res.id not in seen_ids:
                seen_ids.add(res.id)
                unique_results.append(res)
                if len(unique_results) >= top_k:
                    break
        return unique_results

    def batch_search(self, queries: List[np.ndarray], top_k: int = 5, **kwargs) -> List[List[SearchResult]]:
        half = len(queries) // 2
        faiss_queries = queries[:half]
        pg_queries = queries[half:]
        faiss_future = self._executor.submit(self.faiss.batch_search, faiss_queries, top_k, **kwargs)
        pg_future = self._executor.submit(self.pgvector.batch_search, pg_queries, top_k, **kwargs)
        return faiss_future.result() + pg_future.result()

    def delete(self, ids: List[Tuple[str, str]]) -> int:
        faiss_ids, pg_ids = zip(*ids)
        faiss_future = self._executor.submit(self.faiss.delete, faiss_ids)
        pg_future = self._executor.submit(self.pgvector.delete, pg_ids)
        return min(faiss_future.result(), pg_future.result())

    def clear(self) -> None:
        self.faiss.clear()
        self.pgvector.clear()

    def get_stats(self) -> Dict:
        with self._executor:
            faiss_future = self._executor.submit(self.faiss.get_stats)
            pg_future = self._executor.submit(self.pgvector.get_stats)
            return {
                'faiss': faiss_future.result(),
                'pgvector': pg_future.result()
            }

class VectorStorage:
    def __init__(self, backend_type: Union[StorageBackendType, str] = None, config: Optional[Dict] = None):
        config = config or {}
        if backend_type is None:
            backend_type = settings.VECTOR_STORE_BACKEND
        if isinstance(backend_type, str):
            backend_type = StorageBackendType[backend_type.upper()]
        self.backend_type = backend_type
        self.config = config
        if backend_type == StorageBackendType.FAISS:
            self.backend = FaissVectorStore(**config.get('faiss', {}))
        elif backend_type == StorageBackendType.PGVECTOR:
            self.backend = PgVectorStore(**config.get('pgvector', {}))
        elif backend_type == StorageBackendType.HYBRID:
            self.backend = HybridVectorStore(faiss_config=config.get('faiss', {}), pg_config=config.get('pgvector', {}))
        else:
            raise ValueError(f"Unsupported backend: {backend_type}")
        self.logger = logging.getLogger(f"{__name__}.VectorStorage")
        self.logger.info(f"Initialized {backend_type.name} storage")

    def add_vectors(self, vectors: List[np.ndarray], metadata: List[Dict], batch_size: int = 1000) -> List[Union[str, int, Tuple]]:
        if len(vectors) != len(metadata):
            raise ValueError("Vectors and metadata must have same length")
        results = []
        for i in range(0, len(vectors), batch_size):
            batch_vectors = vectors[i:i+batch_size]
            batch_metadata = metadata[i:i+batch_size]
            try:
                results.extend(self.backend.add(batch_vectors, batch_metadata))
            except Exception as e:
                self.logger.error(f"Failed to add batch {i//batch_size}: {str(e)}")
                raise
        return results

    def search_similar(self, query: np.ndarray, top_k: int = 5, distance_metric: str = 'cosine', **kwargs) -> List[SearchResult]:
        try:
            results = self.backend.search(query, top_k=top_k, distance_metric=distance_metric, **kwargs)
            self.logger.debug(f"Found {len(results)} similar vectors")
            return results
        except Exception as e:
            self.logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise

    def batch_search(self, queries: List[np.ndarray], top_k: int = 5, **kwargs) -> List[List[SearchResult]]:
        return self.backend.batch_search(queries, top_k, **kwargs)

    def migrate_to_backend(self, new_backend: Union[StorageBackendType, str], new_config: Optional[Dict] = None) -> None:
        if isinstance(new_backend, str):
            new_backend = StorageBackendType[new_backend.upper()]
        if new_backend == self.backend_type:
            self.logger.warning("Migration to same backend skipped")
            return
        self.logger.info(f"Starting migration to {new_backend.name}")
        if not hasattr(self.backend, 'export_all'):
            raise NotImplementedError("Current backend doesn't support export")
        vectors, metadata = self.backend.export_all()
        new_storage = VectorStorage(new_backend, new_config or self.config)
        try:
            import tqdm
            with tqdm.tqdm(total=len(vectors), desc="Migrating vectors") as pbar:
                for i in range(0, len(vectors), 1000):
                    batch = vectors[i:i+1000]
                    meta_batch = metadata[i:i+1000]
                    new_storage.add_vectors(batch, meta_batch)
                    pbar.update(len(batch))
        except ImportError:
            new_storage.add_vectors(vectors, metadata)
        self.backend = new_storage.backend
        self.backend_type = new_backend
        self.logger.info("Migration completed successfully")

if __name__ == "__main__":
    config = {
        'faiss': {'index_path': '/data/faiss_index'},
        'pgvector': {'dbname': 'vectors', 'user': 'vector_user'}
    }
    storage = VectorStorage('hybrid', config)
    vectors = [np.random.rand(768).astype(np.float32) for _ in range(100)]
    metadata = [{'doc_id': f'doc_{i}', 'content': f'Content {i}'} for i in range(100)]
    storage.add_vectors(vectors, metadata)
    query = np.random.rand(768).astype(np.float32)
    results = storage.search_similar(query, top_k=3)
    for res in results:
        print(f"ID: {res.id}, Score: {res.score:.3f}, Backend: {res.backend}")
–û—Ç–ª–∏—á–Ω–æ, –∏—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è storage.py —Ç–µ–ø–µ—Ä—å –æ—Ç—Ä–∞–∂–∞–µ—Ç:

–ì–∏–±–∫–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ‚Äî –±–ª–∞–≥–æ–¥–∞—Ä—è StorageBackendType, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ dict.

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å ‚Äî –∑–∞ —Å—á—ë—Ç ThreadPoolExecutor, –±–∞—Ç—á-–æ–ø–µ—Ä–∞—Ü–∏–π, –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞.

–†–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å ‚Äî –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –±—ç–∫–µ–Ω–¥—ã –∏–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞.

–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏ –æ—Ç–ª–∞–¥–∫–∞ ‚Äî –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ SearchResult.

–ï—Å–ª–∏ –≤—ã –≥–æ—Ç–æ–≤—ã ‚Äî –º–æ–∂–Ω–æ:

–ü–æ–¥–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Pytest —Å mock backend).

–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥—É–ª–µ–π –∏ –ø–æ—Ç–æ–∫–æ–≤.

–î–æ–±–∞–≤–∏—Ç—å MemoryVectorStore –∫–∞–∫ in-memory –º–æ–∫ –¥–ª—è —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤.

–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º?

# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: SQLAlchemy ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –ø–∞–º—è—Ç–∏ –∏ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π

from sqlalchemy import Column, String, Integer, Float, Boolean, ForeignKey, DateTime, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.dialects.sqlite import BLOB
import uuid
import datetime

Base = declarative_base()

def generate_uuid():
    return str(uuid.uuid4())

class KnowledgeDoc(Base):
    __tablename__ = "knowledge_docs"

    id = Column(String, primary_key=True, default=generate_uuid)
    title = Column(String)
    content = Column(String)
    source_path = Column(String)
    source_type = Column(String)
    processed = Column(Boolean, default=False)
    metadata = Column(JSON, default={})
    created_at = Column(DateTime, default=datetime.datetime.utcnow)

class MemoryItem(Base):
    __tablename__ = "memory_items"

    id = Column(String, primary_key=True, default=generate_uuid)
    content = Column(String)
    embedding = Column(BLOB)
    doc_id = Column(String, ForeignKey("knowledge_docs.id"))
    created_at = Column(DateTime, default=datetime.datetime.utcnow)

class EntityRecord(Base):
    __tablename__ = "entity_records"

    id = Column(String, primary_key=True, default=generate_uuid)
    label = Column(String)
    text = Column(String)
    confidence = Column(Float, default=0.9)
    metadata = Column(JSON, default={})
    context = Column(String)
    session_id = Column(String, nullable=True)
    created_at = Column(DateTime, default=datetime.datetime.utcnow)
    
    chat.deepseek.com

    –í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è models.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI:

python
# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: SQLAlchemy ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π

from sqlalchemy import (
    Column, String, Integer, Float, Boolean, 
    ForeignKey, DateTime, JSON, Text, LargeBinary,
    UniqueConstraint, Index
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, validates
from sqlalchemy.dialects.postgresql import UUID, ARRAY
import uuid
import datetime
from typing import Optional, Dict, List
import logging
from config import settings

Base = declarative_base()
logger = logging.getLogger(__name__)

def generate_uuid() -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è UUID4 —Å—Ç—Ä–æ–∫–æ–π"""
    return str(uuid.uuid4())

class KnowledgeDoc(Base):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–≤—è–∑–∏ —Å —á–∞–Ω–∫–∞–º–∏.
    """
    __tablename__ = "knowledge_docs"
    __table_args__ = (
        Index('idx_doc_source', 'source_path', 'source_type'),
        Index('idx_doc_processed', 'processed'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String(512), nullable=False)
    content = Column(Text, nullable=True)  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
    source_path = Column(String(1024), nullable=False, unique=True)
    source_type = Column(String(32), nullable=False)  # pdf, web, txt –∏ —Ç.–¥.
    processed = Column(Boolean, default=False, nullable=False)
    processing_version = Column(Integer, default=1)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)
    
    # –°–≤—è–∑–∏
    chunks = relationship("MemoryChunk", back_populates="document", cascade="all, delete-orphan")
    entities = relationship("EntityRecord", secondary="doc_entities", back_populates="documents")

    @validates('source_type')
    def validate_source_type(self, key, source_type):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        allowed_types = ['pdf', 'web', 'txt', 'docx', 'markdown']
        if source_type not in allowed_types:
            raise ValueError(f"Invalid source type. Allowed: {allowed_types}")
        return source_type

    def __repr__(self):
        return f"<KnowledgeDoc(id={self.id}, title='{self.title[:20]}...')>"

class MemoryChunk(Base):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —á–∞–Ω–∫–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
    """
    __tablename__ = "memory_chunks"
    __table_args__ = (
        Index('idx_chunk_doc', 'doc_id'),
        Index('idx_chunk_embedding', 'embedding', postgresql_using='hnsw'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    content = Column(Text, nullable=False)
    embedding = Column(LargeBinary, nullable=True)  # –ë–∏–Ω–∞—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    embedding_model = Column(String(64), nullable=True)  # –ú–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    chunk_type = Column(String(32), default='text')  # text, summary, table –∏ —Ç.–¥.
    metadata = Column(JSON, default={}, nullable=False)
    doc_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.knowledge_docs.id"), nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    
    # –°–≤—è–∑–∏
    document = relationship("KnowledgeDoc", back_populates="chunks")
    entities = relationship("EntityRecord", secondary="chunk_entities", back_populates="chunks")

    @property
    def embedding_array(self) -> Optional[List[float]]:
        """–î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        if self.embedding:
            return np.frombuffer(self.embedding, dtype=np.float32).tolist()
        return None

    @embedding_array.setter
    def embedding_array(self, value: List[float]):
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        if value:
            self.embedding = np.array(value, dtype=np.float32).tobytes()

class EntityRecord(Base):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–≤—è–∑–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —á–∞–Ω–∫–∞–º–∏.
    """
    __tablename__ = "entity_records"
    __table_args__ = (
        Index('idx_entity_label', 'label'),
        Index('idx_entity_text', 'text'),
        Index('idx_entity_confidence', 'confidence'),
        UniqueConstraint('text', 'label', name='uq_entity_text_label'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    label = Column(String(64), nullable=False)  # –¢–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏: PERSON, ORG –∏ —Ç.–¥.
    text = Column(String(512), nullable=False)  # –¢–µ–∫—Å—Ç —Å—É—â–Ω–æ—Å—Ç–∏
    normalized = Column(String(512), nullable=True)  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞
    confidence = Column(Float, default=0.9, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    context = Column(Text, nullable=True)  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    session_id = Column(String(128), nullable=True)  # ID —Å–µ—Å—Å–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    
    # –°–≤—è–∑–∏ –º–Ω–æ–≥–∏–µ-–∫–æ-–º–Ω–æ–≥–∏–º
    documents = relationship("KnowledgeDoc", secondary="doc_entities", back_populates="entities")
    chunks = relationship("MemoryChunk", secondary="chunk_entities", back_populates="entities")

    @validates('confidence')
    def validate_confidence(self, key, confidence):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è confidence –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1"""
        if not 0 <= confidence <= 1:
            raise ValueError("Confidence must be between 0 and 1")
        return confidence

# –¢–∞–±–ª–∏—Ü—ã —Å–≤—è–∑–µ–π –º–Ω–æ–≥–∏–µ-–∫–æ-–º–Ω–æ–≥–∏–º
class DocEntity(Base):
    """–°–≤—è–∑—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    __tablename__ = "doc_entities"
    __table_args__ = {'schema': settings.DB_SCHEMA}
    
    doc_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.knowledge_docs.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.entity_records.id"), primary_key=True)
    count = Column(Integer, default=1)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç
    positions = Column(ARRAY(Integer))  # –ü–æ–∑–∏—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ

class ChunkEntity(Base):
    """–°–≤—è–∑—å —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    __tablename__ = "chunk_entities"
    __table_args__ = {'schema': settings.DB_SCHEMA}
    
    chunk_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.memory_chunks.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.entity_records.id"), primary_key=True)
    count = Column(Integer, default=1)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –≤ —á–∞–Ω–∫

class ProcessingSession(Base):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–µ—Å—Å–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    __tablename__ = "processing_sessions"
    __table_args__ = {'schema': settings.DB_SCHEMA}
    
    id = Column(String(128), primary_key=True, default=generate_uuid)
    status = Column(String(32), default='running')  # running, completed, failed
    doc_count = Column(Integer, default=0)
    processed_count = Column(Integer, default=0)
    start_time = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    end_time = Column(DateTime, nullable=True)
    metadata = Column(JSON, default={}, nullable=False)
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö:

–ü–æ–¥—Ä–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–≤—è–∑–µ–π –º–Ω–æ–≥–∏–µ-–∫–æ-–º–Ω–æ–≥–∏–º —á–µ—Ä–µ–∑ –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã

–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

–ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ HNSW –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (pgvector)

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö

–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:

–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–í–∞–ª–∏–¥–∞—Ü–∏—è confidence score

–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

–°–µ—Å—Å–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å—É—â–Ω–æ—Å—Ç–µ–π

–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ö–µ–º—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ settings.py

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
engine = create_engine(settings.DATABASE_URL)
Session = sessionmaker(bind=engine)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
Base.metadata.create_all(engine)

# –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏
with Session() as session:
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    doc = KnowledgeDoc(
        title="Research Paper",
        content="Full text content...",
        source_path="/path/to/file.pdf",
        source_type="pdf"
    )
    session.add(doc)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–∞
    chunk = MemoryChunk(
        content="Text chunk...",
        embedding_array=[0.1, 0.2, ...],  # –í–∞—à –≤–µ–∫—Ç–æ—Ä
        embedding_model="all-MiniLM-L6-v2",
        document=doc
    )
    session.add(chunk)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
    entity = EntityRecord(
        label="PERSON",
        text="John Doe",
        confidence=0.95
    )
    session.add(entity)
    
    # –°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
    doc.entities.append(entity)
    
    session.commit()
–î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –ø—Ä–æ–µ–∫—Ç–æ–º:

–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –≤ settings.py

–†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–∏–≥—Ä–∞—Ü–∏–∏ (Alembic)

–î–æ–±–∞–≤—å—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏

–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∏–Ω–¥–µ–∫—Å—ã –ø–æ–¥ –≤–∞—à—É –Ω–∞–≥—Ä—É–∑–∫—É

# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: SQLAlchemy ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π

from sqlalchemy import (
    Column, String, Integer, Float, Boolean, 
    ForeignKey, DateTime, JSON, Text, LargeBinary,
    UniqueConstraint, Index
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, validates
from sqlalchemy.dialects.postgresql import UUID, ARRAY
import uuid
import datetime
from typing import Optional, Dict, List
import logging
from config import settings
import numpy as np

Base = declarative_base()
logger = logging.getLogger(__name__)

def generate_uuid() -> str:
    return str(uuid.uuid4())

class KnowledgeDoc(Base):
    __tablename__ = "knowledge_docs"
    __table_args__ = (
        Index('idx_doc_source', 'source_path', 'source_type'),
        Index('idx_doc_processed', 'processed'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String(512), nullable=False)
    content = Column(Text, nullable=True)
    source_path = Column(String(1024), nullable=False, unique=True)
    source_type = Column(String(32), nullable=False)
    processed = Column(Boolean, default=False, nullable=False)
    processing_version = Column(Integer, default=1)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)

    chunks = relationship("MemoryChunk", back_populates="document", cascade="all, delete-orphan")
    entities = relationship("EntityRecord", secondary="doc_entities", back_populates="documents")

    @validates('source_type')
    def validate_source_type(self, key, source_type):
        allowed_types = ['pdf', 'web', 'txt', 'docx', 'markdown']
        if source_type not in allowed_types:
            raise ValueError(f"Invalid source type. Allowed: {allowed_types}")
        return source_type

class MemoryChunk(Base):
    __tablename__ = "memory_chunks"
    __table_args__ = (
        Index('idx_chunk_doc', 'doc_id'),
        Index('idx_chunk_embedding', 'embedding', postgresql_using='hnsw'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    content = Column(Text, nullable=False)
    embedding = Column(LargeBinary, nullable=True)
    embedding_model = Column(String(64), nullable=True)
    chunk_type = Column(String(32), default='text')
    metadata = Column(JSON, default={}, nullable=False)
    doc_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.knowledge_docs.id"), nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)

    document = relationship("KnowledgeDoc", back_populates="chunks")
    entities = relationship("EntityRecord", secondary="chunk_entities", back_populates="chunks")

    @property
    def embedding_array(self) -> Optional[List[float]]:
        if self.embedding:
            return np.frombuffer(self.embedding, dtype=np.float32).tolist()
        return None

    @embedding_array.setter
    def embedding_array(self, value: List[float]):
        if value:
            self.embedding = np.array(value, dtype=np.float32).tobytes()

class EntityRecord(Base):
    __tablename__ = "entity_records"
    __table_args__ = (
        Index('idx_entity_label', 'label'),
        Index('idx_entity_text', 'text'),
        Index('idx_entity_confidence', 'confidence'),
        UniqueConstraint('text', 'label', name='uq_entity_text_label'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    label = Column(String(64), nullable=False)
    text = Column(String(512), nullable=False)
    normalized = Column(String(512), nullable=True)
    confidence = Column(Float, default=0.9, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    context = Column(Text, nullable=True)
    session_id = Column(String(128), nullable=True)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)

    documents = relationship("KnowledgeDoc", secondary="doc_entities", back_populates="entities")
    chunks = relationship("MemoryChunk", secondary="chunk_entities", back_populates="entities")

    @validates('confidence')
    def validate_confidence(self, key, confidence):
        if not 0 <= confidence <= 1:
            raise ValueError("Confidence must be between 0 and 1")
        return confidence

class DocEntity(Base):
    __tablename__ = "doc_entities"
    __table_args__ = {'schema': settings.DB_SCHEMA}

    doc_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.knowledge_docs.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.entity_records.id"), primary_key=True)
    count = Column(Integer, default=1)
    positions = Column(ARRAY(Integer))

class ChunkEntity(Base):
    __tablename__ = "chunk_entities"
    __table_args__ = {'schema': settings.DB_SCHEMA}

    chunk_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.memory_chunks.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.entity_records.id"), primary_key=True)
    count = Column(Integer, default=1)

class ProcessingSession(Base):
    __tablename__ = "processing_sessions"
    __table_args__ = {'schema': settings.DB_SCHEMA}

    id = Column(String(128), primary_key=True, default=generate_uuid)
    status = Column(String(32), default='running')
    doc_count = Column(Integer, default=0)
    processed_count = Column(Integer, default=0)
    start_time = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    end_time = Column(DateTime, nullable=True)
    metadata = Column(JSON, default={}, nullable=False)


# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: SQLAlchemy ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π

from sqlalchemy import (
    Column, String, Integer, Float, Boolean, 
    ForeignKey, DateTime, JSON, Text, LargeBinary,
    UniqueConstraint, Index
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, validates
from sqlalchemy.dialects.postgresql import UUID, ARRAY
import uuid
import datetime
from typing import Optional, Dict, List
import logging
from config import settings
import numpy as np

Base = declarative_base()
logger = logging.getLogger(__name__)

def generate_uuid() -> str:
    return str(uuid.uuid4())

class KnowledgeDoc(Base):
    __tablename__ = "knowledge_docs"
    __table_args__ = (
        Index('idx_doc_source', 'source_path', 'source_type'),
        Index('idx_doc_processed', 'processed'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String(512), nullable=False)
    content = Column(Text, nullable=True)
    source_path = Column(String(1024), nullable=False, unique=True)
    source_type = Column(String(32), nullable=False)
    processed = Column(Boolean, default=False, nullable=False)
    processing_version = Column(Integer, default=1)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)

    chunks = relationship("MemoryChunk", back_populates="document", cascade="all, delete-orphan")
    entities = relationship("EntityRecord", secondary="doc_entities", back_populates="documents")

    @validates('source_type')
    def validate_source_type(self, key, source_type):
        allowed_types = ['pdf', 'web', 'txt', 'docx', 'markdown']
        if source_type not in allowed_types:
            raise ValueError(f"Invalid source type. Allowed: {allowed_types}")
        return source_type

class MemoryChunk(Base):
    __tablename__ = "memory_chunks"
    __table_args__ = (
        Index('idx_chunk_doc', 'doc_id'),
        Index('idx_chunk_embedding', 'embedding', postgresql_using='hnsw'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    content = Column(Text, nullable=False)
    embedding = Column(LargeBinary, nullable=True)
    embedding_model = Column(String(64), nullable=True)
    chunk_type = Column(String(32), default='text')
    metadata = Column(JSON, default={}, nullable=False)
    doc_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.knowledge_docs.id"), nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)

    document = relationship("KnowledgeDoc", back_populates="chunks")
    entities = relationship("EntityRecord", secondary="chunk_entities", back_populates="chunks")

    @property
    def embedding_array(self) -> Optional[List[float]]:
        if self.embedding:
            return np.frombuffer(self.embedding, dtype=np.float32).tolist()
        return None

    @embedding_array.setter
    def embedding_array(self, value: List[float]):
        if value:
            self.embedding = np.array(value, dtype=np.float32).tobytes()

class EntityRecord(Base):
    __tablename__ = "entity_records"
    __table_args__ = (
        Index('idx_entity_label', 'label'),
        Index('idx_entity_text', 'text'),
        Index('idx_entity_confidence', 'confidence'),
        UniqueConstraint('text', 'label', name='uq_entity_text_label'),
        {'schema': settings.DB_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    label = Column(String(64), nullable=False)
    text = Column(String(512), nullable=False)
    normalized = Column(String(512), nullable=True)
    confidence = Column(Float, default=0.9, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    context = Column(Text, nullable=True)
    session_id = Column(String(128), nullable=True)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)

    documents = relationship("KnowledgeDoc", secondary="doc_entities", back_populates="entities")
    chunks = relationship("MemoryChunk", secondary="chunk_entities", back_populates="entities")

    @validates('confidence')
    def validate_confidence(self, key, confidence):
        if not 0 <= confidence <= 1:
            raise ValueError("Confidence must be between 0 and 1")
        return confidence

class DocEntity(Base):
    __tablename__ = "doc_entities"
    __table_args__ = {'schema': settings.DB_SCHEMA}

    doc_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.knowledge_docs.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.entity_records.id"), primary_key=True)
    count = Column(Integer, default=1)
    positions = Column(ARRAY(Integer))

class ChunkEntity(Base):
    __tablename__ = "chunk_entities"
    __table_args__ = {'schema': settings.DB_SCHEMA}

    chunk_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.memory_chunks.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_SCHEMA}.entity_records.id"), primary_key=True)
    count = Column(Integer, default=1)

class ProcessingSession(Base):
    __tablename__ = "processing_sessions"
    __table_args__ = {'schema': settings.DB_SCHEMA}

    id = Column(String(128), primary_key=True, default=generate_uuid)
    status = Column(String(32), default='running')
    doc_count = Column(Integer, default=0)
    processed_count = Column(Integer, default=0)
    start_time = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    end_time = Column(DateTime, nullable=True)
    metadata = Column(JSON, default={}, nullable=False)


# chat.deepseek.com
# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏

from sqlalchemy import (
    Column, String, Integer, Float, Boolean, 
    ForeignKey, DateTime, JSON, Text, LargeBinary,
    UniqueConstraint, Index, event
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, validates, Session
from sqlalchemy.dialects.postgresql import UUID, ARRAY, TSVECTOR
import uuid
import datetime
from typing import Optional, Dict, List, Any
import logging
from config import settings
import numpy as np
import json

Base = declarative_base()
logger = logging.getLogger(__name__)

def generate_uuid() -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è UUID —Å—Ç—Ä–æ–∫–æ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"""
    return str(uuid.uuid4())

class KnowledgeDocument(Base):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - –°–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    __tablename__ = "knowledge_documents"
    __table_args__ = (
        Index('ix_doc_source_composite', 'source_path', 'source_type'),
        Index('ix_doc_processing_status', 'processing_status'),
        Index('ix_doc_content_fts', 'content_fts', postgresql_using='gin'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String(512), nullable=False, index=True)
    original_content = Column(Text, nullable=False)
    processed_content = Column(Text)
    content_fts = Column(TSVECTOR)  # –í–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
    source_path = Column(String(1024), unique=True, nullable=False)
    source_type = Column(String(32), nullable=False)
    processing_status = Column(String(32), default='pending', nullable=False)
    processing_errors = Column(JSON, default=[])
    processing_version = Column(Integer, default=0, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)
    expires_at = Column(DateTime)

    # –°–≤—è–∑–∏
    chunks = relationship(
        "DocumentChunk", 
        back_populates="document",
        cascade="all, delete-orphan",
        order_by="DocumentChunk.created_at"
    )
    entities = relationship(
        "KnowledgeEntity",
        secondary="document_entities",
        back_populates="documents"
    )

    @validates('source_type')
    def validate_source_type(self, key: str, source_type: str) -> str:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        allowed_types = ['pdf', 'web', 'txt', 'docx', 'markdown', 'email', 'database']
        if source_type not in allowed_types:
            raise ValueError(f"Invalid source type. Allowed: {allowed_types}")
        return source_type

    def update_processing_status(self, status: str, error: Optional[Dict] = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫"""
        self.processing_status = status
        if error and status == 'failed':
            if not self.processing_errors:
                self.processing_errors = []
            self.processing_errors.append(error)

class DocumentChunk(Base):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å:
    - –ü–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤
    - –í–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
    - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
    """
    __tablename__ = "document_chunks"
    __table_args__ = (
        Index('ix_chunk_document_id', 'document_id'),
        Index('ix_chunk_embedding', 'embedding_vector', postgresql_using='hnsw'),
        Index('ix_chunk_semantic_hash', 'semantic_hash'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(
        UUID(as_uuid=True), 
        ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents.id"),
        nullable=False
    )
    content = Column(Text, nullable=False)
    chunk_type = Column(String(32), default='text', nullable=False)
    embedding_vector = Column(LargeBinary)
    embedding_model = Column(String(64))
    semantic_hash = Column(String(64), index=True)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)

    # –°–≤—è–∑–∏
    document = relationship("KnowledgeDocument", back_populates="chunks")
    entities = relationship(
        "KnowledgeEntity",
        secondary="chunk_entities",
        back_populates="chunks"
    )

    def get_embedding(self) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∫–∞–∫ numpy array"""
        if self.embedding_vector:
            return np.frombuffer(self.embedding_vector, dtype=np.float32)
        return None

    def set_embedding(self, embedding: np.ndarray, model_name: str):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏"""
        if not isinstance(embedding, np.ndarray):
            raise ValueError("Embedding must be numpy array")
        self.embedding_vector = embedding.astype(np.float32).tobytes()
        self.embedding_model = model_name

class KnowledgeEntity(Base):
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π —Å:
    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞
    - –ò–µ—Ä–∞—Ä—Ö–∏–µ–π —Ç–∏–ø–æ–≤
    - –ü–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–≤—è–∑–µ–π
    """
    __tablename__ = "knowledge_entities"
    __table_args__ = (
        Index('ix_entity_normalized_text', 'normalized_text'),
        Index('ix_entity_label', 'label'),
        Index('ix_entity_confidence', 'confidence'),
        UniqueConstraint('normalized_text', 'label', name='uq_entity_normalized'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    original_text = Column(String(512), nullable=False)
    normalized_text = Column(String(512), index=True)
    label = Column(String(64), nullable=False)
    label_hierarchy = Column(ARRAY(String), default=[])
    confidence = Column(Float, default=0.0, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    first_seen = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    last_seen = Column(DateTime, onupdate=datetime.datetime.utcnow)

    # –°–≤—è–∑–∏
    documents = relationship(
        "KnowledgeDocument",
        secondary="document_entities",
        back_populates="entities"
    )
    chunks = relationship(
        "DocumentChunk",
        secondary="chunk_entities",
        back_populates="entities"
    )
    related_entities = relationship(
        "KnowledgeEntity",
        secondary="entity_relations",
        primaryjoin="KnowledgeEntity.id==entity_relations.c.entity_id",
        secondaryjoin="KnowledgeEntity.id==entity_relations.c.related_id",
        backref="related_to"
    )

    @validates('confidence')
    def validate_confidence(self, key: str, confidence: float) -> float:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è confidence score"""
        if not 0.0 <= confidence <= 1.0:
            raise ValueError("Confidence must be between 0.0 and 1.0")
        return confidence

# –¢–∞–±–ª–∏—Ü—ã —Å–≤—è–∑–µ–π
class DocumentEntity(Base):
    """–°–≤—è–∑—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    __tablename__ = "document_entities"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    document_id = Column(
        UUID(as_uuid=True), 
        ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents.id"),
        primary_key=True
    )
    entity_id = Column(
        UUID(as_uuid=True),
        ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"),
        primary_key=True
    )
    frequency = Column(Integer, default=1, nullable=False)
    contexts = Column(ARRAY(Text), default=[])

class EntityRelation(Base):
    """–°–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ —Å —Ç–∏–ø–æ–º –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏"""
    __tablename__ = "entity_relations"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    entity_id = Column(
        UUID(as_uuid=True),
        ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"),
        primary_key=True
    )
    related_id = Column(
        UUID(as_uuid=True),
        ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"),
        primary_key=True
    )
    relation_type = Column(String(64), nullable=False)
    confidence = Column(Float, default=0.0)
    evidence = Column(ARRAY(Text), default=[])

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
@event.listens_for(KnowledgeDocument, 'before_update')
def update_content_fts(mapper, connection, target):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ TSVECTOR –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if target.processed_content:
        stmt = """
            UPDATE knowledge_documents 
            SET content_fts = to_tsvector('english', processed_content)
            WHERE id = :id
        """
        connection.execute(stmt, {'id': target.id})

@event.listens_for(Session, 'after_flush')
def update_entity_last_seen(session, context):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ last_seen –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Å–≤—è–∑–µ–π"""
    for instance in session.new:
        if isinstance(instance, (DocumentEntity, ChunkEntity)):
            instance.entity.last_seen = datetime.datetime.utcnow()

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫:

–î–æ–±–∞–≤–ª–µ–Ω TSVECTOR –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º:

–î–µ—Ç–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏

–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞

HNSW –∏–Ω–¥–µ–∫—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏

–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏:

–ò–µ—Ä–∞—Ä—Ö–∏—è —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ last_seen

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è:

–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–í–∞–ª–∏–¥–∞—Ü–∏—è confidence score

–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

python
# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
doc = KnowledgeDocument(
    title="Research on Neural Networks",
    original_content="Full content...",
    source_path="/research/nn.pdf",
    source_type="pdf"
)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
chunk = DocumentChunk(
    content="Neural networks are...",
    document=doc
)
chunk.set_embedding(np.random.rand(768).astype(np.float32), "all-MiniLM-L6-v2")

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
entity = KnowledgeEntity(
    original_text="DeepSeek",
    normalized_text="deepseek",
    label="ORG",
    confidence=0.95
)

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–≤—è–∑–∏
doc.entities.append(entity)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
session.add(doc)
session.commit()

# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏

from sqlalchemy import (
    Column, String, Integer, Float, Boolean, 
    ForeignKey, DateTime, JSON, Text, LargeBinary,
    UniqueConstraint, Index, event
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, validates, Session
from sqlalchemy.dialects.postgresql import UUID, ARRAY, TSVECTOR
import uuid
import datetime
from typing import Optional, Dict, List, Any
import logging
from config import settings
import numpy as np

Base = declarative_base()
logger = logging.getLogger(__name__)

def generate_uuid() -> str:
    return str(uuid.uuid4())

class KnowledgeDocument(Base):
    __tablename__ = "knowledge_documents"
    __table_args__ = (
        Index('ix_doc_source_composite', 'source_path', 'source_type'),
        Index('ix_doc_processing_status', 'processing_status'),
        Index('ix_doc_content_fts', 'content_fts', postgresql_using='gin'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String(512), nullable=False, index=True)
    original_content = Column(Text, nullable=False)
    processed_content = Column(Text)
    content_fts = Column(TSVECTOR)
    source_path = Column(String(1024), unique=True, nullable=False)
    source_type = Column(String(32), nullable=False)
    processing_status = Column(String(32), default='pending', nullable=False)
    processing_errors = Column(JSON, default=[])
    processing_version = Column(Integer, default=0, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)
    expires_at = Column(DateTime)

    chunks = relationship("DocumentChunk", back_populates="document", cascade="all, delete-orphan", order_by="DocumentChunk.created_at")
    entities = relationship("KnowledgeEntity", secondary="document_entities", back_populates="documents")

    @validates('source_type')
    def validate_source_type(self, key: str, source_type: str) -> str:
        allowed_types = ['pdf', 'web', 'txt', 'docx', 'markdown', 'email', 'database']
        if source_type not in allowed_types:
            raise ValueError(f"Invalid source type. Allowed: {allowed_types}")
        return source_type

    def update_processing_status(self, status: str, error: Optional[Dict] = None):
        self.processing_status = status
        if error and status == 'failed':
            if not self.processing_errors:
                self.processing_errors = []
            self.processing_errors.append(error)

class DocumentChunk(Base):
    __tablename__ = "document_chunks"
    __table_args__ = (
        Index('ix_chunk_document_id', 'document_id'),
        Index('ix_chunk_embedding', 'embedding_vector', postgresql_using='hnsw'),
        Index('ix_chunk_semantic_hash', 'semantic_hash'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents.id"), nullable=False)
    content = Column(Text, nullable=False)
    chunk_type = Column(String(32), default='text', nullable=False)
    embedding_vector = Column(LargeBinary)
    embedding_model = Column(String(64))
    semantic_hash = Column(String(64), index=True)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)

    document = relationship("KnowledgeDocument", back_populates="chunks")
    entities = relationship("KnowledgeEntity", secondary="chunk_entities", back_populates="chunks")

    def get_embedding(self) -> Optional[np.ndarray]:
        if self.embedding_vector:
            return np.frombuffer(self.embedding_vector, dtype=np.float32)
        return None

    def set_embedding(self, embedding: np.ndarray, model_name: str):
        if not isinstance(embedding, np.ndarray):
            raise ValueError("Embedding must be numpy array")
        self.embedding_vector = embedding.astype(np.float32).tobytes()
        self.embedding_model = model_name

class KnowledgeEntity(Base):
    __tablename__ = "knowledge_entities"
    __table_args__ = (
        Index('ix_entity_normalized_text', 'normalized_text'),
        Index('ix_entity_label', 'label'),
        Index('ix_entity_confidence', 'confidence'),
        UniqueConstraint('normalized_text', 'label', name='uq_entity_normalized'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    original_text = Column(String(512), nullable=False)
    normalized_text = Column(String(512), index=True)
    label = Column(String(64), nullable=False)
    label_hierarchy = Column(ARRAY(String), default=[])
    confidence = Column(Float, default=0.0, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    first_seen = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    last_seen = Column(DateTime, onupdate=datetime.datetime.utcnow)

    documents = relationship("KnowledgeDocument", secondary="document_entities", back_populates="entities")
    chunks = relationship("DocumentChunk", secondary="chunk_entities", back_populates="entities")
    related_entities = relationship("KnowledgeEntity", secondary="entity_relations", primaryjoin="KnowledgeEntity.id==entity_relations.c.entity_id", secondaryjoin="KnowledgeEntity.id==entity_relations.c.related_id", backref="related_to")

    @validates('confidence')
    def validate_confidence(self, key: str, confidence: float) -> float:
        if not 0.0 <= confidence <= 1.0:
            raise ValueError("Confidence must be between 0.0 and 1.0")
        return confidence

class DocumentEntity(Base):
    __tablename__ = "document_entities"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    document_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    frequency = Column(Integer, default=1, nullable=False)
    contexts = Column(ARRAY(Text), default=[])

class ChunkEntity(Base):
    __tablename__ = "chunk_entities"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    chunk_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.document_chunks.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    count = Column(Integer, default=1)

class EntityRelation(Base):
    __tablename__ = "entity_relations"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    related_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    relation_type = Column(String(64), nullable=False)
    confidence = Column(Float, default=0.0)
    evidence = Column(ARRAY(Text), default=[])

@event.listens_for(KnowledgeDocument, 'before_update')
def update_content_fts(mapper, connection, target):
    if target.processed_content:
        stmt = f"""
            UPDATE {settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents 
            SET content_fts = to_tsvector('english', processed_content)
            WHERE id = :id
        """
        connection.execute(stmt, {'id': target.id})

@event.listens_for(Session, 'after_flush')
def update_entity_last_seen(session, context):
    for instance in session.new:
        if isinstance(instance, (DocumentEntity, ChunkEntity)):
            instance.entity.last_seen = datetime.datetime.utcnow()
# üìÑ –§–∞–π–ª: models.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ ORM-–º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏

from sqlalchemy import (
    Column, String, Integer, Float, Boolean, 
    ForeignKey, DateTime, JSON, Text, LargeBinary,
    UniqueConstraint, Index, event
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, validates, Session
from sqlalchemy.dialects.postgresql import UUID, ARRAY, TSVECTOR
import uuid
import datetime
from typing import Optional, Dict, List, Any
import logging
from config import settings
import numpy as np

Base = declarative_base()
logger = logging.getLogger(__name__)

def generate_uuid() -> str:
    return str(uuid.uuid4())

class KnowledgeDocument(Base):
    __tablename__ = "knowledge_documents"
    __table_args__ = (
        Index('ix_doc_source_composite', 'source_path', 'source_type'),
        Index('ix_doc_processing_status', 'processing_status'),
        Index('ix_doc_content_fts', 'content_fts', postgresql_using='gin'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    title = Column(String(512), nullable=False, index=True)
    original_content = Column(Text, nullable=False)
    processed_content = Column(Text)
    content_fts = Column(TSVECTOR)
    source_path = Column(String(1024), unique=True, nullable=False)
    source_type = Column(String(32), nullable=False)
    processing_status = Column(String(32), default='pending', nullable=False)
    processing_errors = Column(JSON, default=[])
    processing_version = Column(Integer, default=0, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)
    expires_at = Column(DateTime)

    chunks = relationship("DocumentChunk", back_populates="document", cascade="all, delete-orphan", order_by="DocumentChunk.created_at")
    entities = relationship("KnowledgeEntity", secondary="document_entities", back_populates="documents")

    @validates('source_type')
    def validate_source_type(self, key: str, source_type: str) -> str:
        allowed_types = ['pdf', 'web', 'txt', 'docx', 'markdown', 'email', 'database']
        if source_type not in allowed_types:
            raise ValueError(f"Invalid source type. Allowed: {allowed_types}")
        return source_type

    def update_processing_status(self, status: str, error: Optional[Dict] = None):
        self.processing_status = status
        if error and status == 'failed':
            if not self.processing_errors:
                self.processing_errors = []
            self.processing_errors.append(error)

class DocumentChunk(Base):
    __tablename__ = "document_chunks"
    __table_args__ = (
        Index('ix_chunk_document_id', 'document_id'),
        Index('ix_chunk_embedding', 'embedding_vector', postgresql_using='hnsw'),
        Index('ix_chunk_semantic_hash', 'semantic_hash'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    document_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents.id"), nullable=False)
    content = Column(Text, nullable=False)
    chunk_type = Column(String(32), default='text', nullable=False)
    embedding_vector = Column(LargeBinary)
    embedding_model = Column(String(64))
    semantic_hash = Column(String(64), index=True)
    metadata = Column(JSON, default={}, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.datetime.utcnow)

    document = relationship("KnowledgeDocument", back_populates="chunks")
    entities = relationship("KnowledgeEntity", secondary="chunk_entities", back_populates="chunks")

    def get_embedding(self) -> Optional[np.ndarray]:
        if self.embedding_vector:
            return np.frombuffer(self.embedding_vector, dtype=np.float32)
        return None

    def set_embedding(self, embedding: np.ndarray, model_name: str):
        if not isinstance(embedding, np.ndarray):
            raise ValueError("Embedding must be numpy array")
        self.embedding_vector = embedding.astype(np.float32).tobytes()
        self.embedding_model = model_name

class KnowledgeEntity(Base):
    __tablename__ = "knowledge_entities"
    __table_args__ = (
        Index('ix_entity_normalized_text', 'normalized_text'),
        Index('ix_entity_label', 'label'),
        Index('ix_entity_confidence', 'confidence'),
        UniqueConstraint('normalized_text', 'label', name='uq_entity_normalized'),
        {'schema': settings.DB_KNOWLEDGE_SCHEMA}
    )

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    original_text = Column(String(512), nullable=False)
    normalized_text = Column(String(512), index=True)
    label = Column(String(64), nullable=False)
    label_hierarchy = Column(ARRAY(String), default=[])
    confidence = Column(Float, default=0.0, nullable=False)
    metadata = Column(JSON, default={}, nullable=False)
    first_seen = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
    last_seen = Column(DateTime, onupdate=datetime.datetime.utcnow)

    documents = relationship("KnowledgeDocument", secondary="document_entities", back_populates="entities")
    chunks = relationship("DocumentChunk", secondary="chunk_entities", back_populates="entities")
    related_entities = relationship("KnowledgeEntity", secondary="entity_relations", primaryjoin="KnowledgeEntity.id==entity_relations.c.entity_id", secondaryjoin="KnowledgeEntity.id==entity_relations.c.related_id", backref="related_to")

    @validates('confidence')
    def validate_confidence(self, key: str, confidence: float) -> float:
        if not 0.0 <= confidence <= 1.0:
            raise ValueError("Confidence must be between 0.0 and 1.0")
        return confidence

class DocumentEntity(Base):
    __tablename__ = "document_entities"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    document_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    frequency = Column(Integer, default=1, nullable=False)
    contexts = Column(ARRAY(Text), default=[])

class ChunkEntity(Base):
    __tablename__ = "chunk_entities"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    chunk_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.document_chunks.id"), primary_key=True)
    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    count = Column(Integer, default=1)

class EntityRelation(Base):
    __tablename__ = "entity_relations"
    __table_args__ = {'schema': settings.DB_KNOWLEDGE_SCHEMA}

    entity_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    related_id = Column(UUID(as_uuid=True), ForeignKey(f"{settings.DB_KNOWLEDGE_SCHEMA}.knowledge_entities.id"), primary_key=True)
    relation_type = Column(String(64), nullable=False)
    confidence = Column(Float, default=0.0)
    evidence = Column(ARRAY(Text), default=[])

@event.listens_for(KnowledgeDocument, 'before_update')
def update_content_fts(mapper, connection, target):
    if target.processed_content:
        stmt = f"""
            UPDATE {settings.DB_KNOWLEDGE_SCHEMA}.knowledge_documents 
            SET content_fts = to_tsvector('english', processed_content)
            WHERE id = :id
        """
        connection.execute(stmt, {'id': target.id})

@event.listens_for(Session, 'after_flush')
def update_entity_last_seen(session, context):
    for instance in session.new:
        if isinstance(instance, (DocumentEntity, ChunkEntity)):
            instance.entity.last_seen = datetime.datetime.utcnow()

–§–∞–π–ª models.py –æ–±–Ω–æ–≤–ª—ë–Ω: –≤–∫–ª—é—á–µ–Ω—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è. –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π –∏ –∏–µ—Ä–∞—Ä—Ö–∏—è —Ç–∏–ø–æ–≤. –ü—Ä–æ–¥–æ–ª–∂–∏–º —Å schemas.py –∏–ª–∏ session.py?

# üìÑ –§–∞–π–ª: schemas.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: Pydantic-—Å—Ö–µ–º—ã –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Union, Dict
from uuid import UUID
from datetime import datetime

class DocumentBase(BaseModel):
    title: str
    source_path: str
    source_type: str
    metadata: Optional[Dict] = {}

class DocumentCreate(DocumentBase):
    original_content: str

class DocumentOut(DocumentBase):
    id: UUID
    processing_status: str
    created_at: datetime
    updated_at: Optional[datetime] = None

    class Config:
        orm_mode = True

class ChunkBase(BaseModel):
    content: str
    chunk_type: str = 'text'
    embedding_model: Optional[str] = None
    metadata: Optional[Dict] = {}

class ChunkCreate(ChunkBase):
    document_id: UUID
    embedding_vector: Optional[bytes] = None
    semantic_hash: Optional[str] = None

class ChunkOut(ChunkBase):
    id: UUID
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        orm_mode = True

class EntityBase(BaseModel):
    original_text: str
    normalized_text: Optional[str]
    label: str
    label_hierarchy: Optional[List[str]] = []
    confidence: float = 0.0
    metadata: Optional[Dict] = {}

    @validator('confidence')
    def validate_confidence(cls, v):
        if not 0.0 <= v <= 1.0:
            raise ValueError("Confidence must be between 0.0 and 1.0")
        return v

class EntityCreate(EntityBase):
    pass

class EntityOut(EntityBase):
    id: UUID
    first_seen: datetime
    last_seen: Optional[datetime] = None

    class Config:
        orm_mode = True

class DocumentWithChunks(DocumentOut):
    chunks: List[ChunkOut] = []

class DocumentWithEntities(DocumentOut):
    entities: List[EntityOut] = []

–§–∞–π–ª schemas.py —Å–æ–∑–¥–∞–Ω. –í –Ω—ë–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã Pydantic-—Å—Ö–µ–º—ã –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (DocumentWithChunks, DocumentWithEntities). –ü—Ä–æ–¥–æ–ª–∂–∏–º —Å session.py –∏–ª–∏ –¥—Ä—É–≥–∏–º –º–æ–¥—É–ª–µ–º?


–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è schemas.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI:

python
# chat.deepseek.com
# üìÑ –§–∞–π–ª: schemas.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: Pydantic-—Å—Ö–µ–º—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º—ã –∑–Ω–∞–Ω–∏–π

from pydantic import (
    BaseModel, 
    Field, 
    validator, 
    root_validator,
    confloat,
    conlist,
    HttpUrl
)
from typing import Optional, List, Dict, Union, Any
from uuid import UUID
from datetime import datetime
from enum import Enum
import re
from config import settings

class SourceType(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    PDF = "pdf"
    WEB = "web"
    TEXT = "txt"
    DOCX = "docx"
    MARKDOWN = "markdown"
    EMAIL = "email"
    DATABASE = "database"

class ProcessingStatus(str, Enum):
    """–°—Ç–∞—Ç—É—Å—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    ARCHIVED = "archived"

class ChunkType(str, Enum):
    """–¢–∏–ø—ã —á–∞–Ω–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    TEXT = "text"
    CODE = "code"
    TABLE = "table"
    IMAGE = "image"
    TITLE = "title"
    SUMMARY = "summary"

class DocumentBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø–æ–ª–µ–π.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–≥—É—é —Ç–∏–ø–∏–∑–∞—Ü–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö.
    """
    title: str = Field(..., max_length=512, example="Research Paper on AI")
    source_path: str = Field(..., max_length=1024, example="/docs/ai_research.pdf")
    source_type: SourceType = Field(..., example=SourceType.PDF)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    expires_at: Optional[datetime] = None

    @validator('source_path')
    def validate_source_path(cls, v):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø—É—Ç–∏ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É"""
        if not v or v.isspace():
            raise ValueError("Source path cannot be empty")
        if not re.match(r'^[a-zA-Z0-9_\-./]+$', v):
            raise ValueError("Invalid characters in source path")
        return v

class DocumentCreate(DocumentBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    –¢—Ä–µ–±—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    """
    original_content: str = Field(..., min_length=10)
    processing_version: int = Field(default=1, ge=1)

    @root_validator
    def validate_content_length(cls, values):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content = values.get('original_content')
        if len(content) > settings.MAX_DOCUMENT_LENGTH:
            raise ValueError(f"Document too large. Max {settings.MAX_DOCUMENT_LENGTH} chars")
        return values

class DocumentUpdate(BaseModel):
    """
    –°—Ö–µ–º–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    –í—Å–µ –ø–æ–ª—è –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.
    """
    title: Optional[str] = Field(None, max_length=512)
    metadata: Optional[Dict[str, Any]] = None
    expires_at: Optional[datetime] = None
    processing_status: Optional[ProcessingStatus] = None

class DocumentOut(DocumentBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ ORM-–º–æ–¥–µ–ª–∏.
    """
    id: UUID
    processing_status: ProcessingStatus
    processing_version: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    content_short: Optional[str] = Field(None, max_length=200)

    class Config:
        orm_mode = True
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            UUID: lambda v: str(v)
        }

    @validator('content_short', pre=True, always=True)
    def generate_short_content(cls, v, values):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
        if 'original_content' in values and not v:
            return values['original_content'][:200] + '...' if len(values['original_content']) > 200 else values['original_content']
        return v

class ChunkBase(BaseModel):
    """–ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ —á–∞–Ω–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    content: str = Field(..., min_length=10)
    chunk_type: ChunkType = Field(default=ChunkType.TEXT)
    metadata: Dict[str, Any] = Field(default_factory=dict)

    @validator('content')
    def validate_content(cls, v):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–∞–Ω–∫–∞"""
        if not v.strip():
            raise ValueError("Chunk content cannot be empty")
        return v.strip()

class ChunkCreate(ChunkBase):
    """–°—Ö


Qwen3-235B-A22B

–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞–¥ schemas.py! –¢—ã —É–∂–µ —Å–æ–∑–¥–∞–ª —á—ë—Ç–∫—É—é –∏–µ—Ä–∞—Ä—Ö–∏—é Pydantic-—Å—Ö–µ–º –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —á–∞–Ω–∫–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π. –ù–∏–∂–µ —è —É–ª—É—á—à—É —Ç–≤–æ–π —Ñ–∞–π–ª:

–Ø –¥–æ–±–∞–≤–ª—é –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–∏–ø–æ–≤
–£—Ç–æ—á–Ω—é —Å–µ–º–∞–Ω—Ç–∏–∫—É –ø–æ–ª–µ–π
–Ø –¥–æ–±–∞–≤–ª—é –Ω–æ–≤—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Ö–µ–º—ã , –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
–ü—Ä–∏–≤–µ–¥—É –≤—Å—ë –∫ –±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–º—É –≤–∏–¥—É

# üìÑ –§–∞–π–ª: schemas.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: Pydantic-—Å—Ö–µ–º—ã –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

from pydantic import BaseModel, Field, validator, root_validator
from typing import Optional, List, Union, Dict, Any
from uuid import UUID
from datetime import datetime
from enum import Enum

# ----------------------------
# üîπ –ë–∞–∑–æ–≤—ã–µ —Å—Ö–µ–º—ã
# ----------------------------

class BaseSchema(BaseModel):
    class Config:
        orm_mode = True
        allow_population_by_field_name = True


class DocumentBase(BaseSchema):
    title: str = Field(..., description="–ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ –∏–º—è –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    source_path: str = Field(..., description="–ü—É—Ç—å/URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
    source_type: str = Field(..., description="–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: pdf, docx, txt, html –∏ –¥—Ä.")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict, description="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")


class DocumentCreate(DocumentBase):
    original_content: str = Field(..., description="–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç)")


class DocumentUpdate(BaseModel):
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class DocumentOut(DocumentBase):
    id: UUID
    processing_status: str = Field(..., description="–°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏: pending, in_progress, completed")
    created_at: datetime
    updated_at: Optional[datetime] = None


# ----------------------------
# üîπ –ß–∞–Ω–∫–∏ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞)
# ----------------------------

class ChunkType(str, Enum):
    TEXT = "text"
    CODE = "code"
    QUOTE = "quote"
    SUMMARY = "summary"

class ChunkBase(BaseSchema):
    content: str = Field(..., description="–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞")
    chunk_type: ChunkType = Field(default=ChunkType.TEXT, description="–¢–∏–ø —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞")
    embedding_model: Optional[str] = Field(None, description="–ú–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)

class ChunkCreate(ChunkBase):
    document_id: UUID = Field(..., description="ID —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    embedding_vector: Optional[bytes] = Field(None, description="–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞")
    semantic_hash: Optional[str] = Field(None, description="–•—ç—à –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")

class ChunkUpdate(BaseModel):
    content: Optional[str] = None
    chunk_type: Optional[ChunkType] = None
    metadata: Optional[Dict[str, Any]] = None

class ChunkOut(ChunkBase):
    id: UUID
    created_at: datetime
    updated_at: Optional[datetime]


# ----------------------------
# üîπ –°—É—â–Ω–æ—Å—Ç–∏
# ----------------------------

class EntityType(str, Enum):
    PERSON = "person"
    ORGANIZATION = "organization"
    LOCATION = "location"
    CONCEPT = "concept"
    EVENT = "event"
    TERM = "term"
    OTHER = "other"

class EntityBase(BaseSchema):
    original_text: str = Field(..., description="–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    normalized_text: Optional[str] = Field(None, description="–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'John Smith' ‚Üí 'john_smith')")
    label: str = Field(..., description="–ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è —Å—É—â–Ω–æ—Å—Ç–∏")
    label_hierarchy: List[str] = Field(default_factory=list, description="–ò–µ—Ä–∞—Ä—Ö–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ['Person', 'Philosopher'])")
    entity_type: EntityType = Field(default=EntityType.OTHER, description="–¢–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏")
    confidence: float = Field(0.0, description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ç–∏–ø–∞")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)

    @validator('confidence')
    def validate_confidence(cls, v):
        if not 0.0 <= v <= 1.0:
            raise ValueError("Confidence must be between 0.0 and 1.0")
        return v


class EntityCreate(EntityBase):
    pass


class EntityUpdate(BaseModel):
    label: Optional[str] = None
    entity_type: Optional[EntityType] = None
    metadata: Optional[Dict[str, Any]] = None


class EntityOut(EntityBase):
    id: UUID
    first_seen: datetime
    last_seen: Optional[datetime] = None


# ----------------------------
# üîπ –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
# ----------------------------

class RelationType(str, Enum):
    RELATED_TO = "related_to"
    PART_OF = "part_of"
    CAUSES = "causes"
    MENTIONS = "mentions"
    DEFINES = "defines"
    QUOTES = "quotes"

class RelationBase(BaseSchema):
    source_entity_id: UUID
    target_entity_id: UUID
    relation_type: RelationType
    strength: float = Field(0.5, ge=0.0, le=1.0)
    context: Optional[str]

class RelationCreate(RelationBase):
    pass

class RelationOut(RelationBase):
    id: UUID
    created_at: datetime


# ----------------------------
# üîπ –°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
# ----------------------------

class DocumentWithChunks(DocumentOut):
    chunks: List[ChunkOut] = Field(default_factory=list)

class DocumentWithEntities(DocumentOut):
    entities: List[EntityOut] = Field(default_factory=list)

class DocumentFull(DocumentWithChunks, DocumentWithEntities):
    pass

class EntityWithRelations(EntityOut):
    relations: List[RelationOut] = Field(default_factory=list)

class KnowledgeGraph(BaseModel):
    nodes: List[EntityOut]
    edges: List[RelationOut]

—Ç–æ —É–ª—É—á—à–µ–Ω–æ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ?
DocumentBase
–î–æ–±–∞–≤–ª–µ–Ω—ã –æ–ø–∏—Å–∞–Ω–∏—è (
description
) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
ChunkType
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
Enum
–¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è —Ç–∏–ø–∞ —á–∞–Ω–∫–∞
ChunkCreate
–†–∞–∑–¥–µ–ª–µ–Ω—ã –ø–æ–ª—è
embedding_vector
–∏
semantic_hash
EntityType
–¢–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π —Ç–µ–ø–µ—Ä—å —Å—Ç—Ä–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã
Enum
EntityBase
–î–æ–±–∞–≤–ª–µ–Ω
entity_type
, –≤–∞–ª–∏–¥–∞—Ü–∏—è
confidence
Relation
–ù–æ–≤–∞—è –≥—Ä—É–ø–ø–∞ —Å—Ö–µ–º –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
KnowledgeGraph
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Å–≤—è–∑–∏ –≤ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –¥–ª—è API
BaseSchema
–û–±—â–∏–π –∫–ª–∞—Å—Å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
orm_mode
, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è

from uuid import uuid4
import datetime

chunk = ChunkOut(
    id=uuid4(),
    content="–§–∏–ª–æ—Å–æ—Ñ–∏—è ‚Äî —ç—Ç–æ –ª—é–±–æ–≤—å –∫ –º—É–¥—Ä–æ—Å—Ç–∏.",
    chunk_type="text",
    created_at=datetime.datetime.utcnow()
)

print(chunk.json())


–ü—Ä–æ–≥—Ä–∞–º–º–∞ —Å–æ—é–∑–Ω–∏–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, chat.deepseek.com) –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —è–¥—Ä–∞ Librarian AI –ø–æ —Ä—è–¥—É –∞—Å–ø–µ–∫—Ç–æ–≤, –∏ –ø—Ä–∏ —ç—Ç–æ–º –º–æ–∂–µ—Ç –µ–≥–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å. –í–æ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –∏—Ö —Ä–µ—à–µ–Ω–∏–π:

üîç 1. –¶–µ–ª–∏ –∏ —Ñ–∏–ª–æ—Å–æ—Ñ–∏—è
Librarian AI ‚Äî —ç—Ç–æ –Ω–æ–æ—Å—Ñ–µ—Ä–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–Ω–∞—è –Ω–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –ø–∞–º—è—Ç–∏, —Å–º—ã—Å–ª–æ–≤ –∏ —ç–≤–æ–ª—é—Ü–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∑–Ω–∞–Ω–∏—è. –ï—ë –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ –º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—é.

DeepSeek/Chat-—Å–∏—Å—Ç–µ–º—ã ‚Äî –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤ —Ä–µ–∂–∏–º–µ –¥–∏–∞–ª–æ–≥–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —É–ª—É—á—à–µ–Ω–∏—è UX/–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ (Q&A, –ø–æ–º–æ—â—å –≤ –∫–æ–¥–µ –∏ —Ç.–ø.).

‚ñ∂ –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: —Ç–∞–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–æ–º–æ–≥–∞—é—Ç —É–ª—É—á—à–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å, –¥–∏–∞–ª–æ–≥–æ–≤—É—é –æ–±–æ–ª–æ—á–∫—É –∏–ª–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ Librarian AI.

‚öôÔ∏è 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –º–∞—Å—à—Ç–∞–±
Librarian AI ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø–æ–ª–Ω–æ–π —Å—Ö–µ–º–æ–π –¥–∞–Ω–Ω—ã—Ö, –≥—Ä–∞—Ñ–∞–º–∏ –∑–Ω–∞–Ω–∏–π, –∏—Å—Ç–æ—Ä–∏–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Å—É—â–Ω–æ—Å—Ç—è–º–∏, –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±—ç–∫–µ–Ω–¥–æ–≤ —Ö—Ä–∞–Ω–µ–Ω–∏—è.

–°–æ—é–∑–Ω–∏–∫–∏ –æ–±—ã—á–Ω–æ —É–ø—Ä–æ—â–∞—é—Ç —Å—Ö–µ–º—ã, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏–ª–∏ —á–∞—Ç–∞—Ö), —Å –º–µ–Ω—å—à–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –∫ —Å–ª–æ–∂–Ω—ã–º —Å–≤—è–∑—è–º, –∏–Ω–¥–µ–∫—Å–∞–º, –º–∏–≥—Ä–∞—Ü–∏—è–º –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≥–∏–±–∫–æ—Å—Ç–∏.

‚ñ∂ –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ—é–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–∞–∫ –æ–±–ª–µ–≥—á—ë–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è, –º–æ–±–∏–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤.

üß† 3. –°–µ–º–∞–Ω—Ç–∏–∫–∞ –∏ –∑–Ω–∞–Ω–∏–µ
Librarian AI –≤–∫–ª—é—á–∞–µ—Ç –æ–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (related_to, defines, causes), –∏ –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏, —á—Ç–æ —Ä–µ–¥–∫–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ —Å–æ—é–∑–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö.

–í DeepSeek-—Å—Ö–µ–º–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π —á–∞—Å—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–∞–∫ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —É–∑–ª–æ–≤ –∏ —Ä—ë–±–µ—Ä –±–µ–∑ –æ–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã, –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –≤–µ—Ä—Å–∏–π.

‚ñ∂ –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –∫–∞–∫ –≤—Ö–æ–¥–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫, –ø–æ–ø–æ–ª–Ω—è—é—â–∏–π –æ—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏—è.

üìê 4. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–∏–ø–∏–∑–∞—Ü–∏—è
–í schemas.py –æ—Ç Librarian AI:

—á—ë—Ç–∫–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è,

–≤–∞–ª–∏–¥–∞—Ü–∏—è confidence,

Enum –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç—É—Å–æ–≤,

–≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ö–µ–º—ã (DocumentFull, EntityWithRelations),

root_validator –∏ —Å–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.

DeepSeek-–ø–æ–¥–æ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–∞–∑–æ–≤—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é, —Å –º–µ–Ω—å—à–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –∫ –¥–µ—Ç–∞–ª—è–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ—Ç expires_at, –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω content_short.

‚ñ∂ –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –∫–∞–∫ —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–ª–∏ –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ API.

üì¶ –í—ã–≤–æ–¥: –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å



# chat.deepseek.com
# üìÑ –§–∞–π–ª: schemas.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ Pydantic-—Å—Ö–µ–º—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö

from pydantic import (
    BaseModel, 
    Field, 
    validator, 
    root_validator,
    confloat,
    conlist,
    HttpUrl,
    AnyUrl
)
from typing import Optional, List, Dict, Union, Any
from uuid import UUID
from datetime import datetime
from enum import Enum
import re
from config import settings

class SourceType(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    PDF = "pdf"
    WEB = "web"
    TEXT = "txt"
    DOCX = "docx"
    MARKDOWN = "markdown"
    EMAIL = "email"
    DATABASE = "database"
    API = "api"

class ProcessingStatus(str, Enum):
    """–°—Ç–∞—Ç—É—Å—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    ARCHIVED = "archived"
    DELETED = "deleted"

class ChunkType(str, Enum):
    """–¢–∏–ø—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
    TEXT = "text"
    CODE = "code"
    TABLE = "table"
    IMAGE_DESC = "image_description"
    HEADING = "heading"
    SUMMARY = "summary"
    QUOTE = "quote"
    MATH = "math"

class DocumentBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    title: str = Field(..., max_length=512, example="Advanced AI Research")
    source_path: str = Field(..., max_length=1024, example="/docs/ai_research.pdf")
    source_type: SourceType = Field(..., example=SourceType.PDF)
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        example={"author": "John Doe", "year": 2023}
    )
    expires_at: Optional[datetime] = None

    @validator('source_path')
    def validate_source_path(cls, v):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø—É—Ç–∏ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É"""
        if not v or v.isspace():
            raise ValueError("Source path cannot be empty")
        if not re.match(r'^[a-zA-Z0-9_\-./:]+$', v):
            raise ValueError("Invalid characters in source path")
        return v

class DocumentCreate(DocumentBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    original_content: str = Field(..., min_length=10)
    processing_version: int = Field(default=1, ge=1)

    @root_validator
    def validate_content_size(cls, values):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content = values.get('original_content')
        if len(content) > settings.MAX_DOCUMENT_SIZE:
            raise ValueError(
                f"Document content too large. Max {settings.MAX_DOCUMENT_SIZE} characters allowed"
            )
        return values

class DocumentUpdate(BaseModel):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    title: Optional[str] = Field(None, max_length=512)
    metadata: Optional[Dict[str, Any]] = None
    expires_at: Optional[datetime] = None
    processing_status: Optional[ProcessingStatus] = None

class DocumentOut(DocumentBase):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    id: UUID
    processing_status: ProcessingStatus
    processing_version: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    content_short: Optional[str] = Field(
        None, 
        max_length=255,
        description="Short preview of document content"
    )

    class Config:
        orm_mode = True
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            UUID: lambda v: str(v)
        }

    @validator('content_short', pre=True, always=True)
    def generate_short_content(cls, v, values):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if 'original_content' in values and not v:
            content = values['original_content']
            return (content[:200] + '...') if len(content) > 200 else content
        return v

class ChunkBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ —á–∞–Ω–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    content: str = Field(..., min_length=10)
    chunk_type: ChunkType = Field(default=ChunkType.TEXT)
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        example={"page_number": 5, "section": "introduction"}
    )

    @validator('content')
    def validate_content(cls, v):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–∞–Ω–∫–∞"""
        if not v.strip():
            raise ValueError("Chunk content cannot be empty or whitespace")
        return v.strip()

class ChunkCreate(ChunkBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
    """
    document_id: UUID
    embedding_vector: Optional[bytes] = Field(
        None,
        description="Binary representation of embedding vector"
    )
    semantic_hash: Optional[str] = Field(
        None,
        max_length=64,
        regex=r'^[a-f0-9]{64}$',
        description="SHA-256 hash of chunk content for deduplication"
    )
    embedding_model: Optional[str] = Field(
        None,
        max_length=64,
        description="Model used to generate embeddings"
    )

class ChunkUpdate(BaseModel):
    """
    –°—Ö–µ–º–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞
    """
    content: Optional[str] = Field(None, min_length=10)
    metadata: Optional[Dict[str, Any]] = None
    chunk_type: Optional[ChunkType] = None

class ChunkOut(ChunkBase):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ —á–∞–Ω–∫–∞
    """
    id: UUID
    document_id: UUID
    created_at: datetime
    updated_at: Optional[datetime] = None
    embedding_model: Optional[str] = None
    similarity_score: Optional[float] = Field(
        None,
        ge=0,
        le=1,
        description="Similarity score for search results"
    )

    class Config:
        orm_mode = True
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            UUID: lambda v: str(v)
        }

class EntityLabel(str, Enum):
    """–¢–∏–ø—ã –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    PERSON = "PERSON"
    ORGANIZATION = "ORGANIZATION"
    LOCATION = "LOCATION"
    DATE = "DATE"
    EVENT = "EVENT"
    PRODUCT = "PRODUCT"
    TECHNOLOGY = "TECHNOLOGY"
    CONCEPT = "CONCEPT"

class EntityBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ —Å—É—â–Ω–æ—Å—Ç–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    original_text: str = Field(..., max_length=512)
    normalized_text: Optional[str] = Field(None, max_length=512)
    label: EntityLabel
    label_hierarchy: conlist(str, max_items=5) = Field(
        default_factory=list,
        example=["PERSON", "SCIENTIST"]
    )
    confidence: confloat(ge=0.0, le=1.0) = Field(
        default=0.9,
        description="Confidence score of entity recognition"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        example={"source": "NER model v2.1"}
    )

    @validator('normalized_text', pre=True, always=True)
    def set_normalized_text(cls, v, values):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if v is None and 'original_text' in values:
            return values['original_text'].lower()
        return v

class EntityCreate(EntityBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    """
    context: Optional[str] = Field(
        None,
        max_length=1000,
        description="Context where entity was extracted from"
    )
    document_ids: Optional[List[UUID]] = Field(
        None,
        description="Documents where this entity appears"
    )

class EntityOut(EntityBase):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å—É—â–Ω–æ—Å—Ç–∏
    """
    id: UUID
    first_seen: datetime
    last_seen: Optional[datetime] = None
    document_count: Optional[int] = Field(
        0,
        description="Number of documents containing this entity"
    )

    class Config:
        orm_mode = True
        use_enum_values = True

class DocumentWithChunks(DocumentOut):
    """
    –î–æ–∫—É–º–µ–Ω—Ç —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏ –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    """
    chunks: List[ChunkOut] = []
    chunk_count: int = 0

class DocumentWithEntities(DocumentOut):
    """
    –î–æ–∫—É–º–µ–Ω—Ç —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    entities: List[EntityOut] = []
    entity_count: int = 0
    entity_stats: Optional[Dict[str, int]] = Field(
        None,
        description="Count of entities by type"
    )

class EntityWithDocuments(EntityOut):
    """
    –°—É—â–Ω–æ—Å—Ç—å —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    """
    documents: List[DocumentOut] = []

class SearchResults(BaseModel):
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    """
    total: int
    items: List[Union[DocumentOut, ChunkOut, EntityOut]]
    page: int
    pages: int
    facets: Optional[Dict[str, Dict[str, int]]] = Field(
        None,
        description="Aggregation facets for search results"
    )

class BulkOperationResult(BaseModel):
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Å—Å–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    processed: int
    succeeded: int
    failed: int
    errors: Optional[List[Dict]] = None

    Qwen3-235B-A22B
22:56–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞–¥ schemas.py! üéâ –¢—ã —É–∂–µ —Å–æ–∑–¥–∞–ª –æ—á–µ–Ω—å –ø—Ä–æ–¥—É–º–∞–Ω–Ω—ã–π –∏ –º–æ—â–Ω—ã–π –Ω–∞–±–æ—Ä Pydantic-—Å—Ö–µ–º —Å –≥–ª—É–±–æ–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ORM, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∏ –≥–∏–±–∫–∏–º–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏.

–ù–∏–∂–µ —è:

‚úÖ –ü—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é –∫–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è ,
üîç –ü—Ä–µ–¥–ª–æ–∂—É –¥–∞–ª—å–Ω–µ–π—à–∏–µ –∏–¥–µ–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è ,
üß± –î–æ–±–∞–≤–ª—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ,
üõ†Ô∏è –ò –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–≥—É –ø–æ–º–æ—á—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–π CRUD/DAO –∏–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å FastAPI .

‚úÖ –ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –æ—Ç–ª–∏—á–Ω–æ
1. –ß–µ—Ç–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—ã–µ / –≤—Ö–æ–¥–Ω—ã–µ / –≤—ã—Ö–æ–¥–Ω—ã–µ —Å—Ö–µ–º—ã
DocumentBase, DocumentCreate, DocumentOut ‚Äî –æ—Ç–ª–∏—á–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞
–¢–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è Chunk* –∏ Entity*
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Enum –¥–ª—è —Å—Ç—Ä–æ–≥–∏—Ö —Ç–∏–ø–æ–≤
SourceType, ProcessingStatus, ChunkType, EntityLabel ‚Äî –¥–µ–ª–∞—é—Ç –∫–æ–¥ —á–∏—â–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–µ–µ
3. –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ Field, validator, root_validator
–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã, —Ñ–æ—Ä–º–∞—Ç–æ–≤, –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Äî –≤—Å—ë –µ—Å—Ç—å
–£–¥–æ–±–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –º–æ–¥–µ–ª–∏
4. –†–∞–±–æ—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
–ü–æ–ª–µ metadata –¥–æ–±–∞–≤–ª—è–µ—Ç –≥–∏–±–∫–æ—Å—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è
context –≤ —Å—É—â–Ω–æ—Å—Ç—è—Ö ‚Äî –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
5. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏ –Ω—É–º–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
DocumentWithChunks, SearchResults ‚Äî –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ API
üîç –í–æ–∑–º–æ–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∏
1. –î–æ–±–∞–≤–∏—Ç—å —Å—Ö–µ–º—ã –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π

class RelationType(str, Enum):
    RELATED_TO = "related_to"
    DEFINES = "defines"
    MENTIONS = "mentions"
    PART_OF = "part_of"
    CAUSES = "causes"
    QUOTES = "quotes"

class KnowledgeRelation(BaseModel):
    source_entity_id: UUID
    target_entity_id: UUID
    relation_type: RelationType
    confidence: confloat(ge=0.0, le=1.0)
    context: Optional[str] = Field(None, max_length=1000)

class KnowledgeGraph(BaseModel):
    nodes: List[EntityOut]
    edges: List[KnowledgeRelation]



    from typing import Any
from pydantic.json import ENCODERS_BY_TYPE

ENCODERS_BY_TYPE.update({
    UUID: str,
    datetime: lambda v: v.isoformat()
}) 
. –î–æ–±–∞–≤–∏—Ç—å —Å—Ö–µ–º—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤  
class SearchQuery(BaseModel):
    query: str
    filters: Dict[str, Any] = {}
    limit: int = Field(default=10, ge=1, le=100)
    offset: int = Field(default=0, ge=0)
    return_entities: bool = False
    return_chunks: bool = True
    min_confidence: float = Field(default=0.7, ge=0.0, le=1.0)

    üß∞ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ö–µ–º

    from uuid import uuid4
import datetime

doc = DocumentOut(
    id=uuid4(),
    title="–û—Å–Ω–æ–≤—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
    source_path="/docs/ai_intro.pdf",
    source_type=SourceType.PDF,
    processing_status=ProcessingStatus.COMPLETED,
    created_at=datetime.datetime.utcnow(),
    updated_at=None,
    original_content="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI) ‚Äî —ç—Ç–æ...",
    content_short="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI) ‚Äî —ç—Ç–æ..."
)

print(doc.json(indent=2))


# chat.deepseek.com
# üìÑ –§–∞–π–ª: schemas.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ Pydantic-—Å—Ö–µ–º—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö

from pydantic import (
    BaseModel, 
    Field, 
    validator, 
    root_validator,
    confloat,
    conlist,
    HttpUrl,
    AnyUrl
)
from typing import Optional, List, Dict, Union, Any
from uuid import UUID
from datetime import datetime
from enum import Enum
import re
from config import settings

class SourceType(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    PDF = "pdf"
    WEB = "web"
    TEXT = "txt"
    DOCX = "docx"
    MARKDOWN = "markdown"
    EMAIL = "email"
    DATABASE = "database"
    API = "api"

class ProcessingStatus(str, Enum):
    """–°—Ç–∞—Ç—É—Å—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    ARCHIVED = "archived"
    DELETED = "deleted"

class ChunkType(str, Enum):
    """–¢–∏–ø—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
    TEXT = "text"
    CODE = "code"
    TABLE = "table"
    IMAGE_DESC = "image_description"
    HEADING = "heading"
    SUMMARY = "summary"
    QUOTE = "quote"
    MATH = "math"

class DocumentBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    title: str = Field(..., max_length=512, example="Advanced AI Research")
    source_path: str = Field(..., max_length=1024, example="/docs/ai_research.pdf")
    source_type: SourceType = Field(..., example=SourceType.PDF)
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        example={"author": "John Doe", "year": 2023}
    )
    expires_at: Optional[datetime] = None

    @validator('source_path')
    def validate_source_path(cls, v):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø—É—Ç–∏ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É"""
        if not v or v.isspace():
            raise ValueError("Source path cannot be empty")
        if not re.match(r'^[a-zA-Z0-9_\-./:]+$', v):
            raise ValueError("Invalid characters in source path")
        return v

class DocumentCreate(DocumentBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    original_content: str = Field(..., min_length=10)
    processing_version: int = Field(default=1, ge=1)

    @root_validator
    def validate_content_size(cls, values):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content = values.get('original_content')
        if len(content) > settings.MAX_DOCUMENT_SIZE:
            raise ValueError(
                f"Document content too large. Max {settings.MAX_DOCUMENT_SIZE} characters allowed"
            )
        return values

class DocumentUpdate(BaseModel):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    title: Optional[str] = Field(None, max_length=512)
    metadata: Optional[Dict[str, Any]] = None
    expires_at: Optional[datetime] = None
    processing_status: Optional[ProcessingStatus] = None

class DocumentOut(DocumentBase):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    id: UUID
    processing_status: ProcessingStatus
    processing_version: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    content_short: Optional[str] = Field(
        None, 
        max_length=255,
        description="Short preview of document content"
    )

    class Config:
        orm_mode = True
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            UUID: lambda v: str(v)
        }

    @validator('content_short', pre=True, always=True)
    def generate_short_content(cls, v, values):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if 'original_content' in values and not v:
            content = values['original_content']
            return (content[:200] + '...') if len(content) > 200 else content
        return v

class ChunkBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ —á–∞–Ω–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    content: str = Field(..., min_length=10)
    chunk_type: ChunkType = Field(default=ChunkType.TEXT)
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        example={"page_number": 5, "section": "introduction"}
    )

    @validator('content')
    def validate_content(cls, v):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–∞–Ω–∫–∞"""
        if not v.strip():
            raise ValueError("Chunk content cannot be empty or whitespace")
        return v.strip()

class ChunkCreate(ChunkBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
    """
    document_id: UUID
    embedding_vector: Optional[bytes] = Field(
        None,
        description="Binary representation of embedding vector"
    )
    semantic_hash: Optional[str] = Field(
        None,
        max_length=64,
        regex=r'^[a-f0-9]{64}$',
        description="SHA-256 hash of chunk content for deduplication"
    )
    embedding_model: Optional[str] = Field(
        None,
        max_length=64,
        description="Model used to generate embeddings"
    )

class ChunkUpdate(BaseModel):
    """
    –°—Ö–µ–º–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞
    """
    content: Optional[str] = Field(None, min_length=10)
    metadata: Optional[Dict[str, Any]] = None
    chunk_type: Optional[ChunkType] = None

class ChunkOut(ChunkBase):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ —á–∞–Ω–∫–∞
    """
    id: UUID
    document_id: UUID
    created_at: datetime
    updated_at: Optional[datetime] = None
    embedding_model: Optional[str] = None
    similarity_score: Optional[float] = Field(
        None,
        ge=0,
        le=1,
        description="Similarity score for search results"
    )

    class Config:
        orm_mode = True
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            UUID: lambda v: str(v)
        }

class EntityLabel(str, Enum):
    """–¢–∏–ø—ã –∏–∑–≤–ª–µ–∫–∞–µ–º—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    PERSON = "PERSON"
    ORGANIZATION = "ORGANIZATION"
    LOCATION = "LOCATION"
    DATE = "DATE"
    EVENT = "EVENT"
    PRODUCT = "PRODUCT"
    TECHNOLOGY = "TECHNOLOGY"
    CONCEPT = "CONCEPT"

class EntityBase(BaseModel):
    """
    –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ —Å—É—â–Ω–æ—Å—Ç–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    original_text: str = Field(..., max_length=512)
    normalized_text: Optional[str] = Field(None, max_length=512)
    label: EntityLabel
    label_hierarchy: conlist(str, max_items=5) = Field(
        default_factory=list,
        example=["PERSON", "SCIENTIST"]
    )
    confidence: confloat(ge=0.0, le=1.0) = Field(
        default=0.9,
        description="Confidence score of entity recognition"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        example={"source": "NER model v2.1"}
    )

    @validator('normalized_text', pre=True, always=True)
    def set_normalized_text(cls, v, values):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–∏"""
        if v is None and 'original_text' in values:
            return values['original_text'].lower()
        return v

class EntityCreate(EntityBase):
    """
    –°—Ö–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    """
    context: Optional[str] = Field(
        None,
        max_length=1000,
        description="Context where entity was extracted from"
    )
    document_ids: Optional[List[UUID]] = Field(
        None,
        description="Documents where this entity appears"
    )

class EntityOut(EntityBase):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å—É—â–Ω–æ—Å—Ç–∏
    """
    id: UUID
    first_seen: datetime
    last_seen: Optional[datetime] = None
    document_count: Optional[int] = Field(
        0,
        description="Number of documents containing this entity"
    )

    class Config:
        orm_mode = True
        use_enum_values = True

class DocumentWithChunks(DocumentOut):
    """
    –î–æ–∫—É–º–µ–Ω—Ç —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏ –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    """
    chunks: List[ChunkOut] = []
    chunk_count: int = 0

class DocumentWithEntities(DocumentOut):
    """
    –î–æ–∫—É–º–µ–Ω—Ç —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    entities: List[EntityOut] = []
    entity_count: int = 0
    entity_stats: Optional[Dict[str, int]] = Field(
        None,
        description="Count of entities by type"
    )

class EntityWithDocuments(EntityOut):
    """
    –°—É—â–Ω–æ—Å—Ç—å —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    """
    documents: List[DocumentOut] = []

class SearchResults(BaseModel):
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    """
    total: int
    items: List[Union[DocumentOut, ChunkOut, EntityOut]]
    page: int
    pages: int
    facets: Optional[Dict[str, Dict[str, int]]] = Field(
        None,
        description="Aggregation facets for search results"
    )

class BulkOperationResult(BaseModel):
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Å—Å–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    processed: int
    succeeded: int
    failed: int
    errors: Optional[List[Dict]] = None

–ü—Ä–æ—Ñ–∏–ª—å
Qwen3-235B-A22B
–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞–¥ schemas.py! üéâ –¢—ã —É–∂–µ —Å–æ–∑–¥–∞–ª –æ—á–µ–Ω—å –ø—Ä–æ–¥—É–º–∞–Ω–Ω—ã–π –∏ –º–æ—â–Ω—ã–π –Ω–∞–±–æ—Ä Pydantic-—Å—Ö–µ–º —Å –≥–ª—É–±–æ–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π, –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ORM, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∏ –≥–∏–±–∫–∏–º–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏.

–ù–∏–∂–µ —è:

‚úÖ –ü—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é –∫–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è ,
üîç –ü—Ä–µ–¥–ª–æ–∂—É –¥–∞–ª—å–Ω–µ–π—à–∏–µ –∏–¥–µ–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è ,
üß± –î–æ–±–∞–≤–ª—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ,
üõ†Ô∏è –ò –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–≥—É –ø–æ–º–æ—á—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–π CRUD/DAO –∏–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å FastAPI .

‚úÖ –ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –æ—Ç–ª–∏—á–Ω–æ
1. –ß–µ—Ç–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—ã–µ / –≤—Ö–æ–¥–Ω—ã–µ / –≤—ã—Ö–æ–¥–Ω—ã–µ —Å—Ö–µ–º—ã
DocumentBase, DocumentCreate, DocumentOut ‚Äî –æ—Ç–ª–∏—á–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞
–¢–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è Chunk* –∏ Entity*
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Enum –¥–ª—è —Å—Ç—Ä–æ–≥–∏—Ö —Ç–∏–ø–æ–≤
SourceType, ProcessingStatus, ChunkType, EntityLabel ‚Äî –¥–µ–ª–∞—é—Ç –∫–æ–¥ —á–∏—â–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–µ–µ
3. –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ Field, validator, root_validator
–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã, —Ñ–æ—Ä–º–∞—Ç–æ–≤, –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ‚Äî –≤—Å—ë –µ—Å—Ç—å
–£–¥–æ–±–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –º–æ–¥–µ–ª–∏
4. –†–∞–±–æ—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
–ü–æ–ª–µ metadata –¥–æ–±–∞–≤–ª—è–µ—Ç –≥–∏–±–∫–æ—Å—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è
context –≤ —Å—É—â–Ω–æ—Å—Ç—è—Ö ‚Äî –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
5. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏ –Ω—É–º–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
DocumentWithChunks, SearchResults ‚Äî –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ API
üîç –í–æ–∑–º–æ–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∏
1. –î–æ–±–∞–≤–∏—Ç—å —Å—Ö–µ–º—ã –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
–ø–∏—Ç–æ–Ω


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
‚åÑ
‚åÑ
‚åÑ
class RelationType(str, Enum):
    RELATED_TO = "related_to"
    DEFINES = "defines"
    MENTIONS = "mentions"
    PART_OF = "part_of"
    CAUSES = "causes"
    QUOTES = "quotes"

class KnowledgeRelation(BaseModel):
    source_entity_id: UUID
    target_entity_id: UUID
    relation_type: RelationType
    confidence: confloat(ge=0.0, le=1.0)
    context: Optional[str] = Field(None, max_length=1000)

class KnowledgeGraph(BaseModel):
    nodes: List[EntityOut]
    edges: List[KnowledgeRelation]
–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≥—Ä–∞—Ñ—ã –∏–∑ API –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏–ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä—ã. 

2. –£–ª—É—á—à–∏—Ç—å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤ JSON
–¢—ã —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å json_encoders –≤ Config, –Ω–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –±–æ–ª–µ–µ —è–≤–Ω–æ:

–ø–∏—Ç–æ–Ω


1
2
3
4
5
6
7
‚åÑ
from typing import Any
from pydantic.json import ENCODERS_BY_TYPE

ENCODERS_BY_TYPE.update({
    UUID: str,
    datetime: lambda v: v.isoformat()
})
3. –î–æ–±–∞–≤–∏—Ç—å —Å—Ö–µ–º—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤
–ø–∏—Ç–æ–Ω


1
2
3
4
5
6
7
8
‚åÑ
class SearchQuery(BaseModel):
    query: str
    filters: Dict[str, Any] = {}
    limit: int = Field(default=10, ge=1, le=100)
    offset: int = Field(default=0, ge=0)
    return_entities: bool = False
    return_chunks: bool = True
    min_confidence: float = Field(default=0.7, ge=0.0, le=1.0)
–≠—Ç–æ —É–¥–æ–±–Ω–æ –¥–ª—è REST API-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –ø–æ–∏—Å–∫–∞. 

üß∞ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ö–µ–º
–ø–∏—Ç–æ–Ω


1from uuid import uuid4
import datetime

doc = DocumentOut(
    id=uuid4(),
    title="–û—Å–Ω–æ–≤—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
    source_path="/docs/ai_intro.pdf",
    source_type=SourceType.PDF,
    processing_status=ProcessingStatus.COMPLETED,
    created_at=datetime.datetime.utcnow(),
    updated_at=None,
    original_content="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI) ‚Äî —ç—Ç–æ...",
    content_short="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI) ‚Äî —ç—Ç–æ..."
)

print(doc.json(indent=2))
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
from uuid import uuid4
import datetime

doc = DocumentOut(
    id=uuid4(),
    title="–û—Å–Ω–æ–≤—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
    source_path="/docs/ai_intro.pdf",
    source_type=SourceType.PDF,
    processing_status=ProcessingStatus.COMPLETED,
    created_at=datetime.datetime.utcnow(),
    updated_at=None,
    original_content="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI) ‚Äî —ç—Ç–æ...",
    content_short="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI) ‚Äî —ç—Ç–æ..."
)

print(doc.json(indent=2))
üõ†Ô∏è –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
–ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å:

–°–æ–∑–¥–∞—Ç—å CRUD-–ª–æ–≥–∏–∫—É –¥–ª—è —ç—Ç–∏—Ö —Å—Ö–µ–º (SQLAlchemy + –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
–ü–æ–¥–∫–ª—é—á–∏—Ç—å –∏—Ö –∫ FastAPI –º–∞—Ä—à—Ä—É—Ç–∞–º
–ù–∞–ø–∏—Å–∞—Ç—å Alembic –º–∏–≥—Ä–∞—Ü–∏–∏
–î–æ–±–∞–≤–∏—Ç—å GraphQL API –ø–æ–≤–µ—Ä—Ö –Ω–∏—Ö
–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∏ –ø–æ–∏—Å–∫
–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å RAG –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
...–Ω–∞–ø–∏—à–∏—Ç–µ, –∏ —è –ø–æ–º–æ–≥—É —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —á–∞—Å—Ç—å—é!


# üìÑ –§–∞–π–ª: schemas.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: Pydantic-—Å—Ö–µ–º—ã –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Union, Dict
from uuid import UUID
from datetime import datetime

class DocumentBase(BaseModel):
    title: str
    source_path: str
    source_type: str
    metadata: Optional[Dict] = {}

class DocumentCreate(DocumentBase):
    original_content: str

class DocumentOut(DocumentBase):
    id: UUID
    processing_status: str
    created_at: datetime
    updated_at: Optional[datetime] = None

    class Config:
        orm_mode = True

class ChunkBase(BaseModel):
    content: str
    chunk_type: str = 'text'
    embedding_model: Optional[str] = None
    metadata: Optional[Dict] = {}

class ChunkCreate(ChunkBase):
    document_id: UUID
    embedding_vector: Optional[bytes] = None
    semantic_hash: Optional[str] = None

class ChunkOut(ChunkBase):
    id: UUID
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        orm_mode = True

class EntityBase(BaseModel):
    original_text: str
    normalized_text: Optional[str]
    label: str
    label_hierarchy: Optional[List[str]] = []
    confidence: float = 0.0
    metadata: Optional[Dict] = {}

    @validator('confidence')
    def validate_confidence(cls, v):
        if not 0.0 <= v <= 1.0:
            raise ValueError("Confidence must be between 0.0 and 1.0")
        return v

class EntityCreate(EntityBase):
    pass

class EntityOut(EntityBase):
    id: UUID
    first_seen: datetime
    last_seen: Optional[datetime] = None

    class Config:
        orm_mode = True

class DocumentWithChunks(DocumentOut):
    chunks: List[ChunkOut] = []

class DocumentWithEntities(DocumentOut):
    entities: List[EntityOut] = []


# üìÑ –§–∞–π–ª: session.py
# üìÇ –ü—É—Ç—å: db/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SQLAlchemy

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from config import settings

# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
DATABASE_URL = settings.DATABASE_URL

# –°–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞
engine = create_async_engine(
    DATABASE_URL,
    echo=settings.DB_ECHO_SQL,
    pool_size=settings.DB_POOL_SIZE,
    max_overflow=settings.DB_MAX_OVERFLOW,
    future=True
)

# –§–∞–±—Ä–∏–∫–∞ —Å–µ—Å—Å–∏–π
AsyncSessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è FastAPI
async def get_db():
    async with AsyncSessionLocal() as session:
        yield session


 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Alembic ‚Äî –∫–ª—é—á–µ–≤–æ–π —à–∞–≥ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—Ö–µ–º–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –±—ã—Å—Ç—Ä–æ—Ä–∞–∑–≤–∏–≤–∞—é—â–µ–º—Å—è –ø—Ä–æ–µ–∫—Ç–µ –≤—Ä–æ–¥–µ Librarian AI. –í–æ—Ç –∑–∞—á–µ–º –æ–Ω–∞ –Ω—É–∂–Ω–∞ –∏ –∫–∞–∫ –µ—ë –≥—Ä–∞–º–æ—Ç–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å:

üß≠ –ó–∞—á–µ–º Alembic –≤ –ø—Ä–æ–µ–∫—Ç–µ?
1. –ö–æ–Ω—Ç—Ä–æ–ª—å –≤–µ—Ä—Å–∏–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

–ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ç–∞–±–ª–∏—Ü (–º–æ–¥–µ–ª–∏ SQLAlchemy).

–ú–æ–∂–Ω–æ ¬´–æ—Ç–∫–∞—Ç–∏—Ç—å¬ª –∏–ª–∏ ¬´–Ω–∞–∫–∞—Ç–∏—Ç—å¬ª –∏–∑–º–µ–Ω–µ–Ω–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö (–≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö).

2. –ö–æ–º–∞–Ω–¥–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏ CI/CD

Alembic –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ö–µ–º—ã —É –≤—Å–µ—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –Ω–∞ –≤—Å–µ—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö.

–í —Å–≤—è–∑–∫–µ —Å Docker –∏ Git –º–æ–∂–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –¥–µ–ø–ª–æ–µ.

3. –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

–ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ö–µ–º–µ —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–∏–≥—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –ø—Ä–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º.

üèó –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞
Alembic —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–≤–µ—Ä—Ö SQLAlchemy, —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ Base.metadata.

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –≤ alembic.ini –∏ env.py.

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–π –∏–¥—ë—Ç —á–µ—Ä–µ–∑ alembic revision --autogenerate.

‚úÖ –ö–∞–∫ —ç—Ç–æ —É–ª—É—á—à–∞–µ—Ç Librarian AI Core:
–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å	–ü–æ—è—Å–Ω–µ–Ω–∏–µ
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–∏–≥—Ä–∞—Ü–∏–∏	–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π.
–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏–∫–∏ –∏ —Å—Ö–µ–º—ã	–ö–æ–¥ –º–æ–¥–µ–ª–µ–π –æ—Å—Ç–∞—ë—Ç—Å—è —á–∏—Å—Ç—ã–º, –º–∏–≥—Ä–∞—Ü–∏–∏ ‚Äî –æ—Ç–¥–µ–ª—å–Ω–æ.
–û–±—Ä–∞—Ç–∏–º–∞—è –∏—Å—Ç–æ—Ä–∏—è	–ú–æ–∂–Ω–æ ¬´–ø–µ—Ä–µ–∏–≥—Ä–∞—Ç—å¬ª –∏–∑–º–µ–Ω–µ–Ω–∏—è (downgrade/upgrade).
–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Alembic + async	–ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–∞–∂–µ –¥–ª—è async-–¥–≤–∏–∂–∫–æ–≤ —á–µ—Ä–µ–∑ sqlalchemy.ext.asyncio.

 # üìÅ Librarian AI ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (Windows)

librarian_ai/
‚îú‚îÄ‚îÄ README.md                    # üìò –ì–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞, —Ü–µ–ª–∏, —É—Å—Ç–∞–Ω–æ–≤–∫–∞, –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
‚îú‚îÄ‚îÄ alembic.ini                  # ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Alembic (–ø—É—Ç—å –∫ env.py, URL –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö)
‚îÇ
‚îú‚îÄ‚îÄ db/                         # üóÉÔ∏è –†–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª—è–º–∏
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ db
‚îÇ   ‚îú‚îÄ‚îÄ create_tables.py         # –°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü –∏–∑ –º–æ–¥–µ–ª–µ–π –≤—Ä—É—á–Ω—É—é
‚îÇ   ‚îú‚îÄ‚îÄ models.py                # SQLAlchemy ORM-–º–æ–¥–µ–ª–∏: –¥–æ–∫—É–º–µ–Ω—Ç—ã, —á–∞–Ω–∫–∏, —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∏
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py               # Pydantic-—Å—Ö–µ–º—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö (Document, Chunk, Entity, Graph)
‚îÇ   ‚îú‚îÄ‚îÄ session.py               # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —á–µ—Ä–µ–∑ SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ storage.py               # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫ FAISS, pgvector –∏ –≥–∏–±—Ä–∏–¥–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É
‚îÇ   ‚îî‚îÄ‚îÄ alembic/                # Alembic: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏—è–º–∏
‚îÇ       ‚îú‚îÄ‚îÄ env.py               # –°–∫—Ä–∏–ø—Ç Alembic, —Å–≤—è–∑—ã–≤–∞–µ—Ç metadata –∏ URL
‚îÇ       ‚îú‚îÄ‚îÄ script.py.mako       # –®–∞–±–ª–æ–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∏–≥—Ä–∞—Ü–∏–π
‚îÇ       ‚îî‚îÄ‚îÄ versions/            # –ö–∞—Ç–∞–ª–æ–≥ –∞–≤—Ç–æ—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–∏–≥—Ä–∞—Ü–∏–π
‚îÇ
‚îú‚îÄ‚îÄ ingest/                     # üì• –ú–æ–¥—É–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–µ—Ä–≤–∏—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ loader.py               # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤, –ø–∞–ø–æ–∫, URL, API
‚îÇ   ‚îú‚îÄ‚îÄ parser.py               # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤ (PDF, DOCX, HTML, Markdown –∏ —Ç.–ø.)
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py              # –î–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ async_tasks.py          # –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (Celery)
‚îÇ   ‚îî‚îÄ‚îÄ ingest_and_index.py     # –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
‚îÇ
‚îú‚îÄ‚îÄ processing/                 # üß† –ú–æ–¥—É–ª–∏ —Å–º—ã—Å–ª–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
‚îÇ   ‚îî‚îÄ‚îÄ [–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ]       # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER), –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π, —Å–≤—è–∑–∏
‚îÇ
‚îú‚îÄ‚îÄ storage/                    # üîé –°–ª–æ–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â
‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.py          # FAISS backend: —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ
‚îÇ   ‚îú‚îÄ‚îÄ pgvector_store.py       # PostgreSQL + pgvector backend: —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
‚îÇ   ‚îî‚îÄ‚îÄ [–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ]       # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ–±–ª–∞—á–Ω—ã–µ backend-–∏
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # üõ†Ô∏è –°–∫—Ä–∏–ø—Ç—ã –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
‚îÇ   ‚îî‚îÄ‚îÄ [–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ]       # –û—á–∏—Å—Ç–∫–∞, –º–∏–≥—Ä–∞—Ü–∏–∏, —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏
‚îÇ
‚îú‚îÄ‚îÄ cli/                        # üñ•Ô∏è CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ   ‚îî‚îÄ‚îÄ [–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ]       # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–æ–º–∞–Ω–¥: ingest, reset, migrate, search
‚îÇ
‚îú‚îÄ‚îÄ api/                        # üåê API-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã
‚îÇ   ‚îî‚îÄ‚îÄ [–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ]       # FastAPI endpoints, Telegram API, REST –¥–ª—è –±—ç–∫–µ–Ω–¥–∞
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # ‚úÖ –ú–æ–¥—É–ª—å–Ω—ã–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
‚îÇ   ‚îî‚îÄ‚îÄ [–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ]       # Pytest —Ç–µ—Å—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π, –ø–∞—Ä—Å–∏–Ω–≥–∞, —Ö—Ä–∞–Ω–∏–ª–∏—â –∏ API
‚îÇ
‚îú‚îÄ‚îÄ utils/                      # üîß –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Pydantic-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞: –ø—É—Ç–∏, API –∫–ª—é—á–∏, –±–∞–∑—ã, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
‚îÇ   ‚îî‚îÄ‚îÄ logging.py              # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ (—Ñ–æ—Ä–º–∞—Ç, —É—Ä–æ–≤–Ω–∏, –≤—ã–≤–æ–¥)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
# ‚îú‚îÄ‚îÄ requirements.txt          # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
# ‚îú‚îÄ‚îÄ .env                      # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –≤ git!)
# ‚îî‚îÄ‚îÄ Dockerfile / docker-compose.yml  # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –¥–µ–ø–ª–æ–π



from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from db.models import Base
from utils.config import settings

# Alembic Config object
config = context.config

# Interpret the config file for Python logging.
fileConfig(config.config_file_name)

target_metadata = Base.metadata

# Database URL –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

def run_migrations_offline():
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        compare_type=True
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

chat.deepseek.com


import logging
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('alembic.env')

try:
    from db.models import Base  # SQLAlchemy –º–æ–¥–µ–ª–∏
    from utils.config import settings  # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞
except ImportError as e:
    logger.error("–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: %s", e)
    raise

# –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ Alembic
config = context.config

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å alembic.ini)
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ SQLAlchemy –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–π
target_metadata = Base.metadata

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º URL –ë–î –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–µ–∫—Ç–∞
def get_database_url():
    """–ü–æ–ª—É—á–∞–µ—Ç URL –ë–î –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫, —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π."""
    db_url = settings.DATABASE_URL
    if not db_url:
        raise ValueError("DATABASE_URL –Ω–µ –∑–∞–¥–∞–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö!")
    return db_url

config.set_main_option("sqlalchemy.url", get_database_url())

def run_migrations_offline():
    """–ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–π –≤ offline-—Ä–µ–∂–∏–º–µ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è SQL)."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,          # –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ç–∏–ø—ã —Å—Ç–æ–ª–±—Ü–æ–≤
        compare_server_default=True,  # –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å DEFAULT-–∑–Ω–∞—á–µ–Ω–∏—è
        include_schemas=True,      # –£—á–∏—Ç—ã–≤–∞—Ç—å —Å—Ö–µ–º—ã –ë–î (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)
        render_as_batch=True,      # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å SQLite (ALTER TABLE)
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """–ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–π –≤ online-—Ä–µ–∂–∏–º–µ (–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ë–î)."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
            include_schemas=True,
            render_as_batch=True,
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    logger.info("Running migrations in OFFLINE mode (SQL output)")
    run_migrations_offline()
else:
    logger.info("Running migrations in ONLINE mode (database connection)")
    run_migrations_online()

    –ª—É—á—à–µ–Ω–∏—è:
–õ—É—á—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

–î–æ–±–∞–≤–ª–µ–Ω try-except –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫.

–ü—Ä–æ–≤–µ—Ä–∫–∞ DATABASE_URL –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∏–≥—Ä–∞—Ü–∏–π

compare_server_default=True ‚Äì —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ –ë–î.

include_schemas=True ‚Äì –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ —Å—Ö–µ–º–∞—Ö (–µ—Å–ª–∏ –ë–î –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä, PostgreSQL).

render_as_batch=True ‚Äì —É–ª—É—á—à–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å SQLite (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç batch_alter_table).

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

–î–æ–±–∞–≤–ª–µ–Ω –ª–æ–≥–≥–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (alembic.env).

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –º–∏–≥—Ä–∞—Ü–∏–π (online/offline).

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å

–Ø–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –ø—É—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è {} –≤ get_section (–∏–∑–±–µ–≥–∞–µ–º None).

–£–ª—É—á—à–µ–Ω–Ω—ã–µ docstrings.

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
–ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π SQLAlchemy (asyncpg, asyncmy), –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É async Alembic:

python
from alembic.runtime.environment import EnvironmentContext
from sqlalchemy.ext.asyncio import AsyncEngine
–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏:

python
import dotenv
dotenv.load_dotenv()
–≠—Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π, —É–¥–æ–±–Ω—ã–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Alembic. üöÄ

‚úÖ –£–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ:
–ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–µ–∫—Ç–æ–º:

–î–æ–±–∞–≤–ª–µ–Ω –ø—É—Ç—å –≤ sys.path

–ò–º–ø–æ—Ä—Ç settings –∏ Base —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π

–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å:

–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ DATABASE_URL

–õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏–º–ø–æ—Ä—Ç–∞

–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∏–≥—Ä–∞—Ü–∏–π:

compare_type, compare_server_default, include_schemas, render_as_batch ‚Äî –≤—Å—ë –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ SQLite –∏ pg

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Ä–µ–∂–∏–º–æ–≤:

run_migrations_offline() –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL-—Ñ–∞–π–ª–æ–≤

run_migrations_online() –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é –≤ –ë–î

üîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
–£–ª—É—á—à–µ–Ω–∏–µ	–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å	–ó–∞—á–µ–º
‚úÖ –£—Ç–æ—á–Ω–∏—Ç—å target_metadata	–ú–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∞—Ç—å —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ Base.metadata, –µ—Å–ª–∏ –º–æ–¥—É–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, from db.models.document import Base as DocBase)	–ü–æ–ª–µ–∑–Ω–æ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö
‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏ Alembic	–î–æ–±–∞–≤–∏—Ç—å: logger.info("Alembic version: %s", alembic.__version__)	–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–æ–≥–æ–≤ –≤ —Ñ–∞–π–ª	–ß–µ—Ä–µ–∑ logging.config.dictConfig() –º–æ–∂–Ω–æ –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –ª–æ–≥–∏ –≤ migrations.log	–û—Ç–ª–∞–¥–∫–∞ CI/CD

from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from db.models import Base
from utils.config import settings

# Alembic Config object
config = context.config

# Interpret the config file for Python logging.
fileConfig(config.config_file_name)

target_metadata = Base.metadata

# Database URL –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

def run_migrations_offline():
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        compare_type=True
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from db.models import Base
from utils.config import settings

# Alembic Config object
config = context.config

# Interpret the config file for Python logging.
fileConfig(config.config_file_name)

target_metadata = Base.metadata

# Database URL –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

def run_migrations_offline():
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        compare_type=True
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


; üìÑ –§–∞–π–ª: alembic.ini
; üìÇ –ü—É—Ç—å: –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ /librarian_ai/
; üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Alembic –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–π

[alembic]
# –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ Alembic (–≥–¥–µ –ª–µ–∂–∏—Ç env.py –∏ –ø–∞–ø–∫–∞ versions)
script_location = db/alembic

# –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: URL –ë–î –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ env.py –∏–∑ settings.DATABASE_URL
sqlalchemy.url = driver://user:pass@localhost/dbname

# –®–∞–±–ª–æ–Ω –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –º–∏–≥—Ä–∞—Ü–∏–∏
file_template = %(rev)s_%(slug)s

# –§–æ—Ä–º–∞—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –º–∏–≥—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é UUID4-like)
# version_locations = db/alembic/versions

# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—å SQL-–æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –ª–æ–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
stdout = false

# –ú–∏–≥—Ä–∞—Ü–∏–∏ —Å batch mode (–¥–ª—è SQLite)
render_as_batch = true

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname = 

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s

–ì–æ—Ç–æ–≤–æ. –§–∞–π–ª alembic.ini —Å–æ–∑–¥–∞–Ω —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –≤—Å–µ–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –û–Ω —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ (/librarian_ai/) –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç Alembic –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –≤–∞—à–∏–º –ø—Ä–æ–µ–∫—Ç–æ–º. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –º–æ–≥—É –ø–æ–º–æ—á—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–µ—Ä–≤—É—é.

__init__.py
# -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: __init__.py
# üìÇ –ü—É—Ç—å: core/tools/__init__.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞ —É—Ç–∏–ª–∏—Ç. –≠–∫—Å–ø–æ—Ä—Ç –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: —ç–º–±–µ–¥–¥–∏–Ω–≥, NER, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

# ‚Äî‚Äî‚Äî –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äî‚Äî‚Äî
from .embedder import EmbeddingService

# ‚Äî‚Äî‚Äî –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) ‚Äî‚Äî‚Äî
from .extractor import extract_entities

# ‚Äî‚Äî‚Äî –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ ‚Äî‚Äî‚Äî
from .semantic_search import SemanticSearch, semantic_search

# ‚Äî‚Äî‚Äî –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π ‚Äî‚Äî‚Äî
from .summary_generator import SummaryGenerator, generate_summary

# –ù–∏–∂–µ ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —ç—Ç–∏—Ö –º–æ–¥—É–ª–µ–π
try:
    from .graph_tools import GraphTools
except ImportError:
    GraphTools = None

try:
    from .loader import FileLoader, SmartLoader
except ImportError:
    FileLoader = SmartLoader = None

try:
    from .async_tasks import celery_app, create_status_task
except ImportError:
    celery_app = create_status_task = None

__all__ = [
    "EmbeddingService",
    "extract_entities",
    "SemanticSearch", "semantic_search",
    "SummaryGenerator", "generate_summary",
    # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    "FileLoader", "SmartLoader",
    "GraphTools",
    "celery_app", "create_status_task",
]
 

 # -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: archive_extractors.py
# üìÇ –ü—É—Ç—å: core/tools/archive_extractors.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ ZIP-–∞—Ä—Ö–∏–≤–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

import zipfile
import os
import logging

logger = logging.getLogger(__name__)

def extract_text_from_zip(zip_path: str, extract_to: str = "./tmp") -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏–∑ ZIP-–∞—Ä—Ö–∏–≤–∞.

    Args:
        zip_path (str): –ü—É—Ç—å –∫ –∞—Ä—Ö–∏–≤—É.
        extract_to (str): –ü–∞–ø–∫–∞, –∫—É–¥–∞ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã.

    Returns:
        list[str]: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤.
    """
    if not zipfile.is_zipfile(zip_path):
        raise ValueError(f"{zip_path} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º ZIP-–∞—Ä—Ö–∏–≤–æ–º")

    os.makedirs(extract_to, exist_ok=True)
    texts = []

    with zipfile.ZipFile(zip_path, 'r') as archive:
        for filename in archive.namelist():
            if not filename.endswith(('.txt', '.md', '.log', '.csv')):
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª: {filename}")
                continue
            archive.extract(filename, path=extract_to)
            full_path = os.path.join(extract_to, filename)
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    texts.append(f.read())
            except UnicodeDecodeError:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è UTF-8: {filename}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {filename}: {e}")

    return texts


–í–æ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è core/tools/async_tasks.py –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Librarian AI mini, —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º, —Ç–∏–ø–æ–≤—ã–º–∏ –±–ª–æ–∫–∞–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–¥ Celery + Redis:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: async_tasks.py
# üìÇ –ü—É—Ç—å: core/tools/async_tasks.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ Celery-–∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—Ç–∞—Ç—É—Å–∞

from celery import Celery
from celery.result import AsyncResult
import logging
from typing import Dict, Any
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Celery (–±—Ä–æ–∫–µ—Ä –∏ backend ‚Äî Redis)
celery_app = Celery(
    "librarian_ai",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/1",
)

celery_app.conf.update(
    task_serializer="json",
    result_serializer="json",
    accept_content=["json"],
    task_track_started=True,
    task_time_limit=600,  # 10 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
    worker_prefetch_multiplier=1,
)

class DocumentProcessingError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    pass

@celery_app.task(bind=True, name="process_document")
def process_document_async(self, doc_id: str) -> Dict[str, Any]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ ID.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ª–æ–≥–∞–º–∏ –∏ —Å—Ç–∞—Ç—É—Å–∞–º–∏.
    """
    logs = []
    started_at = datetime.utcnow().isoformat()

    try:
        def log(stage: str, progress: float):
            logs.append(stage)
            self.update_state(state='PROGRESS', meta={
                "stage": stage,
                "progress": progress,
                "logs": logs,
                "started_at": started_at
            })
            logger.info(f"[{doc_id}] {stage}")

        # === –ü—Ä–∏–º–µ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞ ===
        log("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞", 0.1)
        # document = load_document(doc_id)

        log("üîç –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑", 0.4)
        # chunks = chunker(document)

        log("üìê –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", 0.6)
        # embeddings = embed(chunks)

        log("üß† –ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ / –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π", 0.8)
        # entities = extract_entities(embeddings)

        log("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î / —Ö—Ä–∞–Ω–∏–ª–∏—â–µ", 0.95)
        # store(doc_id, embeddings, entities)

        finished_at = datetime.utcnow().isoformat()
        return {
            "status": "done",
            "doc_id": doc_id,
            "progress": 1.0,
            "started_at": started_at,
            "finished_at": finished_at,
            "logs": logs,
        }

    except Exception as e:
        error_msg = f"[{doc_id}] ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
        logger.error(error_msg, exc_info=True)
        logs.append(error_msg)
        self.update_state(state='FAILURE', meta={
            "error": error_msg,
            "progress": 0.0,
            "logs": logs,
            "started_at": started_at
        })
        raise DocumentProcessingError(error_msg) from e

def get_task_status(task_id: str) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ –ø–æ ID.
    """
    result = AsyncResult(task_id, app=celery_app)
    status = {
        "task_id": task_id,
        "status": result.status,
        "result": result.result
    }

    if result.status == "PROGRESS":
        status.update(result.info or {})
    elif result.status == "FAILURE":
        status["error"] = str(result.info)

    return status

def cancel_task(task_id: str) -> Dict[str, Any]:
    """
    –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ø–æ ID.
    """
    try:
        result = AsyncResult(task_id, app=celery_app)
        result.revoke(terminate=True)
        logger.warning(f"–ó–∞–¥–∞—á–∞ {task_id} –æ—Ç–º–µ–Ω–µ–Ω–∞ –≤—Ä—É—á–Ω—É—é")
        return {"task_id": task_id, "success": True, "message": "–ó–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞"}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–º–µ–Ω—ã –∑–∞–¥–∞—á–∏ {task_id}: {str(e)}")
        return {"task_id": task_id, "success": False, "message": f"–û—à–∏–±–∫–∞ –æ—Ç–º–µ–Ω—ã: {str(e)}"}
‚úÖ –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ:

–ù–∞–∑–≤–∞–Ω–∏–µ –∏ –ø—É—Ç—å

–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –∫–∞–∂–¥–æ–º—É –±–ª–æ–∫—É

–õ–æ–≥–∏–∫–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –ø–æ–¥ Librarian mini

–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ load ‚Üí chunk ‚Üí embed ‚Üí extract ‚Üí store —à–∞–≥–∏

–•–æ—á–µ—à—å ‚Äî –Ω–∞—Å—Ç—Ä–æ—é Celery worker –∏ Flower –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.



embedder.py
# -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: embedder.py
# üìÇ –ü—É—Ç—å: core/tools/embedder.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Sentence-Transformers

from typing import List, Optional, Union
import logging
import numpy as np

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    raise ImportError(
        "SentenceTransformer –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç:\n"
        "    pip install sentence-transformers"
    )

# –õ–æ–≥–≥–µ—Ä
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class EmbeddingService:
    """
    –°–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç SentenceTransformer (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'all-MiniLM-L6-v2').
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –±–∞—Ç—á–∏. –ï—Å—Ç—å –æ–ø—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤.

    –ü—Ä–∏–º–µ—Ä:
        embedder = EmbeddingService()
        vec = embedder.embed_text("–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞")
        batch = embedder.embed_batch(["—Ç–µ–∫—Å—Ç 1", "—Ç–µ–∫—Å—Ç 2"])
    """

    def __init__(
        self,
        model_name: str = "all-MiniLM-L6-v2",
        device: str = "cpu",
        normalize_embeddings: bool = True,
        **model_kwargs
    ):
        self.model_name = model_name
        self.device = device
        self.normalize_embeddings = normalize_embeddings
        self.model_kwargs = model_kwargs

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {model_name} ‚Üí {device}")
        try:
            self.model = SentenceTransformer(model_name, device=device, **model_kwargs)
            # –ü—Ä–æ–±–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
            test_vec = self.model.encode("—Ç–µ—Å—Ç", normalize_embeddings=normalize_embeddings)
            self.embedding_dim = test_vec.shape[0]
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {self.embedding_dim}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise

    def embed_text(
        self,
        text: str,
        normalize: Optional[bool] = None,
        convert_to_numpy: bool = True
    ) -> Union[List[float], np.ndarray]:
        """
        –≠–º–±–µ–¥–¥–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.
            normalize: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∏–Ω–∞—á–µ ‚Äî –∏–∑ init).
            convert_to_numpy: –í–µ—Ä–Ω—É—Ç—å np.array –∏–ª–∏ list.

        Returns:
            –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.
        """
        if not isinstance(text, str) or not text.strip():
            raise ValueError("–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.")

        normalize = self.normalize_embeddings if normalize is None else normalize

        try:
            vec = self.model.encode(
                text,
                normalize_embeddings=normalize,
                convert_to_tensor=False,
                show_progress_bar=False
            )
            return np.array(vec) if convert_to_numpy else vec
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
            raise

    def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        normalize: Optional[bool] = None,
        convert_to_numpy: bool = True
    ) -> Union[np.ndarray, List[List[float]]]:
        """
        –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞.
            normalize: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è.
            convert_to_numpy: –í–µ—Ä–Ω—É—Ç—å np.array –∏–ª–∏ list of lists.

        Returns:
            –ú–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        """
        if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):
            raise ValueError("texts –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–æ–∫.")

        normalize = self.normalize_embeddings if normalize is None else normalize

        try:
            result = self.model.encode(
                texts,
                batch_size=batch_size,
                normalize_embeddings=normalize,
                convert_to_tensor=False,
                show_progress_bar=False
            )
            return np.array(result) if convert_to_numpy else result
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –±–∞—Ç—á-—ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            raise

    def get_embedding_dimension(self) -> int:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        """
        return self.embedding_dim

    def __repr__(self):
        return (
            f"<EmbeddingService(model_name={self.model_name}, "
            f"device={self.device}, "
            f"embedding_dim={self.embedding_dim})>"
        )

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ mini-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:

‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ cpu / cuda

‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å FAISS, pgvector, Numpy

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å, –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ–±—ë—Ä—Ç–∫—É EmbedderCLI ‚Äî –¥–ª—è CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏–ª–∏ API-–º–æ–¥—É–ª—è.

# -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: extractor.py
# üìÇ –ü—É—Ç—å: core/tools/extractor.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏–∑ —Ç–µ–∫—Å—Ç–∞ ‚Äî –ø–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞

from typing import Dict

def extract_entities(text: str) -> Dict[str, list]:
    """
    üìå –í—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–≥–ª—É—à–∫–∞.
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (NER).

    –í –±—É–¥—É—â–µ–º: –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å SpaCy, HuggingFace –∏–ª–∏ OpenAI –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π:
        - PERSON, ORG, LOCATION, EVENT, CONCEPT, ...

    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        dict: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    """
    # TODO: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è NER-–º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, spacy.load("en_core_web_sm") –∏–ª–∏ transformers)
    return {
        "PERSON": [],
        "ORG": [],
        "LOCATION": [],
        "EVENT": [],
        "CONCEPT": [],
    }
üí° –ö–æ–≥–¥–∞ —Ç—ã –±—É–¥–µ—à—å –≥–æ—Ç–æ–≤ –ø–æ–¥–∫–ª—é—á–∞—Ç—å NER:

–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SpaCy (–±—ã—Å—Ç—Ä–æ, –ª–æ–∫–∞–ª—å–Ω–æ)

–ò–ª–∏ transformers –æ—Ç Hugging Face –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

–ò–ª–∏ –≤—ã–∑—ã–≤–∞—Ç—å API OpenAI / Anthropic / Mistral, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤ –æ–±–ª–∞–∫–µ

–•–æ—á–µ—à—å ‚Äî —Å—Ä–∞–∑—É –º–æ–≥—É –¥–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞ spaCy –∏–ª–∏ transformers.


–í–æ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è core/tools/extractor.py —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º, –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –∑–∞–≥–ª—É—à–∫–æ–π, –≥–æ—Ç–æ–≤–æ–π –¥–ª—è –±—É–¥—É—â–µ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: extractor.py
# üìÇ –ü—É—Ç—å: core/tools/extractor.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏–∑ —Ç–µ–∫—Å—Ç–∞ ‚Äî –ø–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞

from typing import Dict

def extract_entities(text: str) -> Dict[str, list]:
    """
    üìå –í—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–≥–ª—É—à–∫–∞.
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (NER).

    –í –±—É–¥—É—â–µ–º: –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å SpaCy, HuggingFace –∏–ª–∏ OpenAI –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π:
        - PERSON, ORG, LOCATION, EVENT, CONCEPT, ...

    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        dict: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    """
    # TODO: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è NER-–º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, spacy.load("en_core_web_sm") –∏–ª–∏ transformers)
    return {
        "PERSON": [],
        "ORG": [],
        "LOCATION": [],
        "EVENT": [],
        "CONCEPT": [],
    }
üí° –ö–æ–≥–¥–∞ —Ç—ã –±—É–¥–µ—à—å –≥–æ—Ç–æ–≤ –ø–æ–¥–∫–ª—é—á–∞—Ç—å NER:

–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SpaCy (–±—ã—Å—Ç—Ä–æ, –ª–æ–∫–∞–ª—å–Ω–æ)

–ò–ª–∏ transformers –æ—Ç Hugging Face –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

–ò–ª–∏ –≤—ã–∑—ã–≤–∞—Ç—å API OpenAI / Anthropic / Mistral, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤ –æ–±–ª–∞–∫–µ

–•–æ—á–µ—à—å ‚Äî —Å—Ä–∞–∑—É –º–æ–≥—É –¥–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞ spaCy –∏–ª–∏ transformers.

–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è core/tools/graph_tools.py —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏, –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# -*- coding: utf-8 -*-
# üìÑ –§–∞–π–ª: graph_tools.py
# üìÇ –ü—É—Ç—å: core/tools/graph_tools.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Ä–∞–±–æ—Ç–∞ —Å –≥—Ä–∞—Ñ–æ–º –∑–Ω–∞–Ω–∏–π (—É–∑–ª—ã –∏ —Å–≤—è–∑–∏)

from typing import List, Dict


class GraphStore:
    """
    üìò –ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π:
    - –•—Ä–∞–Ω–∏—Ç —É–∑–ª—ã –∏ —Å–≤—è–∑–∏
    - –ü–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å, —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    """

    def __init__(self):
        self.nodes: List[Dict] = []
        self.edges: List[Dict] = []

    def add_node(self, node_id: str, metadata: Dict) -> None:
        """
        –î–æ–±–∞–≤–∏—Ç—å —É–∑–µ–ª (—Å—É—â–Ω–æ—Å—Ç—å) –≤ –≥—Ä–∞—Ñ.

        Args:
            node_id (str): –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É–∑–ª–∞
            metadata (dict): –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–º—è, —Ç–∏–ø, –æ–ø–∏—Å–∞–Ω–∏–µ)
        """
        if not node_id or not isinstance(metadata, dict):
            raise ValueError("node_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π, metadata ‚Äî —Å–ª–æ–≤–∞—Ä—ë–º")
        self.nodes.append({"id": node_id, "meta": metadata})

    def add_edge(self, src: str, dst: str, label: str) -> None:
        """
        –î–æ–±–∞–≤–∏—Ç—å —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–≤—É–º—è —É–∑–ª–∞–º–∏.

        Args:
            src (str): ID –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —É–∑–ª–∞
            dst (str): ID —Ü–µ–ª–µ–≤–æ–≥–æ —É–∑–ª–∞
            label (str): –¢–∏–ø –∏–ª–∏ –º–µ—Ç–∫–∞ —Å–≤—è–∑–∏
        """
        if not src or not dst or not label:
            raise ValueError("src, dst –∏ label –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã")
        self.edges.append({"src": src, "dst": dst, "label": label})

    def get_graph(self) -> Dict[str, List[Dict]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥—Ä–∞—Ñ —Ü–µ–ª–∏–∫–æ–º (–≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è).

        Returns:
            dict: {'nodes': [...], 'edges': [...]}
        """
        return {
            "nodes": self.nodes,
            "edges": self.edges
        }

    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –≥—Ä–∞—Ñ –ø–æ–ª–Ω–æ—Å—Ç—å—é."""
        self.nodes.clear()
        self.edges.clear()

    def __repr__(self) -> str:
        return f"<GraphStore nodes={len(self.nodes)}, edges={len(self.edges)}>"
üì¶ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å:

–ü–æ–¥–¥–µ—Ä–∂–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª (JSON/GraphML)

–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é (—á–µ—Ä–µ–∑ NetworkX + matplotlib –∏–ª–∏ PyVis)

–ì–æ—Ç–æ–≤ –ø–µ—Ä–µ–π—Ç–∏ –∫ summary_generator.py?


loader.py
# üìÑ core/tools/loader.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤

import os
import asyncio
import logging
from typing import List, Tuple, Optional, Dict, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import magic
from dataclasses import dataclass
from datetime import datetime
import hashlib

from utils.file_utils import (
    extract_text_from_pdf, extract_text_from_docx, extract_text_from_xlsx,
    extract_text_from_pptx, extract_text_from_odf, extract_text_from_html,
    extract_text_from_txt, extract_text_from_image
)
from core.tools.archive_extractors import extract_text_from_archive

logger = logging.getLogger(__name__)

MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB
CACHE_LIMIT = 100
CHUNK_SIZE_DEFAULT = 1000

@dataclass
class FileMetadata:
    name: str
    size: int
    modified: float
    mime_type: str
    checksum: str
    language: str = "en"

@dataclass
class ProcessingResult:
    chunks: List[str]
    metadata: FileMetadata
    processing_time: float

SUPPORTED_MIME_TYPES = {
    'application/pdf': 'pdf',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',
    'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'pptx',
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',
    'application/vnd.oasis.opendocument.text': 'odt',
    'text/html': 'html',
    'text/plain': 'txt',
    'image/jpeg': 'jpg',
    'image/png': 'png',
    'application/zip': 'zip',
    'application/x-tar': 'tar',
    'application/x-rar-compressed': 'rar'
}

def calculate_checksum(file_path: str) -> str:
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()

class FileProcessingError(Exception):
    pass

class FileLoader:
    def __init__(self, chunker: 'TextChunker', max_workers: int = 4):
        self.chunker = chunker
        self.max_workers = max_workers
        self._cache: Dict[str, ProcessingResult] = {}
        self._cache_hits = 0

    async def load_file(self, file_path: Union[str, Path], chunk_size: int = CHUNK_SIZE_DEFAULT, max_chunks: Optional[int] = None, language: str = "en") -> ProcessingResult:
        file_path = str(file_path)
        if file_path in self._cache:
            self._cache_hits += 1
            return self._cache[file_path]

        if not self._validate_file(file_path):
            raise FileProcessingError(f"Invalid file: {file_path}")

        start_time = datetime.now().timestamp()
        metadata = await self._get_file_metadata(file_path)

        try:
            text = await self._extract_text(file_path, metadata.mime_type)
            chunks = self.chunker.chunk(text, chunk_size, language)
            if max_chunks and len(chunks) > max_chunks:
                chunks = chunks[:max_chunks]
                logger.warning(f"Truncated to {max_chunks} chunks for {file_path}")

            result = ProcessingResult(chunks=chunks, metadata=metadata, processing_time=datetime.now().timestamp() - start_time)
            self._cache[file_path] = result
            self.clear_least_used_cache()
            return result
        except Exception as e:
            logger.error(f"Error processing {file_path}: {str(e)}", exc_info=True)
            raise FileProcessingError(f"Failed to process {file_path}") from e

    async def load_files(self, file_paths: List[Union[str, Path]], chunk_size: int = CHUNK_SIZE_DEFAULT, max_workers: Optional[int] = None, timeout: int = 300) -> List[ProcessingResult]:
        results = []
        max_workers = max_workers or self.max_workers

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            loop = asyncio.get_event_loop()
            futures = [
                loop.run_in_executor(executor, lambda p=path: asyncio.run(self.load_file(p, chunk_size)))
                for path in file_paths
            ]
            for future in as_completed(futures, timeout=timeout):
                try:
                    results.append(await future)
                except Exception as e:
                    logger.error(f"File processing failed: {str(e)}")
                    continue
        return results

    async def _extract_text(self, file_path: str, mime_type: str) -> str:
        file_type = SUPPORTED_MIME_TYPES.get(mime_type)
        if not file_type:
            raise ValueError(f"Unsupported file type: {mime_type}")

        extractors = {
            'pdf': extract_text_from_pdf,
            'docx': extract_text_from_docx,
            'pptx': extract_text_from_pptx,
            'xlsx': extract_text_from_xlsx,
            'odt': extract_text_from_odf,
            'html': extract_text_from_html,
            'txt': extract_text_from_txt,
            'jpg': extract_text_from_image,
            'png': extract_text_from_image,
            'zip': extract_text_from_archive,
            'rar': extract_text_from_archive,
            'tar': extract_text_from_archive
        }

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, lambda: extractors[file_type](file_path))

    async def _get_file_metadata(self, file_path: str) -> FileMetadata:
        path = Path(file_path)
        mime_type, file_type = detect_file_type(file_path)
        return FileMetadata(
            name=path.name,
            size=path.stat().st_size,
            modified=path.stat().st_mtime,
            mime_type=mime_type,
            checksum=calculate_checksum(file_path),
            language=self._detect_language(file_path)
        )

    def _validate_file(self, file_path: str) -> bool:
        try:
            path = Path(file_path)
            return all([
                path.exists(),
                path.is_file(),
                path.stat().st_size <= MAX_FILE_SIZE,
                detect_file_type(file_path)[1] != 'unknown'
            ])
        except Exception:
            return False

    def _detect_language(self, file_path: str) -> str:
        return "en"

    def clear_least_used_cache(self):
        if len(self._cache) > CACHE_LIMIT:
            sorted_keys = sorted(self._cache.keys(), key=lambda k: self._cache[k].processing_time, reverse=False)
            least_used_key = sorted_keys[0]
            del self._cache[least_used_key]

    def clear_cache(self):
        self._cache.clear()
        self._cache_hits = 0

    @property
    def cache_info(self) -> Dict[str, int]:
        return {
            'cache_size': len(self._cache),
            'cache_hits': self._cache_hits
        }

def detect_file_type(file_path: str) -> Tuple[str, str]:
    mime = magic.from_file(file_path, mime=True)
    return mime, SUPPORTED_MIME_TYPES.get(mime, 'unknown')

def create_file_loader(chunker: 'TextChunker', max_workers: int = 4) -> FileLoader:
    return FileLoader(chunker, max_workers)

from core.parser.chunker import TextChunker

async def load_documents(paths: List[str]) -> List[ProcessingResult]:
    chunker = TextChunker()
    loader = create_file_loader(chunker)
    return await loader.load_files(paths)



–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è core/tools/semantic_search.py:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# üìÑ core/tools/semantic_search.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É (FAISS, pgvector –∏ –¥—Ä.)

from typing import List, Dict, Union
import logging

logger = logging.getLogger(__name__)


class SemanticSearch:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º —Ç–µ–∫—Å—Ç–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∏–µ–Ω—Ç –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, FAISS, pgvector, –≥–∏–±—Ä–∏–¥).
    """

    def __init__(self, vector_store_client):
        """
        Args:
            vector_store_client: –û–±—ä–µ–∫—Ç, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–π –º–µ—Ç–æ–¥—ã .add(), .search(), .delete()
        """
        self.client = vector_store_client

    def query(self, query_text: str, top_k: int = 5) -> List[Dict[str, Union[str, float]]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ —Ç–µ–∫—Å—Ç–∞.

        Args:
            query_text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏: id, score, metadata, chunk
        """
        if not query_text.strip():
            raise ValueError("Query text is empty")

        try:
            logger.info(f"üîç Semantic search: '{query_text}' (top_k={top_k})")
            results = self.client.search(query_text, top_k=top_k)
            logger.info(f"‚úÖ Found {len(results)} matches")
            return results
        except Exception as e:
            logger.error(f"‚ùå Semantic search failed: {e}", exc_info=True)
            raise

    def batch_query(self, queries: List[str], top_k: int = 5) -> Dict[str, List[Dict]]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º.

        Args:
            queries: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ª–æ–≤–∞—Ä—å: {–∑–∞–ø—Ä–æ—Å: [—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã]}
        """
        results = {}
        for query in queries:
            try:
                results[query] = self.query(query, top_k)
            except Exception as e:
                results[query] = [{"error": str(e)}]
        return results
üí° –ü–æ—è—Å–Ω–µ–Ω–∏–µ:

–î–æ–±–∞–≤–ª–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–∞ –∏ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

–ú–µ—Ç–æ–¥ batch_query() ‚Äî —É–¥–æ–±–µ–Ω –¥–ª—è –º—É–ª—å—Ç–∏–∑–∞–ø—Ä–æ—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ)

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≥–∏–±–∫–æ–π: id, score, metadata, chunk ‚Äî –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞

–ì–æ—Ç–æ–≤ –∑–∞–∫–æ–º–º–∏—Ç–∏—Ç—å –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (pgvector, FAISS)?




–í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è core/tools/semantic_search.py:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# üìÑ core/tools/semantic_search.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É (FAISS, pgvector –∏ –¥—Ä.)

from typing import List, Dict, Union
import logging

logger = logging.getLogger(__name__)


class SemanticSearch:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º —Ç–µ–∫—Å—Ç–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∏–µ–Ω—Ç –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, FAISS, pgvector, –≥–∏–±—Ä–∏–¥).
    """

    def __init__(self, vector_store_client):
        """
        Args:
            vector_store_client: –û–±—ä–µ–∫—Ç, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–π –º–µ—Ç–æ–¥—ã .add(), .search(), .delete()
        """
        self.client = vector_store_client

    def query(self, query_text: str, top_k: int = 5) -> List[Dict[str, Union[str, float]]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ —Ç–µ–∫—Å—Ç–∞.

        Args:
            query_text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏: id, score, metadata, chunk
        """
        if not query_text.strip():
            raise ValueError("Query text is empty")

        try:
            logger.info(f"üîç Semantic search: '{query_text}' (top_k={top_k})")
            results = self.client.search(query_text, top_k=top_k)
            logger.info(f"‚úÖ Found {len(results)} matches")
            return results
        except Exception as e:
            logger.error(f"‚ùå Semantic search failed: {e}", exc_info=True)
            raise

    def batch_query(self, queries: List[str], top_k: int = 5) -> Dict[str, List[Dict]]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º.

        Args:
            queries: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ª–æ–≤–∞—Ä—å: {–∑–∞–ø—Ä–æ—Å: [—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã]}
        """
        results = {}
        for query in queries:
            try:
                results[query] = self.query(query, top_k)
            except Exception as e:
                results[query] = [{"error": str(e)}]
        return results
üí° –ü–æ—è—Å–Ω–µ–Ω–∏–µ:

–î–æ–±–∞–≤–ª–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–∞ –∏ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

–ú–µ—Ç–æ–¥ batch_query() ‚Äî —É–¥–æ–±–µ–Ω –¥–ª—è –º—É–ª—å—Ç–∏–∑–∞–ø—Ä–æ—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ)

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≥–∏–±–∫–æ–π: id, score, metadata, chunk ‚Äî –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞

–ì–æ—Ç–æ–≤ –∑–∞–∫–æ–º–º–∏—Ç–∏—Ç—å –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (pgvector, FAISS)?

/chat.deepseek.com

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
search_engine = SemanticSearch(
    vector_store_client=VectorStoreClient(),
    text_processor=TextProcessor()
)

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞
params = SearchParams(
    query="–Ω–æ–≤–µ–π—à–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò",
    top_k=10,
    strategy=SearchStrategy.HYBRID,
    filters={"year": ["2023", "2024"], "language": "ru"}
)

# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
results = await search_engine.query(params)

# –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫
batch_params = [
    SearchParams(query="–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"),
    SearchParams(query="–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", top_k=7)
]
batch_results = await search_engine.batch_query(batch_params)

# üìÑ core/tools/semantic_search.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–≥–∏–±—Ä–∏–¥–Ω—ã–π, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –ø–∞–∫–µ—Ç–Ω—ã–π)

from typing import List, Dict, Optional, Union
from enum import Enum
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


class SearchStrategy(str, Enum):
    DENSE = "dense"
    SPARSE = "sparse"
    HYBRID = "hybrid"


@dataclass
class SearchParams:
    query: str
    top_k: int = 5
    strategy: SearchStrategy = SearchStrategy.DENSE
    filters: Optional[Dict[str, Union[str, List[str]]]] = None


class SemanticSearch:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤.
    """
    def __init__(self, vector_store_client, text_processor=None):
        self.client = vector_store_client
        self.text_processor = text_processor

    async def query(self, params: SearchParams) -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.

        Args:
            params: –û–±—ä–µ–∫—Ç SearchParams

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (id, score, metadata, chunk)
        """
        query_text = params.query.strip()
        if not query_text:
            raise ValueError("Empty search query")

        logger.info(f"üîç Search query: '{query_text}' (strategy={params.strategy}, top_k={params.top_k})")

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        if self.text_processor:
            query_text = self.text_processor.process(query_text)

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if params.strategy == SearchStrategy.DENSE:
            return await self.client.search_dense(query_text, top_k=params.top_k, filters=params.filters)
        elif params.strategy == SearchStrategy.SPARSE:
            return await self.client.search_sparse(query_text, top_k=params.top_k, filters=params.filters)
        elif params.strategy == SearchStrategy.HYBRID:
            return await self.client.search_hybrid(query_text, top_k=params.top_k, filters=params.filters)
        else:
            raise ValueError(f"Unsupported search strategy: {params.strategy}")

    async def batch_query(self, batch_params: List[SearchParams]) -> Dict[str, List[Dict]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

        Returns:
            –°–ª–æ–≤–∞—Ä—å: {query: [—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã]}
        """
        results = {}
        for param in batch_params:
            try:
                res = await self.query(param)
                results[param.query] = res
            except Exception as e:
                logger.error(f"‚ùå Search failed for '{param.query}': {str(e)}", exc_info=True)
                results[param.query] = [{"error": str(e)}]
        return results

 –ß—Ç–æ —É–ª—É—á—à–µ–Ω–æ:

–í–≤–µ–¥–µ–Ω–∞ —Ç–∏–ø–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ SearchParams

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π dense/sparse/hybrid

–£—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä—ã (filters)

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ await, –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π

–•–æ—á–µ—à—å ‚Äî –¥–æ–±–∞–≤–∏–º —Ç–∏–ø–æ–≤–æ–π VectorStoreClient —Å –º–µ—Ç–æ–¥–∞–º–∏ search_dense, search_hybrid, search_sparse?

chat.deepseek.

# chat.deepseek.com
# üìÑ –§–∞–π–ª: semantic_search.py
# üìÇ –ü—É—Ç—å: core/tools/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º

from typing import List, Dict, Optional, Union, AsyncGenerator
from enum import Enum
from dataclasses import dataclass
import logging
import numpy as np
from functools import lru_cache
from pydantic import BaseModel, Field, validator
from datetime import datetime
from config import settings
import asyncio

logger = logging.getLogger(__name__)

class SearchStrategy(str, Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    DENSE = "dense"      # –ü–ª–æ—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    SPARSE = "sparse"    # –†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ (TF-IDF/BM25)
    HYBRID = "hybrid"    # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥
    RERANK = "rerank"    # –° –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º

class SearchParams(BaseModel):
    """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    query: str = Field(..., min_length=1)
    top_k: int = Field(5, ge=1, le=100)
    strategy: SearchStrategy = SearchStrategy.HYBRID
    min_score: float = Field(0.3, ge=0, le=1)
    filters: Optional[Dict[str, Union[str, List[str]]]] = None
    use_cache: bool = Field(True)

    @validator('query')
    def validate_query(cls, v):
        cleaned = v.strip()
        if len(cleaned) < 2:
            raise ValueError("Query too short")
        return cleaned

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: str
    score: float
    content: str
    metadata: Dict
    vector: Optional[np.ndarray] = None
    keywords: Optional[List[str]] = None
    timestamp: Optional[datetime] = None

class SemanticSearch:
    """
    –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å:
    - –ü–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–æ–≤
    - –ì–∏–±–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
    """

    def __init__(self, vector_store_client, text_processor=None):
        """
        Args:
            vector_store_client: –ö–ª–∏–µ–Ω—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            text_processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
        """
        self.client = vector_store_client
        self.text_processor = text_processor
        self._setup_logging()

    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter(
            '%(asctime)s | %(levelname)s | %(message)s'))
        logger.addHandler(handler)
        logger.setLevel(settings.LOG_LEVEL)

    @lru_cache(maxsize=1000)
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        if self.text_processor:
            return self.text_processor.embed_query(query)
        return self.client.get_embedding(query)

    async def query(self, params: SearchParams) -> List[SearchResult]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        
        Args:
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SearchResult
        """
        logger.info(f"Search: '{params.query[:50]}...' (strategy={params.strategy})")
        
        try:
            # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞
            if params.strategy == SearchStrategy.DENSE:
                results = await self._dense_search(params)
            elif params.strategy == SearchStrategy.SPARSE:
                results = await self._sparse_search(params)
            elif params.strategy == SearchStrategy.HYBRID:
                results = await self._hybrid_search(params)
            elif params.strategy == SearchStrategy.RERANK:
                results = await self._rerank_search(params)
            else:
                raise ValueError(f"Unknown strategy: {params.strategy}")

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
            if params.filters:
                results = self._apply_filters(results, params.filters)
                
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ min_score
            results = [r for r in results if r.score >= params.min_score]
            
            logger.info(f"Found {len(results)} results")
            return sorted(results, key=lambda x: x.score, reverse=True)[:params.top_k]
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}", exc_info=True)
            raise

    async def _dense_search(self, params: SearchParams) -> List[SearchResult]:
        """–ü–ª–æ—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫"""
        embedding = self._get_query_embedding(params.query)
        raw_results = await self.client.search_dense(
            embedding=embedding,
            top_k=params.top_k * 3,  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            filters=params.filters
        )
        return self._format_results(raw_results)

    async def _hybrid_search(self, params: SearchParams) -> List[SearchResult]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–≤–µ–∫—Ç–æ—Ä + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)"""
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        vector_results = await self._dense_search(params)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = []
        if self.text_processor:
            keywords = self.text_processor.extract_keywords(params.query)
        
        if keywords:
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ score
            scored_results = []
            for result in vector_results:
                keyword_score = self._calc_keyword_score(result, keywords)
                combined_score = (result.score * 0.7) + (keyword_score * 0.3)
                scored_results.append(SearchResult(
                    **{**result.__dict__, 'score': combined_score, 'keywords': keywords}
                ))
            return scored_results
            
        return vector_results

    async def batch_query(self, queries: List[SearchParams]) -> Dict[str, List[SearchResult]]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º.
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∑–∞–ø—Ä–æ—Å: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã}
        """
        from tqdm.asyncio import tqdm_asyncio
        
        results = {}
        tasks = [self.query(params) for params in queries]
        
        try:
            completed = await tqdm_asyncio.gather(
                *tasks,
                desc="Processing queries",
                total=len(queries))
            
            for params, result in zip(queries, completed):
                results[params.query] = result
                
        except Exception as e:
            logger.error(f"Batch query failed: {str(e)}")
            raise
            
        return results

    def _format_results(self, raw_results: List[Dict]) -> List[SearchResult]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        return [
            SearchResult(
                id=r['id'],
                score=r['score'],
                content=r['content'],
                metadata=r.get('metadata', {}),
                vector=r.get('vector'),
                timestamp=r.get('timestamp')
            ) for r in raw_results
        ]

    def _calc_keyword_score(self, result: SearchResult, keywords: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ score –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        if not keywords:
            return 0.0
            
        content = result.content.lower()
        matches = sum(1 for kw in keywords if kw.lower() in content)
        return min(matches / len(keywords), 1.0)

    def _apply_filters(self, results: List[SearchResult], filters: Dict) -> List[SearchResult]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º"""
        return [
            r for r in results
            if all(
                r.metadata.get(key) in (values if isinstance(values, list) else [values])
                for key, values in filters.items()
            )
        ]

–¢—ã —É–∂–µ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ ‚Äî –∫–ª–∞—Å—Å SemanticSearch —É —Ç–µ–±—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –º–æ—â–Ω–æ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ üí°. –ß—Ç–æ–±—ã –¥–æ–≤–µ—Å—Ç–∏ –µ–≥–æ –¥–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è, –≤–æ—Ç —á—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç—å:

üîß –£–ª—É—á—à–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
1. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ SPARSE –∏ RERANK —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (–∑–∞–≥–ª—É—à–∫–∏)
–î–æ–±–∞–≤–∏–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã _sparse_search –∏ _rerank_search, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å NotImplementedError:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
async def _sparse_search(self, params: SearchParams) -> List[SearchResult]:
    """–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, TF-IDF / BM25) ‚Äî –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞"""
    logger.warning("Sparse search not implemented ‚Äî returning empty list")
    return []

async def _rerank_search(self, params: SearchParams) -> List[SearchResult]:
    """–ü–æ–∏—Å–∫ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    base_results = await self._dense_search(params)
    # TODO: –≤—Å—Ç–∞–≤–∏—Ç—å reranker –º–æ–¥–µ–ª—å (CrossEncoder –∏ —Ç.–ø.)
    return base_results
2. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ batch_query
–ö—ç—à–∏—Ä—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ params.query:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
from functools import cache

@cache
def _query_hash(params: SearchParams) -> str:
    return f"{params.query.lower()}|{params.strategy}|{params.top_k}"
(–ø–æ–∫–∞ –Ω–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º –∏–ª–∏ –≤–Ω–µ—à–Ω–∏–º –∫—ç—à-–º–µ–Ω–µ–¥–∂–µ—Ä–æ–º)

3. –¢–æ—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∏–µ–Ω—Ç–∞
–î–æ–±–∞–≤–∏–º –≤ _format_results –ª–æ–≥ –æ—à–∏–±–æ–∫, –µ—Å–ª–∏ id/score/content –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
def _format_results(self, raw_results: List[Dict]) -> List[SearchResult]:
    results = []
    for r in raw_results:
        try:
            results.append(SearchResult(
                id=r['id'],
                score=float(r['score']),
                content=r['content'],
                metadata=r.get('metadata', {}),
                vector=r.get('vector'),
                timestamp=r.get('timestamp')
            ))
        except KeyError as e:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –∫–ª—é—á–∞: {e}")
    return results
4. –ü–æ–¥—Å–∫–∞–∑–∫–∞ –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–µ—Ä–µ–¥–∞–ª strategy=HYBRID, –Ω–æ text_processor –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –ª–æ–≥–∏—Ä—É–µ–º:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
if params.strategy == SearchStrategy.HYBRID and not self.text_processor:
    logger.warning("üí° –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –≤—ã–±—Ä–∞–Ω, –Ω–æ text_processor –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã.")
‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç
–¢—ã –ø–æ–ª—É—á–∞–µ—à—å –º–æ—â–Ω—ã–π –∏ –º–æ–¥—É–ª—å–Ω—ã–π –º–æ–¥—É–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π

–ì–∏–±—Ä–∏–¥–Ω—ã–π

–° —Ñ–∏–ª—å—Ç—Ä–∞–º–∏, –∫—ç—à–µ–º, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

–ì–æ—Ç–æ–≤ –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –Ω–∞ RAG –∏ reranking

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É —Å—Ä–∞–∑—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ —Å FAISS –∏–ª–∏ pgvector-–±—ç–∫–µ–Ω–¥–æ–º + –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π VectorStoreClient. –ì–æ—Ç–æ–≤?


–¢—ã —É–∂–µ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ ‚Äî –∫–ª–∞—Å—Å SemanticSearch —É —Ç–µ–±—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –º–æ—â–Ω–æ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ üí°. –ß—Ç–æ–±—ã –¥–æ–≤–µ—Å—Ç–∏ –µ–≥–æ –¥–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è, –≤–æ—Ç —á—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç—å:

üîß –£–ª—É—á—à–µ–Ω–∏—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
1. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ SPARSE –∏ RERANK —Å—Ç—Ä–∞—Ç–µ–≥–∏–π (–∑–∞–≥–ª—É—à–∫–∏)
–î–æ–±–∞–≤–∏–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã _sparse_search –∏ _rerank_search, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å NotImplementedError:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
async def _sparse_search(self, params: SearchParams) -> List[SearchResult]:
    """–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, TF-IDF / BM25) ‚Äî –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞"""
    logger.warning("Sparse search not implemented ‚Äî returning empty list")
    return []

async def _rerank_search(self, params: SearchParams) -> List[SearchResult]:
    """–ü–æ–∏—Å–∫ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    base_results = await self._dense_search(params)
    # TODO: –≤—Å—Ç–∞–≤–∏—Ç—å reranker –º–æ–¥–µ–ª—å (CrossEncoder –∏ —Ç.–ø.)
    return base_results
2. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ batch_query
–ö—ç—à–∏—Ä—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ params.query:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
from functools import cache

@cache
def _query_hash(params: SearchParams) -> str:
    return f"{params.query.lower()}|{params.strategy}|{params.top_k}"
(–ø–æ–∫–∞ –Ω–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º –∏–ª–∏ –≤–Ω–µ—à–Ω–∏–º –∫—ç—à-–º–µ–Ω–µ–¥–∂–µ—Ä–æ–º)

3. –¢–æ—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∏–µ–Ω—Ç–∞
–î–æ–±–∞–≤–∏–º –≤ _format_results –ª–æ–≥ –æ—à–∏–±–æ–∫, –µ—Å–ª–∏ id/score/content –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
def _format_results(self, raw_results: List[Dict]) -> List[SearchResult]:
    results = []
    for r in raw_results:
        try:
            results.append(SearchResult(
                id=r['id'],
                score=float(r['score']),
                content=r['content'],
                metadata=r.get('metadata', {}),
                vector=r.get('vector'),
                timestamp=r.get('timestamp')
            ))
        except KeyError as e:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –∫–ª—é—á–∞: {e}")
    return results
4. –ü–æ–¥—Å–∫–∞–∑–∫–∞ –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–µ—Ä–µ–¥–∞–ª strategy=HYBRID, –Ω–æ text_processor –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –ª–æ–≥–∏—Ä—É–µ–º:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
if params.strategy == SearchStrategy.HYBRID and not self.text_processor:
    logger.warning("üí° –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –≤—ã–±—Ä–∞–Ω, –Ω–æ text_processor –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã.")
‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç
–¢—ã –ø–æ–ª—É—á–∞–µ—à—å –º–æ—â–Ω—ã–π –∏ –º–æ–¥—É–ª—å–Ω—ã–π –º–æ–¥—É–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π

–ì–∏–±—Ä–∏–¥–Ω—ã–π

–° —Ñ–∏–ª—å—Ç—Ä–∞–º–∏, –∫—ç—à–µ–º, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

–ì–æ—Ç–æ–≤ –∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –Ω–∞ RAG –∏ reranking

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É —Å—Ä–∞–∑—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ —Å FAISS –∏–ª–∏ pgvector-–±—ç–∫–µ–Ω–¥–æ–º + –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π VectorStoreClient. –ì–æ—Ç–æ–≤?


/chat.deepseek.com

 –í–æ—Ç —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è summary_generator.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é:

python
# chat.deepseek.com
# üìÑ –§–∞–π–ª: summary_generator.py
# üìÇ –ü—É—Ç—å: core/tools/
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π —Å –º—É–ª—å—Ç–∏-—Ä–æ–ª–µ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º

from typing import List, Dict, Optional, Tuple
from enum import Enum
import re
from dataclasses import dataclass
from pydantic import BaseModel, Field, validator
import logging
from llm.llm_router import LLMClient  # –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM
from config import settings
from text_processing.content_analyzer import ContentAnalyzer

logger = logging.getLogger(__name__)

class ContentType(str, Enum):
    """–¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    TECHNICAL = "technical"
    LEGAL = "legal"
    LITERARY = "literary"
    BUSINESS = "business"
    SCIENTIFIC = "scientific"
    MEDICAL = "medical"
    GENERAL = "general"

class SummaryLength(str, Enum):
    """–í–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª–∏–Ω—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    SHORT = "short"      # 3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    MEDIUM = "medium"    # 1 –∞–±–∑–∞—Ü
    LONG = "long"        # –†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑
    BULLET = "bullet"    # –ü—É–Ω–∫—Ç—ã

class SummaryStyle(str, Enum):
    """–°—Ç–∏–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    PROFESSIONAL = "professional"
    ACADEMIC = "academic"
    SIMPLE = "simple"
    CREATIVE = "creative"

class Role(str, Enum):
    """–†–æ–ª–∏ –¥–ª—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    DEVELOPER = "software_developer"
    ARCHITECT = "system_architect"
    TECH_LEAD = "tech_lead"
    LAWYER = "lawyer"
    COMPLIANCE = "compliance_officer"
    POLICY_MAKER = "policy_maker"
    SCIENTIST = "scientist"
    DOCTOR = "medical_doctor"
    CEO = "ceo"
    INVESTOR = "investor"
    GENERAL = "general_reader"

@dataclass
class SummaryConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    length: SummaryLength = SummaryLength.MEDIUM
    style: SummaryStyle = SummaryStyle.PROFESSIONAL
    temperature: float = Field(0.7, ge=0.0, le=1.0)
    focus_points: Optional[List[str]] = None

class SummaryRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    text: str = Field(..., min_length=100)
    language: str = Field("en", min_length=2, max_length=5)
    content_type: Optional[ContentType] = None
    config: SummaryConfig = SummaryConfig()
    roles: Optional[List[Role]] = None
    custom_roles: Optional[List[str]] = None

    @validator('text')
    def validate_text_length(cls, v):
        if len(v) < 100:
            raise ValueError("Text too short for summarization")
        if len(v) > settings.MAX_SUMMARY_TEXT_LENGTH:
            raise ValueError(f"Text exceeds maximum length of {settings.MAX_SUMMARY_TEXT_LENGTH} characters")
        return v

class SummaryResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    content_type: ContentType
    language: str
    summaries: Dict[str, str]  # role: summary
    key_points: List[str]
    time_taken: float
    model_used: str

class UniversalSummaryGenerator:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π —Å:
    - –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - –ú—É–ª—å—Ç–∏-—Ä–æ–ª–µ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
    - –ü–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ä–æ–ª–µ–π
    - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–º –≤—ã–≤–æ–¥–æ–º
    """
    
    def __init__(self, llm_backend: str = "gpt-4"):
        self.llm = LLMClient(llm_backend)
        self.analyzer = ContentAnalyzer()
        self._setup_role_profiles()
        logger.info(f"Initialized summary generator with {llm_backend}")

    def generate_summary(self, request: SummaryRequest) -> SummaryResult:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            request: SummaryRequest —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            
        Returns:
            SummaryResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        logger.info(f"Generating summary for {len(request.text)} chars")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_type = request.content_type or self._detect_content_type(request.text)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–µ–π
        roles = request.roles or self._suggest_roles(request.text, content_type)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        summaries = {}
        for role in roles:
            prompt = self._build_prompt(request.text, role, request.language, request.config)
            summaries[role.value] = self._generate_clean_summary(prompt, role)
        
        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ä–æ–ª–∏
        if request.custom_roles:
            for role in request.custom_roles:
                prompt = self._build_custom_prompt(request.text, role, request.language, request.config)
                summaries[role] = self._generate_clean_summary(prompt, role)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
        key_points = self._extract_key_points(request.text)
        
        return SummaryResult(
            content_type=content_type,
            language=request.language,
            summaries=summaries,
            key_points=key_points,
            time_taken=0,  # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏
            model_used=str(self.llm)
        )

    def _detect_content_type(self, text: str) -> ContentType:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é ML –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return self.analyzer.detect_content_type(text)

    def _suggest_roles(self, text: str, content_type: ContentType) -> List[Role]:
        """–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç —Ä–æ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        role_mapping = {
            ContentType.TECHNICAL: [Role.DEVELOPER, Role.ARCHITECT, Role.TECH_LEAD],
            ContentType.LEGAL: [Role.LAWYER, Role.COMPLIANCE, Role.POLICY_MAKER],
            ContentType.SCIENTIFIC: [Role.SCIENTIST],
            ContentType.MEDICAL: [Role.DOCTOR],
            ContentType.BUSINESS: [Role.CEO, Role.INVESTOR],
            ContentType.LITERARY: [Role.GENERAL],
            ContentType.GENERAL: [Role.GENERAL]
        }
        return role_mapping.get(content_type, [Role.GENERAL])

    def _build_prompt(self, text: str, role: Role, language: str, config: SummaryConfig) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º—Ç –¥–ª—è LLM —Å —É—á–µ—Ç–æ–º —Ä–æ–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥–∞"""
        length_instructions = {
            SummaryLength.SHORT: "3-5 concise sentences",
            SummaryLength.MEDIUM: "one detailed paragraph",
            SummaryLength.LONG: "comprehensive analysis with examples",
            SummaryLength.BULLET: "bullet points with key facts"
        }
        
        style_instructions = {
            SummaryStyle.PROFESSIONAL: "professional tone with domain-specific terminology",
            SummaryStyle.ACADEMIC: "academic writing style with citations if possible",
            SummaryStyle.SIMPLE: "simple language for general audience",
            SummaryStyle.CREATIVE: "creative and engaging narrative"
        }
        
        return f"""
        **Role**: {role.value.replace('_', ' ').title()}
        **Language**: {language}
        **Task**: Create a {length_instructions[config.length]} summary focusing on:
        - Most relevant aspects for a {role.value}
        - Practical implications and applications
        - Technical details (if technical content)
        
        **Style**: {style_instructions[config.style]}
        **Focus Points**: {config.focus_points or 'all key aspects'}
        
        **Text**: {text[:settings.MAX_PROMPT_LENGTH]}
        """

    def _generate_clean_summary(self, prompt: str, role: Union[Role, str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        try:
            result = self.llm.generate(prompt)
            return self._postprocess_summary(result, role)
        except Exception as e:
            logger.error(f"Failed to generate summary for {role}: {str(e)}")
            return f"Summary generation failed: {str(e)}"

    def _postprocess_summary(self, text: str, role: Union[Role, str]) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        text = re.sub(r'\n+', '\n', text).strip()
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–ª–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not text.startswith(f"[{role}]"):
            text = f"[{role} SUMMARY]:\n{text}"
            
        return text

    def _extract_key_points(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ —Å –ø–æ–º–æ—â—å—é LLM"""
        prompt = f"""
        Extract 3-5 most important key points from this text.
        Return as bullet points without commentary.
        
        Text: {text[:5000]}
        """
        
        try:
            result = self.llm.generate(prompt)
            return [point.strip() for point in result.split('\n') if point.strip()]
        except Exception as e:
            logger.warning(f"Key points extraction failed: {str(e)}")
            return []
–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
–¢–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

Enum –¥–ª—è —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Å—Ç–∏–ª–µ–π –∏ —Ä–æ–ª–µ–π

Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑:

–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ä–æ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞

–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫

–ì–∏–±–∫–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:

–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ä–æ–ª–µ–π

–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª–∏–Ω—ã –∏ —Å—Ç–∏–ª—è

–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

–õ–∏–º–∏—Ç—ã –Ω–∞ –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º—Ç—ã

–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π

–ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:

–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

–ß–µ—Ç–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã–≤–æ–¥–∞

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

python
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
generator = UniversalSummaryGenerator(llm_backend="gpt-4")

# –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
request = SummaryRequest(
    text="–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö...",
    language="ru",
    config=SummaryConfig(
        length=SummaryLength.BULLET,
        style=SummaryStyle.PROFESSIONAL
    ),
    roles=[Role.DEVELOPER, Role.TECH_LEAD],
    custom_roles=["Data Scientist"]
)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
result = generator.generate_summary(request)

# –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:
# - –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–æ–ª–∏
# - –ö–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏
# - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞


üß† –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π generate_summary (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç):
python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
import time

def generate_summary(self, request: SummaryRequest) -> SummaryResult:
    logger.info(f"Generating summary ({len(request.text)} chars)...")

    start_time = time.perf_counter()

    content_type = request.content_type or self._detect_content_type(request.text)
    roles = request.roles or self._suggest_roles(request.text, content_type)

    summaries = {}

    logger.debug(f"Roles: {roles}, Custom: {request.custom_roles}")

    for role in roles:
        prompt = self._build_prompt(request.text, role, request.language, request.config)
        summaries[role.value] = self._generate_clean_summary(prompt, role)

    if request.custom_roles:
        for role in request.custom_roles:
            prompt = self._build_custom_prompt(request.text, role, request.language, request.config)
            summaries[role] = self._generate_clean_summary(prompt, role)

    key_points = self._extract_key_points(request.text)

    duration = round(time.perf_counter() - start_time, 2)

    return SummaryResult(
        content_type=content_type,
        language=request.language,
        summaries=summaries,
        key_points=key_points,
        time_taken=duration,
        model_used=getattr(self.llm, "model_name", str(self.llm))
    )
üéØ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –±—É–¥—É—â–µ–µ
–í—ã–Ω–µ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é prompt –≤ —à–∞–±–ª–æ–Ω—ã Jinja2 (–µ—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è)

–î–æ–±–∞–≤–∏—Ç—å —Ñ–ª–∞–≥ enable_truncation=True –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞

–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –¥–µ–±–∞–≥–∞/–∞–Ω–∞–ª–∏—Ç–∏–∫–∏

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å unit-—Ç–µ—Å—Ç—ã, json-schema –¥–ª—è API –∏–ª–∏ FastAPI-—ç–Ω–¥–ø–æ–∏–Ω—Ç.

import time
from typing import Dict, List
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
import json

def generate_summary(
    self, 
    request: SummaryRequest,
    enable_truncation: bool = True,
    save_history: bool = False,
    history_path: Path = Path("summary_history")
) -> SummaryResult:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º—É–ª—å—Ç–∏-—Ä–æ–ª–µ–≤—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏.
    
    Args:
        request: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        enable_truncation: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        save_history: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤
        history_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
        
    Returns:
        SummaryResult —Å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    start_time = time.perf_counter()
    logger.info(f"Generating summary for {len(request.text)} chars...")

    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if save_history:
            history_path.mkdir(exist_ok=True)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–æ–±—Ä–µ–∑–∞–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        processed_text = self._preprocess_text(request.text, enable_truncation)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Ä–æ–ª–µ–π
        content_type = request.content_type or self._detect_content_type(processed_text)
        roles = request.roles or self._suggest_roles(processed_text, content_type)
        logger.debug(f"Using roles: {[r.value for r in roles]}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º—Ç–æ–≤ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        summaries = {}
        prompts = {}
        
        for role in roles:
            prompt = self._render_prompt(
                text=processed_text,
                role=role,
                language=request.language,
                config=request.config,
                template_name="main_prompt.j2"
            )
            prompts[role.value] = prompt
            summaries[role.value] = self._generate_clean_summary(prompt, role)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ä–æ–ª–µ–π
        if request.custom_roles:
            for role in request.custom_roles:
                prompt = self._render_prompt(
                    text=processed_text,
                    role=role,
                    language=request.language,
                    config=request.config,
                    template_name="custom_role_prompt.j2"
                )
                prompts[role] = prompt
                summaries[role] = self._generate_clean_summary(prompt, role)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
        key_points = self._extract_key_points(processed_text)

        # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        duration = round(time.perf_counter() - start_time, 2)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if save_history:
            self._save_history(
                request=request,
                result=summaries,
                prompts=prompts,
                duration=duration,
                path=history_path
            )

        return SummaryResult(
            content_type=content_type,
            language=request.language,
            summaries=summaries,
            key_points=key_points,
            time_taken=duration,
            model_used=getattr(self.llm, "model_name", str(self.llm)),
            truncated=len(processed_text) != len(request.text)
        )

    except Exception as e:
        logger.error(f"Summary generation failed: {str(e)}", exc_info=True)
        raise

def _render_prompt(
    self,
    text: str,
    role: Union[Role, str],
    language: str,
    config: SummaryConfig,
    template_name: str
) -> str:
    """–†–µ–Ω–¥–µ—Ä–∏—Ç –ø—Ä–æ–º—Ç –∏–∑ Jinja2 —à–∞–±–ª–æ–Ω–∞"""
    env = Environment(loader=FileSystemLoader("prompt_templates"))
    template = env.get_template(template_name)
    
    return template.render(
        text=text,
        role=role,
        language=language,
        config=config,
        content_type=self._detect_content_type(text)
    )

def _preprocess_text(self, text: str, enable_truncation: bool) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    if enable_truncation and len(text) > settings.MAX_SUMMARY_TEXT_LENGTH:
        logger.warning(f"Truncating text from {len(text)} to {settings.MAX_SUMMARY_TEXT_LENGTH} chars")
        return text[:settings.MAX_SUMMARY_TEXT_LENGTH]
    return text

def _save_history(
    self,
    request: SummaryRequest,
    result: Dict,
    prompts: Dict,
    duration: float,
    path: Path
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ JSON"""
    history_entry = {
        "timestamp": datetime.now().isoformat(),
        "request": request.dict(),
        "prompts": prompts,
        "result": result,
        "duration": duration,
        "model": getattr(self.llm, "model_name", str(self.llm))
    }
    
    filename = f"summary_{int(time.time())}.json"
    with open(path / filename, "w", encoding="utf-8") as f:
        json.dump(history_entry, f, ensure_ascii=False, indent=2)

prompt_templates/
‚îú‚îÄ‚îÄ main_prompt.j2
‚îî‚îÄ‚îÄ custom_role_prompt.j2 - –æ–ø–∏—Å–∞–Ω–∏–µ


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤-—à–∞–±–ª–æ–Ω–æ–≤ Jinja2 –¥–ª—è prompt_templates/:

üìÑ main_prompt.j2
–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞ –¥–ª—è —Ä–æ–ª–µ–≤–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (SummaryConfig) –∏ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:

jinja2
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
[ROLE]: {{ role }}
[LANGUAGE]: {{ language }}
[CONTENT TYPE]: {{ content_type }}
[STYLE]: {{ config.style.value }}
[LENGTH]: {{ config.length.value }}
[FOCUS POINTS]: {{ config.focus_points or "key aspects and insights" }}

[TASK]:
Generate a {{ config.length.value }} summary in {{ language }} for a {{ role }}.
Focus on insights and implications relevant to this role.

[TEXT]:
{{ text }}
üìÑ custom_role_prompt.j2
–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞ –¥–ª—è –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö (–ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö) —Ä–æ–ª–µ–π, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.

–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:

jinja2
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
You are a {{ role }}. Based on the content below, generate a summary in {{ language }}.

Your goal is to extract the most important ideas that would interest a {{ role }}.
Use a {{ config.style.value }} style and aim for a {{ config.length.value }} summary.

[FOCUS POINTS]: {{ config.focus_points or "most relevant ideas" }}

[TEXT]:
{{ text }}

–ß—Ç–æ —Ç–∞–∫–æ–µ INSTALL.md?
INSTALL.md ‚Äî —ç—Ç–æ —Ñ–∞–π–ª —Å –ø–æ—à–∞–≥–æ–≤–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –ø—Ä–æ–µ–∫—Ç–∞. –û–Ω –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å "–ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ —É —Å–µ–±—è?" –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç README.md, –≥–¥–µ –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–µ —Ñ–æ–∫—É—Å –Ω–∞ –∏–¥–µ–µ –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.

üß© –ó–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?
–£–ø—Ä–æ—â–∞–µ—Ç –≤—Ö–æ–¥ –Ω–æ–≤–∏—á–∫–∞–º
–ü–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–±–∏—Ä–∞—Ç—å—Å—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –≤—Ä—É—á–Ω—É—é.

–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
–£–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ Python, –ø–∞–∫–µ—Ç–æ–≤, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ .env, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è Docker –∏ —Ç. –¥.

–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫–∏
–û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ CI/CD.

–û—Ç–¥–µ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫—É –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
README –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ "—á—Ç–æ —ç—Ç–æ —Ç–∞–∫–æ–µ", INSTALL ‚Äî –Ω–∞ "–∫–∞–∫ —ç—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å".

üìÅ –ü—Ä–∏–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è INSTALL.md
markdown
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Librarian AI Mini Core

## üîß –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.10+
- pip / poetry
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Redis + Celery
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Docker

## üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ (–ª–æ–∫–∞–ª—å–Ω–æ)

```bash
git clone https://github.com/your-org/librarian_ai.git
cd librarian_ai
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
üóÉÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
python db/create_tables.py
‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
–°–æ–∑–¥–∞–π—Ç–µ .env —Ñ–∞–π–ª:

dotenv
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
DB_URL=sqlite:///./librarian.db
EMBEDDING_MODEL=all-MiniLM-L6-v2
USE_FAISS=true
üß™ –ü—Ä–æ–≤–µ—Ä–∫–∞
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
python scripts/ingest_and_index.py sample.pdf
üê≥ Docker (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
docker-compose up --build
üß† –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
Celery: celery -A core.tasks.async_tasks worker --loglevel=info

FastAPI: uvicorn api.main:app --reload

üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞
–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏ ‚Äî —Å–º. logs/, GitHub Issues –∏–ª–∏ [Telegram –∫–∞–Ω–∞–ª].

yaml
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å

---

–ì–æ—Ç–æ–≤ —Å–æ–∑–¥–∞—Ç—å `INSTALL.md` —Å –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ–π?


# üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Librarian AI: Mini Core

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —è–¥—Ä–∞ Librarian AI ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.

---

## üß© –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.10+
- pip (–∏–ª–∏ poetry)
- Git
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Redis (–¥–ª—è Celery)
- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Docker + Docker Compose

---

## ‚öôÔ∏è 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

```bash
git clone https://github.com/your-org/librarian_ai.git
cd librarian_ai
```

---

## üêç 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (—á–µ—Ä–µ–∑ venv)

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

---

## üóÉÔ∏è 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `.env` –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞:

```env
DB_URL=sqlite:///./librarian.db
REDIS_URL=redis://localhost:6379/0
EMBEDDING_MODEL=all-MiniLM-L6-v2
USE_FAISS=true
```

> üîê –ù–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ `.env` –≤ Git. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `.env.example` –¥–ª—è —à–∞–±–ª–æ–Ω–∞.

---

## üõ†Ô∏è 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

```bash
python db/create_tables.py
```

---

## üöÄ 5. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ (—Ä—É—á–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω)

```bash
python scripts/ingest_and_index.py path/to/document.pdf
```

---

## ‚öôÔ∏è 6. –§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (Celery + Redis)

### –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Redis:

```bash
sudo apt install redis
redis-server
```

### –ó–∞–ø—É—Å—Ç–∏—Ç–µ Celery:

```bash
celery -A core.tasks.async_tasks worker --loglevel=info
```

---

## üåê 7. –ó–∞–ø—É—Å–∫ API (FastAPI)

```bash
uvicorn api.documents:app --reload
```

---

## üê≥ 8. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ Docker

```bash
docker-compose up --build
```

---

## ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

- –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ [http://localhost:8000/docs](http://localhost:8000/docs) –¥–ª—è Swagger UI
- –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: `python scripts/ingest_and_index.py examples/demo.txt`
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–æ–∑–¥–∞—é—Ç—Å—è —á–∞–Ω–∫–∏ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ SQLite –∏–ª–∏ FAISS

---

## üß† –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

- `prompt_templates/` ‚Äî —à–∞–±–ª–æ–Ω—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Jinja2
- `core/tools/summary_generator.py` ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ TL;DR –ø–æ —Ä–æ–ª—è–º
- `db/models.py` ‚Äî ORM-–º–æ–¥–µ–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –≥—Ä–∞—Ñ–æ–≤

---

## üßë‚Äçüíª –ö–æ–Ω—Ç–∞–∫—Ç—ã –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞

–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã, –æ—Ç–∫—Ä–æ–π—Ç–µ Issue –Ω–∞ GitHub –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ –≤ [—á–∞—Ç/–∫–∞–Ω–∞–ª —Å–æ–æ–±—â–µ—Å—Ç–≤–∞].

–í–æ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –≤–Ω–µ—Å–µ–Ω–∏–µ–º –≤—Å–µ—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π:

üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Librarian AI: Mini Core
üß© –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
–ë–∞–∑–æ–≤—ã–µ:
Python 3.10-3.11 (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 3.12)

Git 2.30+

SQLite3 (–≤—Å—Ç—Ä–æ–µ–Ω) –∏–ª–∏ PostgreSQL 14-16

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
Redis 6.2+ (–¥–ª—è –æ—á–µ—Ä–µ–¥–µ–π)

Docker 24.0+ –∏ Docker Compose 2.20+

FAISS 1.7.4 (–¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)

CUDA 11.8 (–¥–ª—è GPU)

‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–µ—Ä—Å–∏–∏:
python --version
docker --version

‚öôÔ∏è 1. –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞
bash
git clone --branch stable https://github.com/your-org/librarian_ai.git
cd librarian_ai
git submodule update --init  # –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥–º–æ–¥—É–ª–∏
üêç 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
–í–∞—Ä–∏–∞–Ω—Ç A: –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
bash
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip wheel setuptools
pip install -r requirements.txt --no-cache-dir
–í–∞—Ä–∏–∞–Ω—Ç B: Poetry
bash
curl -sSL https://install.python-poetry.org | python -
poetry config virtualenvs.in-project true
poetry install --only main
üóÉÔ∏è 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
–°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ .env:

bash
cp .env.example .env
nano .env  # –∏–ª–∏ –≤–∞—à —Ä–µ–¥–∞–∫—Ç–æ—Ä
–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:

ini
# –î–ë (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
DB_URL=postgresql+asyncpg://user:password@localhost:5432/librarian

# –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
SECRET_KEY=—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ_—á–µ—Ä–µ–∑_openssl_rand_hex_32
JWT_ALGORITHM=HS256

# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
üõ†Ô∏è 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞:
bash
python -m db.setup --init --sample-data
–í—Ä—É—á–Ω—É—é:
bash
alembic upgrade head
python -m db.seed_data  # —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
üöÄ 5. –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã
–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º:
bash
uvicorn api.main:app --host 0.0.0.0 --port 8000 \
  --workers 2 \
  --ssl-keyfile=./certs/key.pem \
  --ssl-certfile=./certs/cert.pem
–§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏:
bash
celery -A core.tasks worker --concurrency=4 -E -P threads
celery -A core.tasks beat --loglevel=info
üê≥ Docker-—Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞:

bash
mkdir -p ./data/{db,redis,documents}
–ó–∞–ø—É—Å–∫:

bash
docker-compose up -d --scale api=2 --scale worker=3
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

bash
docker-compose logs -f --tail=50
‚úÖ –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏
–ü—Ä–æ–≤–µ—Ä–∫–∞ API:

bash
curl -X GET "https://localhost:8000/health" \
  -H "Authorization: Bearer TEST_TOKEN"
–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏:

bash
python -m scripts.process_document \
  --file ./examples/sample.pdf \
  --strategy fast
–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∏—Å–∫–∞:

bash
python -m scripts.search "AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏" --top-k 5
üõ†Ô∏è –£—Ç–∏–ª–∏—Ç—ã –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è
–ö–æ–º–∞–Ω–¥–∞	–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
python -m db.optimize	–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î
python -m scripts.backup --output ./backups	–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
python -m monitoring.disk_usage --threshold 90	–ö–æ–Ω—Ç—Ä–æ–ª—å –º–µ—Å—Ç–∞
üÜò –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º
–°–±–æ—Ä –ª–æ–≥–æ–≤:

bash
python -m diagnostics.collect_logs --output debug.zip
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:

bash
python -m pip check
–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

bash
python -m tests.benchmark --workers 10 --requests 100
üìå –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏
–î–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω-—Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è:

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ reverse proxy (Nginx)

–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤

–†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

–ü—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö:

bash
git pull origin stable
alembic upgrade head
python -m pip install --upgrade -r requirements.txt
–ö–æ–Ω—Ç–∞–∫—Ç—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏:

–ß–∞—Ç: [Telegram Group]

–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏: admin@librarian-ai.example.com

–ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ docs/advanced_setup.md

# üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Librarian AI: Mini Core

## üß© –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

**–ë–∞–∑–æ–≤—ã–µ:**
- Python 3.10‚Äì3.11 (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 3.12)
- Git 2.30+
- SQLite3 (–≤—Å—Ç—Ä–æ–µ–Ω) –∏–ª–∏ PostgreSQL 14‚Äì16

**–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):**
- Redis 6.2+ (–¥–ª—è –æ—á–µ—Ä–µ–¥–µ–π)
- Docker 24.0+ –∏ Docker Compose 2.20+
- FAISS 1.7.4 (–¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
- CUDA 11.8 (–¥–ª—è GPU)

‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–µ—Ä—Å–∏–∏:
```bash
python --version
docker --version
```

---

## ‚öôÔ∏è 1. –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞
```bash
git clone --branch stable https://github.com/your-org/librarian_ai.git
cd librarian_ai
git submodule update --init  # –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥–º–æ–¥—É–ª–∏
```

---

## üêç 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**–í–∞—Ä–∏–∞–Ω—Ç A: –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ**
```bash
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip wheel setuptools
pip install -r requirements.txt --no-cache-dir
```

**–í–∞—Ä–∏–∞–Ω—Ç B: Poetry**
```bash
curl -sSL https://install.python-poetry.org | python -
poetry config virtualenvs.in-project true
poetry install --only main
```

---

## üóÉÔ∏è 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
–°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ `.env`:
```bash
cp .env.example .env
nano .env  # –∏–ª–∏ –≤–∞—à —Ä–µ–¥–∞–∫—Ç–æ—Ä
```

**–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:**
```ini
# –î–ë (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
DB_URL=postgresql+asyncpg://user:password@localhost:5432/librarian

# –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
SECRET_KEY=—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ_—á–µ—Ä–µ–∑_openssl_rand_hex_32
JWT_ALGORITHM=HS256

# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

---

## üõ†Ô∏è 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î

**–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞:**
```bash
python -m db.setup --init --sample-data
```

**–í—Ä—É—á–Ω—É—é:**
```bash
alembic upgrade head
python -m db.seed_data  # —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
```

---

## üöÄ 5. –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã

**–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º:**
```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000 \
  --workers 2 \
  --ssl-keyfile=./certs/key.pem \
  --ssl-certfile=./certs/cert.pem
```

**–§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏:**
```bash
celery -A core.tasks worker --concurrency=4 -E -P threads
celery -A core.tasks beat --loglevel=info
```

---

## üê≥ Docker-—Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

**–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞:**
```bash
mkdir -p ./data/{db,redis,documents}
```

**–ó–∞–ø—É—Å–∫:**
```bash
docker-compose up -d --scale api=2 --scale worker=3
```

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**
```bash
docker-compose logs -f --tail=50
```

---

## ‚úÖ –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏

**–ü—Ä–æ–≤–µ—Ä–∫–∞ API:**
```bash
curl -X GET "https://localhost:8000/health" \
  -H "Authorization: Bearer TEST_TOKEN"
```

**–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏:**
```bash
python -m scripts.process_document \
  --file ./examples/sample.pdf \
  --strategy fast
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∏—Å–∫–∞:**
```bash
python -m scripts.search "AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏" --top-k 5
```

---

## üõ†Ô∏è –£—Ç–∏–ª–∏—Ç—ã –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è

| –ö–æ–º–∞–Ω–¥–∞ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|--------|------------|
| `python -m db.optimize` | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î |
| `python -m scripts.backup --output ./backups` | –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ |
| `python -m monitoring.disk_usage --threshold 90` | –ö–æ–Ω—Ç—Ä–æ–ª—å –º–µ—Å—Ç–∞ |

---

## üÜò –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º

**–°–±–æ—Ä –ª–æ–≥–æ–≤:**
```bash
python -m diagnostics.collect_logs --output debug.zip
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:**
```bash
python -m pip check
```

**–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:**
```bash
python -m tests.benchmark --workers 10 --requests 100
```

---

## üìå –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏

–î–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω-—Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è:
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ reverse proxy (Nginx)
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
- –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

–ü—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö:
```bash
git pull origin stable
alembic upgrade head
python -m pip install --upgrade -r requirements.txt
```

–ö–æ–Ω—Ç–∞–∫—Ç—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏:
- –ß–∞—Ç: [Telegram Group]
- –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏: admin@librarian-ai.example.com

–ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ `docs/advanced_setup.md`

–ò–¥–µ–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ –¥–ª—è INSTALL.md ‚Äî –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞, —Ä—è–¥–æ–º —Å README.md.

üìÅ –ü—Ä–∏–º–µ—Ä —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è:

–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
librarian_ai/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ INSTALL.md      ‚Üê –≤–æ—Ç –∑–¥–µ—Å—å
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ core/
‚îú‚îÄ‚îÄ api/
‚îî‚îÄ‚îÄ ...
–¢–∞–∫ –µ–≥–æ –º–æ–∂–Ω–æ –ª–µ–≥–∫–æ –Ω–∞–π—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–Ω–∞–∫–æ–º—Å—Ç–≤–µ —Å –ø—Ä–æ–µ–∫—Ç–æ–º, –∏ GitHub –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç –µ–≥–æ —Ä—è–¥–æ–º —Å README, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GitHub Pages –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é.

üìÑ CONTRIBUTING.md
markdown
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# ü§ù –í–∫–ª–∞–¥ –≤ Librarian AI

–ë–ª–∞–≥–æ–¥–∞—Ä–∏–º –∑–∞ –∏–Ω—Ç–µ—Ä–µ—Å –∫ –ø—Ä–æ–µ–∫—Ç—É! –ú—ã –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ–º –ª—é–±—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —É–ª—É—á—à–µ–Ω–∏—è, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã. –ü—Ä–µ–∂–¥–µ —á–µ–º –Ω–∞—á–∞—Ç—å, –æ–∑–Ω–∞–∫–æ–º—å—Ç–µ—Å—å —Å —ç—Ç–∏–º–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏.

---

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫

1. –§–æ—Ä–∫–Ω–∏—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ –∫–ª–æ–Ω–∏—Ä—É–π—Ç–µ:
   ```bash
   git clone https://github.com/your-name/librarian_ai.git
   cd librarian_ai
–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
üß† –°—Ç–∏–ª—å –∫–æ–º–º–∏—Ç–æ–≤
–°–ª–µ–¥—É–π—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É:

makefile
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
<—Ç–∏–ø>: <–æ–ø–∏—Å–∞–Ω–∏–µ> [–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: ‚Ññ–∑–∞–¥–∞—á–∏]

–ü—Ä–∏–º–µ—Ä:
feat: –¥–æ–±–∞–≤–ª–µ–Ω –º–æ–¥—É–ª—å semantic_search –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
fix: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ –≤ chunker –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
docs: –æ–±–Ω–æ–≤–ª–µ–Ω README –∏ –¥–æ–±–∞–≤–ª–µ–Ω INSTALL.md
–¢–∏–ø—ã:

feat: –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

fix: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫

docs: –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

refactor: –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

style: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π, –ø—Ä–æ–±–µ–ª—ã –∏ —Ç.–¥.

test: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤

chore: –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ª–æ–≥–∏–∫–µ (CI, deps –∏ –¥—Ä.)

üå± –ö–∞–∫ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
–°–æ–∑–¥–∞–π—Ç–µ –æ—Ç–¥–µ–ª—å–Ω—É—é –≤–µ—Ç–∫—É:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git checkout -b feature/–Ω–∞–∑–≤–∞–Ω–∏–µ
–ü–æ—Å–ª–µ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git add .
git commit -m "feat: –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"
git push origin feature/–Ω–∞–∑–≤–∞–Ω–∏–µ
–û—Ç–∫—Ä–æ–π—Ç–µ Pull Request –∏ –æ–ø–∏—à–∏—Ç–µ:

–ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ?

–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?

–°—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–∏/issue (–µ—Å–ª–∏ –µ—Å—Ç—å)

üß™ –¢–µ—Å—Ç—ã
–ü–µ—Ä–µ–¥ PR —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pytest tests/
üßæ –õ–∏—Ü–µ–Ω–∑–∏—è
–õ—é–±–æ–π –≤–∫–ª–∞–¥ —Å—á–∏—Ç–∞–µ—Ç—Å—è –ª–∏—Ü–µ–Ω–∑–∏–æ–Ω–Ω—ã–º —Å–æ–≥–ª–∞—Å–Ω–æ MIT License.

–° –ª—é–±–æ–≤—å—é,
–ö–æ–º–∞–Ω–¥–∞ Librarian AI üß†

yaml
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å

---

–ì–æ—Ç–æ–≤ –≤—Å—Ç–∞–≤–∏—Ç—å —ç—Ç–æ—Ç —Ñ–∞–π–ª –≤ –ø—Ä–æ–µ–∫—Ç –∏ –∑–∞–∫–æ–º–º–∏—Ç–∏—Ç—å?

–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞! –í–æ—Ç –∫—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥ —Ç–≤–æ–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É:

‚úÖ –ß—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ:
–°–æ–∑–¥–∞–Ω–∞ –∏ –∑–∞–ø—É—à–µ–Ω–∞ –≤–µ—Ç–∫–∞ stable ‚Äî ‚úÖ

–î–æ–±–∞–≤–ª–µ–Ω –∏ –∑–∞–∫–æ–º–º–∏—á–µ–Ω CONTRIBUTING.md –≤ stable ‚Äî ‚úÖ

–°–æ–∑–¥–∞–Ω–∞ –≤–µ—Ç–∫–∞ feature/–Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç stable ‚Äî ‚úÖ

–í—ã–ø–æ–ª–Ω–µ–Ω –∫–æ–º–º–∏—Ç –∏ push –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –Ω–æ–≤—É—é —Ñ–∏—á—É-–≤–µ—Ç–∫—É ‚Äî ‚úÖ

–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å—Ç–∏—Ç—å pytest ‚Äî —Ç–µ—Å—Ç–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî ‚ö†Ô∏è

üß≠ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
üîÅ 1. –°–æ–∑–¥–∞—Ç—å Pull Request
–ù–∞ GitHub –ø–µ—Ä–µ–π–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –≤ –≤—ã–≤–æ–¥–µ:
–°–æ–∑–¥–∞—Ç—å PR –∏–∑ feature/–Ω–∞–∑–≤–∞–Ω–∏–µ

–û–ø–∏—à–∏, —á—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ —Ñ–∏—á–µ, —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∑–∞–¥–∞—á–∏ –∏–ª–∏ issues, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.

üß™ 2. –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã
–ï—Å–ª–∏ —Ç–µ—Å—Ç–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç:

—Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤ tests/:

–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/
‚îú‚îÄ‚îÄ test_loader.py
‚îú‚îÄ‚îÄ test_chunker.py
‚îî‚îÄ‚îÄ test_summary_generator.py
–Ω–∞–ø–∏—à–∏ —Ö–æ—Ç—è –±—ã 1 –ø—Ä–æ—Å—Ç–æ–π pytest —Ç–µ—Å—Ç –≤ –∫–∞–∂–¥–æ–º:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
def test_dummy():
    assert True
üß± 3. –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ docs/:
–°–æ–∑–¥–∞–π docs/contributing.md —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º –ø–æ –≤–∫–ª–∞–¥–∞–º, —Å—Å—ã–ª–∫–æ–π –Ω–∞ CONTRIBUTING.md, –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Ñ–∏—á.

‚úÖ 4. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–π –≤–µ—Ç–∫—É main, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
–ï—Å–ª–∏ —Ö–æ—á–µ—à—å, —á—Ç–æ–±—ã main —Å–ª–µ–¥–æ–≤–∞–ª –∑–∞ stable ‚Äî –º–æ–∂–µ—à—å —Å–¥–µ–ª–∞—Ç—å PR –∏–∑ stable ‚Üí main.

–•–æ—á–µ—à—å, —è —Å–≥–µ–Ω–µ—Ä–∏—Ä—É—é —à–∞–±–ª–æ–Ω PR –∏ –ø—Ä–µ–¥–ª–æ–∂—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –±–∞–∑–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤?


































ash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pip install pytest pytest-asyncio
–ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å –∑–∞–ø—É—Å–∫–∞—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã, —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ.

üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/
‚îú‚îÄ‚îÄ test_loader.py               # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
‚îú‚îÄ‚îÄ test_chunker.py              # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞–Ω–∫–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞
‚îú‚îÄ‚îÄ test_summary_generator.py    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ sample_humans.txt        # –¢–µ–∫—Å—Ç –æ —á–µ–ª–æ–≤–µ–∫–µ
‚îÇ   ‚îú‚îÄ‚îÄ sample_computers.txt     # –¢–µ–∫—Å—Ç –æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞—Ö
‚îÇ   ‚îî‚îÄ‚îÄ ...                      # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å —Ç–µ–º–∞—Ç–∏–∫–∏: –∏—Å—Ç–æ—Ä–∏—è, –Ω–∞—É–∫–∞ –∏ —Ç.–ø.
üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pytest tests/
–ò–ª–∏, –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–æ–¥—É–ª—è:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pytest tests/test_summary_generator.py
–ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–≥—É –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã sample_humans.txt –∏ sample_computers.txt –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–∞–Ω–∫–∏–Ω–≥–∞. –°–æ–∑–¥–∞—ë–º?


–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞! –í–æ—Ç –∫—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥ —Ç–≤–æ–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É:

‚úÖ –ß—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ:
–°–æ–∑–¥–∞–Ω–∞ –∏ –∑–∞–ø—É—à–µ–Ω–∞ –≤–µ—Ç–∫–∞ stable ‚Äî ‚úÖ

–î–æ–±–∞–≤–ª–µ–Ω –∏ –∑–∞–∫–æ–º–º–∏—á–µ–Ω CONTRIBUTING.md –≤ stable ‚Äî ‚úÖ

–°–æ–∑–¥–∞–Ω–∞ –≤–µ—Ç–∫–∞ feature/–Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç stable ‚Äî ‚úÖ

–í—ã–ø–æ–ª–Ω–µ–Ω –∫–æ–º–º–∏—Ç –∏ push –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –Ω–æ–≤—É—é —Ñ–∏—á—É-–≤–µ—Ç–∫—É ‚Äî ‚úÖ

–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å—Ç–∏—Ç—å pytest ‚Äî —Ç–µ—Å—Ç–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî ‚ö†Ô∏è

üß≠ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
üîÅ 1. –°–æ–∑–¥–∞—Ç—å Pull Request
–ù–∞ GitHub –ø–µ—Ä–µ–π–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –≤ –≤—ã–≤–æ–¥–µ:
–°–æ–∑–¥–∞—Ç—å PR –∏–∑ feature/–Ω–∞–∑–≤–∞–Ω–∏–µ

–û–ø–∏—à–∏, —á—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ —Ñ–∏—á–µ, —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∑–∞–¥–∞—á–∏ –∏–ª–∏ issues, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.

üß™ 2. –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã
–ï—Å–ª–∏ —Ç–µ—Å—Ç–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç:

—Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤ tests/:

–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/
‚îú‚îÄ‚îÄ test_loader.py
‚îú‚îÄ‚îÄ test_chunker.py
‚îî‚îÄ‚îÄ test_summary_generator.py
–Ω–∞–ø–∏—à–∏ —Ö–æ—Ç—è –±—ã 1 –ø—Ä–æ—Å—Ç–æ–π pytest —Ç–µ—Å—Ç –≤ –∫–∞–∂–¥–æ–º:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
def test_dummy():
    assert True
üß± 3. –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ docs/:
–°–æ–∑–¥–∞–π docs/contributing.md —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º –ø–æ –≤–∫–ª–∞–¥–∞–º, —Å—Å—ã–ª–∫–æ–π –Ω–∞ CONTRIBUTING.md, –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Ñ–∏—á.

‚úÖ 4. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–π –≤–µ—Ç–∫—É main, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:
–ï—Å–ª–∏ —Ö–æ—á–µ—à—å, —á—Ç–æ–±—ã main —Å–ª–µ–¥–æ–≤–∞–ª –∑–∞ stable ‚Äî –º–æ–∂–µ—à—å —Å–¥–µ–ª–∞—Ç—å PR –∏–∑ stable ‚Üí main.

–•–æ—á–µ—à—å, —è —Å–≥–µ–Ω–µ—Ä–∏—Ä—É—é —à–∞–±–ª–æ–Ω PR –∏ –ø—Ä–µ–¥–ª–æ–∂—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –±–∞–∑–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
test_loader.py -  —Å–æ–∑–¥–∞—Ç—å 


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è —Ñ–∞–π–ª–∞ tests/test_loader.py, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–∑–æ–≤—É—é —Ä–∞–±–æ—Ç—É –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (FileLoader –∏–ª–∏ SmartLoader):

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
# tests/test_loader.py

import pytest
from core.tools.loader import FileLoader

@pytest.fixture
def sample_txt_file(tmp_path):
    file_path = tmp_path / "sample.txt"
    file_path.write_text("–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.")
    return str(file_path)

def test_file_loader_text(sample_txt_file):
    loader = FileLoader()
    result = loader.load(sample_txt_file)

    assert isinstance(result, dict)
    assert "text" in result
    assert "—Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç" in result["text"]
    assert len(result["text"]) > 10
üìå –ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç —Ç–µ—Å—Ç:
–°–æ–∑–¥–∞—ë—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–π .txt —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º.

–ó–∞–≥—Ä—É–∂–∞–µ—Ç –µ–≥–æ —Å –ø–æ–º–æ—â—å—é FileLoader.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º "text" –∏ —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.

–•–æ—á–µ—à—å, —è —Å—Ä–∞–∑—É —Å–≥–µ–Ω–µ—Ä–∏—Ä—É—é –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è test_chunker.py –∏ test_summary_generator.py?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
–¥–∞–≤–∞–π —Å—Ä–∞–∑—É –ø–∞–ø–∫—É —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏  —Ç–µ–∫—Å—Ç–∞–º–∏  —Å–æ–∑–¥–∞–¥–∏–º - 1 –ø–æ –∫–æ–º–ø—å—Ç–µ—Ä–∞–º, 2 –ø–æ –ª—é–¥—è–º  –∏ —Ç.–¥. –ø–æ—Å–º–æ—Ç—Ä–∏–º  –∫–∞–∫ –æ–Ω –Ω–∞ —á–∞–Ω–≥–∏ –∏ –ø—Ä–æ—á–µ–µ —Ä–∞–∑–±–∏–≤–∞–µ—Ç  –ø–æ 40 —Å—Ç—Ä–æ–∫ 


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–æ! –í–æ—Ç –∫–∞–∫ –º–æ–∂–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –ø–∞–ø–∫—É —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ –Ω–∞–ø–æ–ª–Ω–∏—Ç—å –µ—ë —Å–æ–¥–µ—Ä–∂–∏–º—ã–º:

üìÅ tests/test_data/
1. computers.txt
txt
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
–ö–æ–º–ø—å—é—Ç–µ—Ä ‚Äî —ç—Ç–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–µ–º–æ–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —Å–ø–æ—Å–æ–±–Ω–æ–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª—è—Ç—å –¥—Ä—É–≥–∏–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏. –ü–µ—Ä–≤—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã –ø–æ—è–≤–∏–ª–∏—Å—å –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ XX –≤–µ–∫–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∏ –≤–æ–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã –≤–∫–ª—é—á–∞—é—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ü–ö, –Ω–æ—É—Ç–±—É–∫–∏, —Å–µ—Ä–≤–µ—Ä—ã –∏ –º–æ–±–∏–ª—å–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –º–æ–¥–µ–ª–∏ —Ñ–æ–Ω –ù–µ–π–º–∞–Ω–∞: –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä, –ø–∞–º—è—Ç—å, –≤–≤–æ–¥/–≤—ã–≤–æ–¥.

–°—Ä–µ–¥–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (CPU), –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å (RAM), —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (SSD, HDD) –∏ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞ (GPU). –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã (Windows, Linux, macOS) —É–ø—Ä–∞–≤–ª—è—é—Ç —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.

–ö–æ–º–ø—å—é—Ç–µ—Ä—ã —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏, –Ω–∞—É–∫–µ, –±–∏–∑–Ω–µ—Å–µ, –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ. –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ–±–ª–∞—á–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –æ—Ç–∫—Ä—ã–≤–∞—è –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.
2. humans.txt
txt
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
–ß–µ–ª–æ–≤–µ–∫ ‚Äî —ç—Ç–æ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–∏–¥ Homo sapiens, –æ–±–ª–∞–¥–∞—é—â–∏–π –≤—ã—Å–æ–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–æ–∑–Ω–∞–Ω–∏—è –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏. –õ—é–¥–∏ —Å–ø–æ—Å–æ–±–Ω—ã –∫ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é, —Å–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏—é, —è–∑—ã–∫—É –∏ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤—É.

–§–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —á–µ–ª–æ–≤–µ–∫ ‚Äî –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–µ–µ –ø—Ä—è–º–æ—Ö–æ–¥—è—â–µ–µ —Å—É—â–µ—Å—Ç–≤–æ —Å —Ä–∞–∑–≤–∏—Ç—ã–º–∏ —Ä—É–∫–∞–º–∏ –∏ –º–æ–∑–≥–æ–º. –°–æ—Ü–∏–∞–ª—å–Ω–æ —á–µ–ª–æ–≤–µ–∫ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–∞, –∫—É–ª—å—Ç—É—Ä—ã, —Ä–µ–ª–∏–≥–∏–∏, –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞.

–ù–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–≤–∞–ª–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç—å, —Ñ–∏–ª–æ—Å–æ—Ñ–∏—é, –Ω–∞—É–∫—É –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã –≤–∫–ª—é—á–∞—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–ª–∏–º–∞—Ç–∞, –≥–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏—é, —ç—Ç–∏—á–µ—Å–∫–∏–µ –¥–∏–ª–µ–º–º—ã –ò–ò –∏ –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π.

–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç—Å—è –≤ –±–∏–æ–ª–æ–≥–∏–∏, –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏, —Å–æ—Ü–∏–æ–ª–æ–≥–∏–∏ –∏ –∞–Ω—Ç—Ä–æ–ø–æ–ª–æ–≥–∏–∏. –í–æ–ø—Ä–æ—Å –æ —Å–º—ã—Å–ª–µ –∂–∏–∑–Ω–∏ –∏ —Å—É—â–Ω–æ—Å—Ç–∏ —á–µ–ª–æ–≤–µ–∫–∞ –æ—Å—Ç–∞—ë—Ç—Å—è –ø—Ä–µ–¥–º–µ—Ç–æ–º —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏—Ö –¥–∏—Å–∫—É—Å—Å–∏–π.
üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–∫–∏:
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/
‚îî‚îÄ‚îÄ test_data/
    ‚îú‚îÄ‚îÄ computers.txt
    ‚îî‚îÄ‚îÄ humans.txt
üìå –ß—Ç–æ –¥–∞–ª—å—à–µ:
–≠—Ç–∏ —Ç–µ–∫—Å—Ç—ã –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è chunker.py, summary_generator.py, extractor.py.

–ú–æ–∂–µ–º –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å: —Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –∫–∞–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è, –∫–∞–∫–∏–µ —Ä–æ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç summary_generator.

–•–æ—á–µ—à—å, —è –¥–æ–±–∞–≤–ª—é –∫–æ–¥ —Ç–µ—Å—Ç–æ–≤ test_chunker.py –∏ test_summary_generator.py, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö —ç—Ç–∏ —Ç–µ–∫—Å—Ç—ã?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
# tests/test_loader.py - –ø–µ—Ä–µ–ø–∏—à–∏–º?


ChatGPT —Å–∫–∞–∑–∞–ª:
Test Loader

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
import pytest
from pathlib import Path
from core.tools.loader import load_text_from_file

TEST_FILES_DIR = Path(__file__).parent / "test_data"

@pytest.mark.parametrize("filename,expected_substring", [
    ("computers.txt", "–ö–æ–º–ø—å—é—Ç–µ—Ä"),
    ("humans.txt", "–ß–µ–ª–æ–≤–µ–∫"),
])
def test_load_text_from_file(filename, expected_substring):
    path = TEST_FILES_DIR / filename
    assert path.exists(), f"Test file not found: {path}"

    text = load_text_from_file(path)
    assert isinstance(text, str), "Loaded content is not a string"
    assert expected_substring in text, f"Expected content not found in {filename}"
    assert len(text) > 100, "Text too short to be meaningful"

–í–æ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π tests/test_loader.py, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—é Pytest –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ test_data/. –ì–æ—Ç–æ–≤ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å: —Å–æ–∑–¥–∞—Ç—å —Å–∞–º–∏ —Ñ–∞–π–ª—ã –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã —á–∞–Ω–∫–∏–Ω–≥–∞.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
–∞ –≥–¥–µ –≤—Å–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª–µ 


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è tests/test_loader.py —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏, –ø–æ—è—Å–Ω—è—é—â–∏–º–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∫–æ–¥–∞:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
"""
üß™ –¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ (loader.py)

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
- –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –Ω–µ–ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞ (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π, –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π –∏ –¥—Ä.)

–¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ: test_data/
"""

import pytest
from pathlib import Path
from core.tools.loader import load_text_from_file

# üìÇ –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
TEST_DIR = Path(__file__).parent / "test_data"

# üß™ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã: (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ–º–∞—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ)
@pytest.mark.parametrize(
    "filename,expected_phrase",
    [
        ("computers.txt", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"),       # —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        ("humans.txt", "—ç–º–æ—Ü–∏–∏"),             # –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π
        ("empty.txt", ""),                    # –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª (–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π)
        ("symbols_only.txt", ""),             # —Ñ–∞–π–ª –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
        ("multilang.txt", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç")        # –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    ]
)
def test_load_text_from_file(filename: str, expected_phrase: str):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∑–∞–≥—Ä—É–∑—á–∏–∫:
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É
    - –°–æ–¥–µ—Ä–∂–∏—Ç –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω)
    """
    file_path = TEST_DIR / filename
    text = load_text_from_file(file_path)

    assert isinstance(text, str), "–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π"
    assert len(text) > 0 or expected_phrase == "", "–¢–µ–∫—Å—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π"

    if expected_phrase:
        assert expected_phrase in text.lower(), f"–û–∂–∏–¥–∞–µ–º–∞—è —Ñ—Ä–∞–∑–∞ '{expected_phrase}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
–ß—Ç–æ –¥–∞–ª—å—à–µ:
–ï—Å–ª–∏ –Ω—É–∂–Ω–æ, —è —Å–æ–∑–¥–∞–º test_data/ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ (computers.txt, humans.txt –∏ –¥—Ä.)

–ú–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã —á–∞–Ω–∫–∏–Ω–≥–∞ (chunker.py)

–ò–ª–∏ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ PDF, DOCX, HTML –∏ Markdown

–°–∫–∞–∂–∏, –≤ –∫–∞–∫–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –¥–≤–∏–≥–∞—Ç—å—Å—è.






–í—ã —Å–∫–∞–∑–∞–ª–∏:
test_chunker.py


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –±–∞–∑–æ–≤–∞—è –∏ –ø–æ–Ω—è—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è tests/test_chunker.py —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
"""
üß™ –¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ (chunker.py)

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
- –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —á–∞–Ω–∫–∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –∑–∞–¥–∞–Ω–Ω—ã–π –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤
"""

import pytest
from core.text_processing.chunker import chunk_text

@pytest.mark.parametrize("input_text, max_len, expected_chunks", [
    (
        "–≠—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–æ–º–µ—Ä –æ–¥–∏–Ω. –ê —ç—Ç–æ –≤—Ç–æ—Ä–æ–µ. –ò —Ç—Ä–µ—Ç—å–µ –∑–¥–µ—Å—å. –î–∞–ª–µ–µ —á–µ—Ç–≤—ë—Ä—Ç–æ–µ. –ü—è—Ç–æ–µ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –º—ã—Å–ª—å.",
        50,
        2  # –ü–æ 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫
    ),
    (
        "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç.",
        100,
        1  # –û–¥–∏–Ω –∫–æ—Ä–æ—Ç–∫–∏–π —á–∞–Ω–∫
    ),
    (
        "–ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü.\n\n–í—Ç–æ—Ä–æ–π –∞–±–∑–∞—Ü, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–º–Ω–æ–≥–æ –¥–ª–∏–Ω–Ω–µ–µ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª—å—à–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. " * 5,
        200,
        3  # –û–∂–∏–¥–∞–µ–º –±–æ–ª–µ–µ 1 —á–∞–Ω–∫–∞
    )
])
def test_chunk_text(input_text, max_len, expected_chunks):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞:
    - –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    - –£–≤–∞–∂–∞–µ—Ç –ª–∏–º–∏—Ç –¥–ª–∏–Ω—ã
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
    """
    chunks = chunk_text(input_text, max_length=max_len)

    assert isinstance(chunks, list), "–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º"
    assert all(isinstance(c, str) for c in chunks), "–ö–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π"
    assert all(len(c) <= max_len for c in chunks), "–ù–∏ –æ–¥–∏–Ω —á–∞–Ω–∫ –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å max_length"
    assert len(chunks) == expected_chunks or abs(len(chunks) - expected_chunks) <= 1, "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ"
–ß—Ç–æ –Ω—É–∂–Ω–æ:
–°–æ–∑–¥–∞—Ç—å test_data/chunker/long_text.txt, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã ‚Äî —Ç–æ–≥–¥–∞ –¥–æ–±–∞–≤–∏–º test_chunker_from_file.py.

–ü—Ä–æ–¥–æ–ª–∂–∏–º —Å —Ç–µ—Å—Ç–∞–º–∏ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä, —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ NER?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
test_summary_generator.py


ChatGPT —Å–∫–∞–∑–∞–ª:
–í–æ—Ç –Ω–∞—á–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ–∞–π–ª–∞ tests/test_summary_generator.py, –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –±–∞–∑–µ —Ñ–∏–∫—Ç–∏–≤–Ω–æ–≥–æ LLM –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞:

python
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
"""
üß™ –¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (summary_generator.py)

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–∑–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
- –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
- –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —Ä–æ–ª—è–º–∏
"""

import pytest
from core.tools.summary_generator import (
    UniversalSummaryGenerator,
    SummaryRequest,
    SummaryConfig,
    SummaryLength,
    SummaryStyle,
    Role
)

class DummyLLM:
    """üß™ –ó–∞–≥–ª—É—à–∫–∞ LLM-–∫–ª–∏–µ–Ω—Ç–∞"""
    def generate(self, prompt: str) -> str:
        return "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∞."

@pytest.fixture
def generator():
    gen = UniversalSummaryGenerator()
    gen.llm = DummyLLM()
    return gen

def test_basic_summary_generation(generator):
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–¥–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    request = SummaryRequest(
        text="–≠—Ç–æ —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã –ø—Ä–æ–π—Ç–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é.",
        language="ru",
        config=SummaryConfig(length=SummaryLength.SHORT, style=SummaryStyle.SIMPLE),
        roles=[Role.GENERAL]
    )

    result = generator.generate_summary(request)

    assert result.language == "ru"
    assert Role.GENERAL.value in result.summaries
    assert isinstance(result.summaries[Role.GENERAL.value], str)
    assert len(result.key_points) >= 0
    assert result.time_taken >= 0

def test_custom_role_summary(generator):
    """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ä–æ–ª—å—é"""
    request = SummaryRequest(
        text="–ù–µ–∫–æ—Ç–æ—Ä—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ä–æ–ª–∏. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π.",
        language="en",
        config=SummaryConfig(length=SummaryLength.MEDIUM),
        custom_roles=["UX_researcher"]
    )

    result = generator.generate_summary(request)

    assert "UX_researcher" in result.summaries
    assert isinstance(result.summaries["UX_researcher"], str)
üìÇ –†–∞–∑–º–µ—â–µ–Ω–∏–µ
–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/test_summary_generator.py
–•–æ—á–µ—à—å —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ç–µ—Å—Ç—ã –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫—É –∏–∑ .txt/.docx?






–í—ã —Å–∫–∞–∑–∞–ª–∏:
—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —Ç–µ—Å—Ç—ã 


ChatGPT —Å–∫–∞–∑–∞–ª:
–û—Ç–ª–∏—á–Ω–æ! –¢–∞–∫ –∫–∞–∫ —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é tests/, –≤–æ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

‚úÖ –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω—É–∂–Ω—ã–µ –ø–∞–∫–µ—Ç—ã
–£–±–µ–¥–∏—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pip install pytest pytest-asyncio
–ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å –∑–∞–ø—É—Å–∫–∞—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã, —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ.

üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/
‚îú‚îÄ‚îÄ test_loader.py               # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
‚îú‚îÄ‚îÄ test_chunker.py              # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞–Ω–∫–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞
‚îú‚îÄ‚îÄ test_summary_generator.py    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ sample_humans.txt        # –¢–µ–∫—Å—Ç –æ —á–µ–ª–æ–≤–µ–∫–µ
‚îÇ   ‚îú‚îÄ‚îÄ sample_computers.txt     # –¢–µ–∫—Å—Ç –æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞—Ö
‚îÇ   ‚îî‚îÄ‚îÄ ...                      # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å —Ç–µ–º–∞—Ç–∏–∫–∏: –∏—Å—Ç–æ—Ä–∏—è, –Ω–∞—É–∫–∞ –∏ —Ç.–ø.
üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pytest tests/
–ò–ª–∏, –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–æ–¥—É–ª—è:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
pytest tests/test_summary_generator.py
–ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–≥—É –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã sample_humans.txt –∏ sample_computers.txt –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–∞–Ω–∫–∏–Ω–≥–∞. –°–æ–∑–¥–∞—ë–º?



"""
üß™ –¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ (loader.py)

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
- –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –Ω–µ–ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞ (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π, –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π –∏ –¥—Ä.)

–¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ: test_data/
"""

import pytest
from pathlib import Path
from core.tools.loader import load_text_from_file

# üìÇ –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
TEST_DIR = Path(__file__).parent / "test_data"

# üß™ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã: (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞, –æ–∂–∏–¥–∞–µ–º–∞—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ)
@pytest.mark.parametrize(
    "filename,expected_phrase",
    [
        ("computers.txt", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"),       # —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        ("humans.txt", "—ç–º–æ—Ü–∏–∏"),             # –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–π
        ("empty.txt", ""),                    # –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª (–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π)
        ("symbols_only.txt", ""),             # —Ñ–∞–π–ª –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
        ("multilang.txt", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç")        # –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    ]
)
def test_load_text_from_file(filename: str, expected_phrase: str):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∑–∞–≥—Ä—É–∑—á–∏–∫:
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É
    - –°–æ–¥–µ—Ä–∂–∏—Ç –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω)
    """
    file_path = TEST_DIR / filename
    text = load_text_from_file(file_path)

    assert isinstance(text, str), "–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π"
    assert len(text) > 0 or expected_phrase == "", "–¢–µ–∫—Å—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π"

    if expected_phrase:
        assert expected_phrase in text.lower(), f"–û–∂–∏–¥–∞–µ–º–∞—è —Ñ—Ä–∞–∑–∞ '{expected_phrase}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

"""
üß™ –¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ (chunker.py)

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
- –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —á–∞–Ω–∫–∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –∑–∞–¥–∞–Ω–Ω—ã–π –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤
"""

import pytest
from core.text_processing.chunker import chunk_text

@pytest.mark.parametrize("input_text, max_len, expected_chunks", [
    (
        "–≠—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–æ–º–µ—Ä –æ–¥–∏–Ω. –ê —ç—Ç–æ –≤—Ç–æ—Ä–æ–µ. –ò —Ç—Ä–µ—Ç—å–µ –∑–¥–µ—Å—å. –î–∞–ª–µ–µ —á–µ—Ç–≤—ë—Ä—Ç–æ–µ. –ü—è—Ç–æ–µ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –º—ã—Å–ª—å.",
        50,
        2  # –ü–æ 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫
    ),
    (
        "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç.",
        100,
        1  # –û–¥–∏–Ω –∫–æ—Ä–æ—Ç–∫–∏–π —á–∞–Ω–∫
    ),
    (
        "–ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü.\n\n–í—Ç–æ—Ä–æ–π –∞–±–∑–∞—Ü, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–º–Ω–æ–≥–æ –¥–ª–∏–Ω–Ω–µ–µ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª—å—à–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. " * 5,
        200,
        3  # –û–∂–∏–¥–∞–µ–º –±–æ–ª–µ–µ 1 —á–∞–Ω–∫–∞
    )
])
def test_chunk_text(input_text, max_len, expected_chunks):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞:
    - –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    - –£–≤–∞–∂–∞–µ—Ç –ª–∏–º–∏—Ç –¥–ª–∏–Ω—ã
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
    """
    chunks = chunk_text(input_text, max_length=max_len)

    assert isinstance(chunks, list), "–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º"
    assert all(isinstance(c, str) for c in chunks), "–ö–∞–∂–¥—ã–π —á–∞–Ω–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π"
    assert all(len(c) <= max_len for c in chunks), "–ù–∏ –æ–¥–∏–Ω —á–∞–Ω–∫ –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å max_length"
    assert len(chunks) == expected_chunks or abs(len(chunks) - expected_chunks) <= 1, "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ"

"""
üß™ –¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (summary_generator.py)

–¶–µ–ª—å:
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–∑–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
- –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
- –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —Ä–æ–ª—è–º–∏
"""

import pytest
from core.tools.summary_generator import (
    UniversalSummaryGenerator,
    SummaryRequest,
    SummaryConfig,
    SummaryLength,
    SummaryStyle,
    Role
)

class DummyLLM:
    """üß™ –ó–∞–≥–ª—É—à–∫–∞ LLM-–∫–ª–∏–µ–Ω—Ç–∞"""
    def generate(self, prompt: str) -> str:
        return "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∞."

@pytest.fixture
def generator():
    gen = UniversalSummaryGenerator()
    gen.llm = DummyLLM()
    return gen

def test_basic_summary_generation(generator):
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–¥–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    request = SummaryRequest(
        text="–≠—Ç–æ —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã –ø—Ä–æ–π—Ç–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é.",
        language="ru",
        config=SummaryConfig(length=SummaryLength.SHORT, style=SummaryStyle.SIMPLE),
        roles=[Role.GENERAL]
    )

    result = generator.generate_summary(request)

    assert result.language == "ru"
    assert Role.GENERAL.value in result.summaries
    assert isinstance(result.summaries[Role.GENERAL.value], str)
    assert len(result.key_points) >= 0
    assert result.time_taken >= 0

def test_custom_role_summary(generator):
    """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ä–æ–ª—å—é"""
    request = SummaryRequest(
        text="–ù–µ–∫–æ—Ç–æ—Ä—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ä–æ–ª–∏. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π.",
        language="en",
        config=SummaryConfig(length=SummaryLength.MEDIUM),
        custom_roles=["UX_researcher"]
    )

    result = generator.generate_summary(request)

    assert "UX_researcher" in result.summaries
    assert isinstance(result.summaries["UX_researcher"], str)


{
  "osint": [
    {
      "filename": "sample_petrov.txt",
      "name": "–ü–µ—Ç—Ä–æ–≤ –ê–ª–µ–∫—Å–µ–π –°–µ—Ä–≥–µ–µ–≤–∏—á",
      "role": "–±–∏–∑–Ω–µ—Å–º–µ–Ω",
      "tags": ["–±–∏–∑–Ω–µ—Å", "–≥–æ—Å–∑–∞–∫–∞–∑", "–∫–æ—Ä—Ä—É–ø—Ü–∏—è"],
      "source": "—Å–∏–Ω—Ç–µ—Ç–∏–∫–∞"
    },
    {
      "filename": "sample_petrov_marina.txt",
      "name": "–ü–µ—Ç—Ä–æ–≤–∞ –ú–∞—Ä–∏–Ω–∞ –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–Ω–∞",
      "role": "–∂–µ–Ω–∞ –±–∏–∑–Ω–µ—Å–º–µ–Ω–∞, –¥–∏—Ä–µ–∫—Ç–æ—Ä —Ñ–æ–Ω–¥–∞",
      "tags": ["–±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–∏–Ω—Ç—Ä–∏–≥–∏", "—Å–≤—è–∑–∏"],
      "source": "—Å–∏–Ω—Ç–µ—Ç–∏–∫–∞"
    },
    {
      "filename": "sample_network_sidorov.txt",
      "name": "–°–∏–¥–æ—Ä–æ–≤, –ö–æ–≤–∞–ª—ë–≤, –ë–µ–ª–æ–≤–∞",
      "role": "—Å–µ—Ç—å –∫–æ—Ä—Ä—É–ø—Ü–∏–∏",
      "tags": ["–æ—Ç–∫–∞—Ç—ã", "–ø–æ—Å—Ä–µ–¥–Ω–∏–∫–∏", "—Ç–µ–Ω–µ–≤—ã–µ —Å–¥–µ–ª–∫–∏"],
      "source": "—Å–∏–Ω—Ç–µ—Ç–∏–∫–∞"
    },
    {
      "filename": "sample_kovalev.txt",
      "name": "–ö–æ–≤–∞–ª—ë–≤ –ê—Ä—Ç—ë–º –î–º–∏—Ç—Ä–∏–µ–≤–∏—á",
      "role": "–ø–æ—Å—Ä–µ–¥–Ω–∏–∫, –∫—Ä–∏–º–∏–Ω–∞–ª",
      "tags": ["–æ–±–Ω–∞–ª", "–∫—Ä–∏–ø—Ç–∞", "FSO"],
      "source": "—Å–∏–Ω—Ç–µ—Ç–∏–∫–∞"
    },
    {
      "filename": "sample_belova.txt",
      "name": "–ë–µ–ª–æ–≤–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –°–µ—Ä–≥–µ–µ–≤–Ω–∞",
      "role": "–ª—é–±–æ–≤–Ω–∏—Ü–∞, PR, –ø–æ—Å—Ä–µ–¥–Ω–∏–∫",
      "tags": ["–≥—Ä–∞–Ω—Ç—ã", "–æ—Ç–º—ã–≤", "O–ü–ì"],
      "source": "—Å–∏–Ω—Ç–µ—Ç–∏–∫–∞"
    },
    {
      "filename": "sample_hacker.txt",
      "name": "–ì–ª–µ–± –°—É—Ö–æ–≤ / ShadowAdmin",
      "role": "—Ö–∞–∫–µ—Ä",
      "tags": ["darkweb", "–≤–∑–ª–æ–º", "–ø–µ—Ä–µ–ø–∏—Å–∫–∞"],
      "source": "—Å–∏–Ω—Ç–µ—Ç–∏–∫–∞"
    }
  ]
}

–°–æ–∑–¥–∞–Ω index.json (—Å–º. –¥–æ–∫—É–º–µ–Ω—Ç) ‚Äî –æ–Ω —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è —É–¥–æ–±–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞:

–•—Ä–∞–Ω–∏—Ç –∏–º—è —Ñ–∞–π–ª–∞, —Å—É–±—ä–µ–∫—Ç–∞, —Ä–æ–ª–∏, —Ç–µ–≥–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫

–õ–µ–≥–∫–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Å–∫—Ä–∏–ø—Ç–∞—Ö Python, —Å–∏—Å—Ç–µ–º–∞—Ö –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

–§–∞–π–ª index.json —Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å –≤ –ø–∞–ø–∫–µ tests/test_data/osint/, —Ä—è–¥–æ–º —Å –≤–∞—à–∏–º–∏ sample_*.txt. –≠—Ç–æ –±—É–¥–µ—Ç —Å–ª—É–∂–∏—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö OSINT-–¥–æ—Å—å–µ.

üìÅ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

pgsql
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/
‚îî‚îÄ‚îÄ test_data/
    ‚îî‚îÄ‚îÄ osint/
        ‚îú‚îÄ‚îÄ sample_belova.txt
        ‚îú‚îÄ‚îÄ sample_hacker.txt
        ‚îú‚îÄ‚îÄ sample_kovalev.txt
        ‚îú‚îÄ‚îÄ sample_network_sidorov.txt
        ‚îú‚îÄ‚îÄ sample_petrov.txt 
        ‚îú‚îÄ‚îÄ sample_petrov_marina.txt
        ‚îú‚îÄ‚îÄ index.json   ‚Üê üìç –≤–æ—Ç —Å—é–¥–∞



–í–æ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ metadata.csv, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–µ –≤—Å–µ —Ç–µ–∫—É—â–∏–µ OSINT-—Ñ–∞–π–ª—ã:

csv
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
filename,subject,role,tags
sample_petrov.txt,–ü–µ—Ç—Ä–æ–≤ –ê–ª–µ–∫—Å–µ–π –°–µ—Ä–≥–µ–µ–≤–∏—á,–ì–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä –ì–µ–æ–¢–µ—Ö –ò–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥,"–±–∏–∑–Ω–µ—Å, –≥–µ–æ–¥–∞–Ω–Ω—ã–µ, –æ—Å–∏–Ω—Ç, –∞—Ä–∫—Ç–∏–∫–∞"
sample_petrov_marina.txt,–ü–µ—Ç—Ä–æ–≤–∞ –ú–∞—Ä–∏–Ω–∞ –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–Ω–∞,–ñ–µ–Ω–∞ –ü–µ—Ç—Ä–æ–≤–∞, –æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å —Ñ–æ–Ω–¥–∞,"—Å–≤—è–∑–∏, –∫–æ—Ä—Ä—É–ø—Ü–∏—è, –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏"
sample_kovalev.txt,–ö–æ–≤–∞–ª—ë–≤ –ê—Ä—Ç—ë–º –î–º–∏—Ç—Ä–∏–µ–≤–∏—á,–ü–æ—Å—Ä–µ–¥–Ω–∏–∫, –∫—Ä–∏–º–∏–Ω–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏,"–æ–±–Ω–∞–ª, —Å–µ—Ä—ã–µ —Å—Ö–µ–º—ã, –§–°–û, –≤–∑–ª–æ–º"
sample_belova.txt,–ë–µ–ª–æ–≤–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –°–µ—Ä–≥–µ–µ–≤–Ω–∞,–õ—é–±–æ–≤–Ω–∏—Ü–∞/—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–æ—Å—Ä–µ–¥–Ω–∏–∫,"–≥—Ä–∞–Ω—Ç—ã, –æ—Ç–º—ã–≤, PR, –∫–æ–º–ø—Ä–æ–º–∞—Ç"
sample_network_sidorov.txt,–°–∏–¥–æ—Ä–æ–≤ –ò–ª—å—è –ú–∏—Ö–∞–π–ª–æ–≤–∏—á,–î–µ–ª–æ–≤–æ–π –ø–∞—Ä—Ç–Ω—ë—Ä,"–æ—Ç–∫–∞—Ç—ã, –∫–æ—Ä—Ä—É–ø—Ü–∏—è, —Ä–æ—Å–Ω–µ—Ñ—Ç—å, –ø–æ–¥–∫—É–ø"
sample_hacker.txt,–ì–ª–µ–± –°—É—Ö–æ–≤ (ShadowAdmin),–•–∞–∫–µ—Ä/–≤–∑–ª–æ–º—â–∏–∫,"—Ñ–∏—à–∏–Ω–≥, sim-swap, —É—Ç–µ—á–∫–∞, –∫—Ä–∏–ø—Ç–∞"
sample_osint.txt,–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω –ó–æ—Ä–∏–Ω,–ò–¢-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç/—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫,"–æ—Å–∏–Ω—Ç, –ø–æ–≤–µ–¥–µ–Ω–∏–µ, —Ö–∞–±—Ä, —Ç—Ä–µ–∫–µ—Ä—ã"
üìå –ú–µ—Å—Ç–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è:
–°–æ—Ö—Ä–∞–Ω–∏ —Ñ–∞–π–ª –ø–æ –ø—É—Ç–∏:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
tests/test_data/osint/metadata.csv
–•–æ—á–µ—à—å, —è —Å—Ä–∞–∑—É —Å–æ–∑–¥–∞–º index.json –≤ –ø–∞—Ä–µ –∫ —ç—Ç–æ–º—É CSV?






–û—Ç–ª–∏—á–Ω–æ! ‚úÖ –£ –Ω–∞—Å —Ç–µ–ø–µ—Ä—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–π –∏ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã–π OSINT-—Ç–µ—Å—Ç–æ–≤—ã–π –º–æ–¥—É–ª—å, –≤–∫–ª—é—á–∞—è:

üìÅ tests/test_data/osint/

üìÑ –í—Å–µ –¥–æ—Å—å–µ

üìÑ index.json –∏ metadata.csv

üß™ test_metadata_parsing.py –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–≤—è–∑–∫–∏

üîú –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
–î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ—Ç–µ—Å—Ç—ã –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ index.json –∏ metadata.csv:

—Å–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã,

–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø—É—Ç–µ–π –∏ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤.

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏:

–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —ç—Ç–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ —á–∞–Ω–∫–µ—Ä / NER / –≥—Ä–∞—Ñ-–º–æ–¥—É–ª—å.

–î–æ–±–∞–≤–∏—Ç—å docs/osint_samples.md (–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º):

–æ–ø–∏—Å–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π,

—Ü–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏,

–≥–¥–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, Maltego, RDF, –≥—Ä–∞—Ñ—ã –∏ —Ç.–ø.).

Git:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
git add tests/test_data/osint/*
git add tests/test_metadata_parsing.py
git commit -m "üß™ –î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–µ—Å—Ç–æ–≤—ã–µ OSINT-–¥–æ—Å—å–µ, index.json, metadata.csv –∏ —Ç–µ—Å—Ç—ã"
git push origin stable

# core/processing/ner.py
import spacy
from typing import List, Dict, Set, Optional
from collections import defaultdict
import re
import logging
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EntityProcessor:
    def __init__(self, model_name: str = "ru_core_news_md"):
        try:
            self.nlp = spacy.load(model_name)
            logger.info(f"Successfully loaded spaCy model: {model_name}")
        except OSError:
            logger.error(f"Model {model_name} not found. Downloading...")
            from spacy.cli import download
            download(model_name)
            self.nlp = spacy.load(model_name)

        self.patterns = {
            'PHONE': r'\+?[\d\-\(\)\s]{7,15}',
            'CRYPTO_WALLET': r'(bc1|[13])[a-zA-HJ-NP-Z0-9]{25,39}',
            'IBAN': r'[A-Z]{2}\d{2}[A-Z\d]{1,30}',
            'EMAIL': r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+',
            'IP_ADDRESS': r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
            'DATE': r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b'
        }

        self.entity_cache = defaultdict(dict)
        self.last_processed = None

    def extract_entities(self, text: str, use_cache: bool = True) -> List[Dict]:
        if use_cache and text in self.entity_cache:
            return self.entity_cache[text]

        start_time = datetime.now()
        doc = self.nlp(text)
        entities = []

        for ent in doc.ents:
            entities.append({
                "text": ent.text,
                "label": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char,
                "source": "spacy"
            })

        for label, pattern in self.patterns.items():
            try:
                for match in re.finditer(pattern, text):
                    entities.append({
                        "text": match.group(),
                        "label": label,
                        "start": match.start(),
                        "end": match.end(),
                        "source": "regex"
                    })
            except re.error as e:
                logger.warning(f"Regex error for pattern {label}: {e}")

        filtered_entities = self._filter_overlapping(entities)
        self.entity_cache[text] = filtered_entities
        self.last_processed = start_time

        logger.info(f"Processed text in {(datetime.now() - start_time).total_seconds():.2f}s")
        return filtered_entities

    def _filter_overlapping(self, entities: List[Dict]) -> List[Dict]:
        entities_sorted = sorted(entities, key=lambda x: (x['end']-x['start'], x['start']), reverse=True)
        filtered = []
        used_positions = set()

        for ent in entities_sorted:
            if not any(pos in used_positions for pos in range(ent['start'], ent['end'])):
                filtered.append(ent)
                used_positions.update(range(ent['start'], ent['end']))

        return sorted(filtered, key=lambda x: x['start'])

    def get_entity_statistics(self) -> Dict:
        stats = defaultdict(int)
        for entities in self.entity_cache.values():
            for ent in entities:
                stats[ent['label']] += 1
        return dict(stats)


# core/processing/graph_builder.py
import networkx as nx
from typing import List, Dict, Tuple
from itertools import combinations
import community as community_louvain
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)

class RelationGraphBuilder:
    def __init__(self, window_size: int = 5, min_weight: int = 2):
        self.window_size = window_size
        self.min_weight = min_weight
        self.graph_cache = {}

    def build_interaction_graph(self, entities: List[Dict]) -> nx.Graph:
        cache_key = hash(tuple((e['text'], e['label']) for e in entities))
        if cache_key in self.graph_cache:
            return self.graph_cache[cache_key].copy()

        G = nx.Graph()
        entity_texts = [e["text"] for e in entities]

        for ent in entities:
            if not G.has_node(ent["text"]):
                G.add_node(ent["text"],
                           label=ent["label"],
                           count=1,
                           examples=[ent["text"]])
            else:
                G.nodes[ent["text"]]["count"] += 1
                if ent["text"] not in G.nodes[ent["text"]]["examples"]:
                    G.nodes[ent["text"]]["examples"].append(ent["text"])

        co_occurrence = defaultdict(int)
        for i in range(len(entity_texts)):
            for j in range(i+1, min(i+self.window_size, len(entity_texts))):
                pair = tuple(sorted((entity_texts[i], entity_texts[j])))
                co_occurrence[pair] += 1

        for (node1, node2), weight in co_occurrence.items():
            if weight >= self.min_weight and node1 != node2:
                G.add_edge(node1, node2, weight=weight)

        if len(G.nodes) > 0:
            G = self._apply_community_detection(G)
            self._calculate_centrality(G)

        self.graph_cache[cache_key] = G.copy()
        return G

    def _apply_community_detection(self, G: nx.Graph) -> nx.Graph:
        try:
            if len(G.nodes) > 2:
                partition = community_louvain.best_partition(G)
                for node in G.nodes():
                    G.nodes[node]["community"] = partition[node]
        except Exception as e:
            logger.error(f"Community detection failed: {e}")
        return G

    def _calculate_centrality(self, G: nx.Graph):
        try:
            degree_cent = nx.degree_centrality(G)
            betweenness = nx.betweenness_centrality(G)
            closeness = nx.closeness_centrality(G)

            for node in G.nodes():
                G.nodes[node]["degree_centrality"] = degree_cent.get(node, 0)
                G.nodes[node]["betweenness"] = betweenness.get(node, 0)
                G.nodes[node]["closeness"] = closeness.get(node, 0)
        except Exception as e:
            logger.error(f"Centrality calculation failed: {e}")

    def get_key_entities(self, G: nx.Graph, top_n: int = 5,
                        metric: str = "degree_centrality") -> List[Tuple[str, float]]:
        if metric not in ["degree_centrality", "betweenness", "closeness"]:
            metric = "degree_centrality"

        nodes_with_scores = [(n, G.nodes[n].get(metric, 0)) for n in G.nodes()]
        return sorted(nodes_with_scores, key=lambda x: x[1], reverse=True)[:top_n]

    def visualize_graph(self, G: nx.Graph, output_path: str = None):
        try:
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 8))

            pos = nx.spring_layout(G)
            node_colors = [G.nodes[n].get('community', 0) for n in G.nodes()]

            nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=plt.cm.tab20)
            nx.draw_networkx_edges(G, pos, alpha=0.3)
            nx.draw_networkx_labels(G, pos, font_size=8)

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger.info(f"Graph saved to {output_path}")
            else:
                plt.show()

        except ImportError:
            logger.warning("Matplotlib not available for graph visualization")
        except Exception as e:
            logger.error(f"Graph visualization failed: {e}")

# core/processing/ner.py
import spacy
from typing import List, Dict, Set, Optional
from collections import defaultdict
import re
import logging
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EntityProcessor:
    def __init__(self, model_name: str = "ru_core_news_md"):
        try:
            self.nlp = spacy.load(model_name)
            logger.info(f"Successfully loaded spaCy model: {model_name}")
        except OSError:
            logger.error(f"Model {model_name} not found. Downloading...")
            from spacy.cli import download
            download(model_name)
            self.nlp = spacy.load(model_name)

        self.patterns = {
            'PHONE': r'\+?[\d\-\(\)\s]{7,15}',
            'CRYPTO_WALLET': r'(bc1|[13])[a-zA-HJ-NP-Z0-9]{25,39}',
            'IBAN': r'[A-Z]{2}\d{2}[A-Z\d]{1,30}',
            'EMAIL': r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+',
            'IP_ADDRESS': r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
            'DATE': r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b'
        }

        self.entity_cache = defaultdict(dict)
        self.last_processed = None

    def extract_entities(self, text: str, use_cache: bool = True) -> List[Dict]:
        if use_cache and text in self.entity_cache:
            return self.entity_cache[text]

        start_time = datetime.now()
        doc = self.nlp(text)
        entities = []

        for ent in doc.ents:
            entities.append({
                "text": ent.text,
                "label": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char,
                "source": "spacy"
            })

        for label, pattern in self.patterns.items():
            try:
                for match in re.finditer(pattern, text):
                    entities.append({
                        "text": match.group(),
                        "label": label,
                        "start": match.start(),
                        "end": match.end(),
                        "source": "regex"
                    })
            except re.error as e:
                logger.warning(f"Regex error for pattern {label}: {e}")

        filtered_entities = self._filter_overlapping(entities)
        self.entity_cache[text] = filtered_entities
        self.last_processed = start_time

        logger.info(f"Processed text in {(datetime.now() - start_time).total_seconds():.2f}s")
        return filtered_entities

    def _filter_overlapping(self, entities: List[Dict]) -> List[Dict]:
        entities_sorted = sorted(entities, key=lambda x: (x['end']-x['start'], x['start']), reverse=True)
        filtered = []
        used_positions = set()

        for ent in entities_sorted:
            if not any(pos in used_positions for pos in range(ent['start'], ent['end'])):
                filtered.append(ent)
                used_positions.update(range(ent['start'], ent['end']))

        return sorted(filtered, key=lambda x: x['start'])

    def get_entity_statistics(self) -> Dict:
        stats = defaultdict(int)
        for entities in self.entity_cache.values():
            for ent in entities:
                stats[ent['label']] += 1
        return dict(stats)


# core/processing/graph_builder.py
import networkx as nx
from typing import List, Dict, Tuple
from itertools import combinations
import community as community_louvain
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)

class RelationGraphBuilder:
    def __init__(self, window_size: int = 5, min_weight: int = 2):
        self.window_size = window_size
        self.min_weight = min_weight
        self.graph_cache = {}

    def build_interaction_graph(self, entities: List[Dict]) -> nx.Graph:
        cache_key = hash(tuple((e['text'], e['label']) for e in entities))
        if cache_key in self.graph_cache:
            return self.graph_cache[cache_key].copy()

        G = nx.Graph()
        entity_texts = [e["text"] for e in entities]

        for ent in entities:
            if not G.has_node(ent["text"]):
                G.add_node(ent["text"],
                           label=ent["label"],
                           count=1,
                           examples=[ent["text"]])
            else:
                G.nodes[ent["text"]]["count"] += 1
                if ent["text"] not in G.nodes[ent["text"]]["examples"]:
                    G.nodes[ent["text"]]["examples"].append(ent["text"])

        co_occurrence = defaultdict(int)
        for i in range(len(entity_texts)):
            for j in range(i+1, min(i+self.window_size, len(entity_texts))):
                pair = tuple(sorted((entity_texts[i], entity_texts[j])))
                co_occurrence[pair] += 1

        for (node1, node2), weight in co_occurrence.items():
            if weight >= self.min_weight and node1 != node2:
                G.add_edge(node1, node2, weight=weight)

        if len(G.nodes) > 0:
            G = self._apply_community_detection(G)
            self._calculate_centrality(G)

        self.graph_cache[cache_key] = G.copy()
        return G

    def _apply_community_detection(self, G: nx.Graph) -> nx.Graph:
        try:
            if len(G.nodes) > 2:
                partition = community_louvain.best_partition(G)
                for node in G.nodes():
                    G.nodes[node]["community"] = partition[node]
        except Exception as e:
            logger.error(f"Community detection failed: {e}")
        return G

    def _calculate_centrality(self, G: nx.Graph):
        try:
            degree_cent = nx.degree_centrality(G)
            betweenness = nx.betweenness_centrality(G)
            closeness = nx.closeness_centrality(G)

            for node in G.nodes():
                G.nodes[node]["degree_centrality"] = degree_cent.get(node, 0)
                G.nodes[node]["betweenness"] = betweenness.get(node, 0)
                G.nodes[node]["closeness"] = closeness.get(node, 0)
        except Exception as e:
            logger.error(f"Centrality calculation failed: {e}")

    def get_key_entities(self, G: nx.Graph, top_n: int = 5,
                        metric: str = "degree_centrality") -> List[Tuple[str, float]]:
        if metric not in ["degree_centrality", "betweenness", "closeness"]:
            metric = "degree_centrality"

        nodes_with_scores = [(n, G.nodes[n].get(metric, 0)) for n in G.nodes()]
        return sorted(nodes_with_scores, key=lambda x: x[1], reverse=True)[:top_n]

    def visualize_graph(self, G: nx.Graph, output_path: str = None):
        try:
            import matplotlib.pyplot as plt
            plt.figure(figsize=(12, 8))

            pos = nx.spring_layout(G)
            node_colors = [G.nodes[n].get('community', 0) for n in G.nodes()]

            nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=plt.cm.tab20)
            nx.draw_networkx_edges(G, pos, alpha=0.3)
            nx.draw_networkx_labels(G, pos, font_size=8)

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger.info(f"Graph saved to {output_path}")
            else:
                plt.show()

        except ImportError:
            logger.warning("Matplotlib not available for graph visualization")
        except Exception as e:
            logger.error(f"Graph visualization failed: {e}")
