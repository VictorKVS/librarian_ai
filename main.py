import argparse
from loader import load_file
from parser import parse_text
from chunker import split_into_chunks
from extractor import extract_entities
from summary_generator import generate_summary
from graph_tools import build_knowledge_graph


def process_document(path: str):
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ñ–∞–π–ª: {path}")
    raw_text = load_file(path)

    print("‚úÇÔ∏è –û—á–∏—Å—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥...")
    parsed_text = parse_text(raw_text)

    print("üî™ –î–µ–ª–∏–º –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = split_into_chunks(parsed_text)
    print(f"  -> –ß–∞–Ω–∫–æ–≤: {len(chunks)}")

    print("üß¨ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π...")
    entities = extract_entities(parsed_text)
    print(f"  -> –ù–∞–π–¥–µ–Ω–æ: {len(entities)} —Å—É—â–Ω–æ—Å—Ç–µ–π")

    print("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è TL;DR...")
    summary = generate_summary(parsed_text)
    print(f"  -> –°–≤–æ–¥–∫–∞: {summary[:200]}...")

    print("üìà –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...")
    graph = build_knowledge_graph(entities)
    print(f"  -> –£–∑–ª–æ–≤: {len(graph.nodes())}, –°–≤—è–∑–µ–π: {len(graph.edges())}")

    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    return {
        "summary": summary,
        "entities": entities,
        "graph": graph
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="üìö Librarian Mini")
    parser.add_argument("path", type=str, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    args = parser.parse_args()

    process_document(args.path)
